{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portfolio-Exam Part I - Sentiment Analysis\n",
    "\n",
    "* Social Media Analytics - MADS-SMA\n",
    "* Valentin Werger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different approaches\n",
    "# Train own word embedding\n",
    "# Use model trained on yelp for other data\n",
    "# Try out sentiments towards types of entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading required ML packages and functions\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, FunctionTransformer\n",
    "from sklearn.metrics import roc_curve, RocCurveDisplay, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold, GridSearchCV, RandomizedSearchCV, RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from scipy.stats import uniform, randint\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import gensim.downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Hamburg Yelp reviews\n",
    "yelp = pd.read_csv(\"data/yelp_reviews_hamburg_en.csv\", parse_dates=[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3025 entries, 0 to 3024\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype         \n",
      "---  ------  --------------  -----         \n",
      " 0   url     3025 non-null   object        \n",
      " 1   stars   3025 non-null   float64       \n",
      " 2   text    3025 non-null   object        \n",
      " 3   date    3025 non-null   datetime64[ns]\n",
      "dtypes: datetime64[ns](1), float64(1), object(2)\n",
      "memory usage: 94.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# Overview of the data\n",
    "yelp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.yelp.com/biz/il-buco-hamburg</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Fantastic little restaurant!Great staff and fo...</td>\n",
       "      <td>2017-08-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.yelp.com/biz/campus-suite-hamburg-7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>We went there to grab some breakfast. They are...</td>\n",
       "      <td>2015-09-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.yelp.com/biz/campus-suite-hamburg-7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Good coffee,  sandwiches,  and yogurts close t...</td>\n",
       "      <td>2016-01-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.yelp.com/biz/campus-suite-hamburg-7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>When XING handed out coupons for free coffee d...</td>\n",
       "      <td>2008-04-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.yelp.com/biz/campus-suite-hamburg-7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>I love Campus Suite franchise. after the Balza...</td>\n",
       "      <td>2010-01-15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               url  stars  \\\n",
       "0         https://www.yelp.com/biz/il-buco-hamburg    5.0   \n",
       "1  https://www.yelp.com/biz/campus-suite-hamburg-7    4.0   \n",
       "2  https://www.yelp.com/biz/campus-suite-hamburg-7    4.0   \n",
       "3  https://www.yelp.com/biz/campus-suite-hamburg-7    3.0   \n",
       "4  https://www.yelp.com/biz/campus-suite-hamburg-7    4.0   \n",
       "\n",
       "                                                text       date  \n",
       "0  Fantastic little restaurant!Great staff and fo... 2017-08-12  \n",
       "1  We went there to grab some breakfast. They are... 2015-09-29  \n",
       "2  Good coffee,  sandwiches,  and yogurts close t... 2016-01-13  \n",
       "3  When XING handed out coupons for free coffee d... 2008-04-24  \n",
       "4  I love Campus Suite franchise. after the Balza... 2010-01-15  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the top 5 rows\n",
    "yelp.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract name of the location from url\n",
    "yelp[\"name\"] = yelp.apply(lambda row: re.sub(\"https://www.yelp.com/biz/\", \"\", row[\"url\"]), axis=1)\n",
    "yelp = yelp.drop(columns=\"url\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Count'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inspect length of review text\n",
    "text_length_distribution = np.array([len(text) for text in yelp.text])\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=[15,8])\n",
    "sns.histplot(data = text_length_distribution)\n",
    "\n",
    "# Problem: Maximum length of Bert is 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading glove-wiki-gigaword-50\n",
      "Downloading glove-twitter-50\n",
      "Downloading word2vec-google-news-300\n"
     ]
    }
   ],
   "source": [
    "# Load potential sets of word vectors\n",
    "#list(gensim.downloader.info()['models'].keys())\n",
    "potential_wv = {}\n",
    "print(\"Downloading glove-wiki-gigaword-50\")\n",
    "potential_wv[\"wv_glove_wiki\"] = gensim.downloader.load('glove-wiki-gigaword-300')\n",
    "print(\"Downloading glove-twitter-50\")\n",
    "potential_wv[\"wv_glove_twitter\"] = gensim.downloader.load('glove-twitter-200')\n",
    "print(\"Downloading word2vec-google-news-300\")\n",
    "potential_wv[\"wv_word2vec\"] = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class tester():\n",
    "\n",
    "    def __init__(self):\n",
    "        print(\"init\")\n",
    "\n",
    "    def produce_embeddings(self, X, wv):\n",
    "        print(wv)\n",
    "        if wv not in [\"wv_glove_wiki\",\"wv_glove_twitter\",\"wv_word2vec\"]:\n",
    "            print(\"Not a valid set of word vectors - Choose one of the following\")\n",
    "            print([\"wv_glove_wiki\",\"wv_glove_twitter\",\"wv_word2vec\"])\n",
    "\n",
    "        vocabulary = set(potential_wv[wv].index_to_key)\n",
    "        wpt = nltk.WordPunctTokenizer()\n",
    "\n",
    "        def avg_embeddings(document):\n",
    "            words = wpt.tokenize(document)\n",
    "            invocab = [word for word in words if word in vocabulary]\n",
    "            avg = np.mean(potential_wv[wv][invocab], axis=0) if len(invocab) >= 1 else []\n",
    "            return avg\n",
    "\n",
    "        print(X.text.values)\n",
    "        doc_embeddings = [avg_embeddings(doc) for doc in X.text.values]\n",
    "        return doc_embeddings\n",
    "\n",
    "    def test_function_transformer(self):\n",
    "        new_transformer = FunctionTransformer(self.produce_embeddings, kw_args={'wv':\"wv_glove_wiki\"})\n",
    "        new_transformer.fit_transform(yelp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init\n"
     ]
    }
   ],
   "source": [
    "new_tester = tester()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wv_glove_wiki\n",
      "['fantastic little restaurantgreat staff food good time dining restaurant doesnt menu tells different food day problem making vegetarian option friend made fresh homemade pasta filled pear never tried fantasticthe area outside hansaplatz bit shady people sitting outside drinking day'\n",
      " 'went grab breakfast cafe also lunch small snacks chose soy yogurt nuts honey found refrigerated section offer two different flavors soy yogurt also quite big tasty looking selection sandwiches bagels display interior nice welcoming prices moderate okay others overpriced 2 fruit infused water great place quick breakfast lunch'\n",
      " 'good coffee sandwiches yogurts close marriott hotel much better value plenty space friendly staff'\n",
      " ...\n",
      " 'good chinese placethe food premium quality 4 persons took tsingtao menuit consisted soup springrolls duck beef chicken lamb course rice à gogo everything delicious sat table rotating part middle able pass around plates without making mess restaurant cheap worth every penny'\n",
      " 'tried seven different dim sum dishes half tasty authentic fried dumplings smelt already pork fat served didnt taste good either since regular hong kong singapore know taste addition amazed service either actually fairly unfriendly thats deduced another starnevertheless 10000km away hong kong authentic chinese food difficult get try chicken feed dim sum house really good tasty'\n",
      " 'dim sum ok main dish sweet sour terrible cheap low quality chicken sauce made love either 1190 real joke']\n"
     ]
    }
   ],
   "source": [
    "new_tester.test_function_transformer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analyser class\n",
    "class SentimentAnalyser():\n",
    "\n",
    "    def __init__(self, data, target, normalize = True):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.target_distribution = self.data[self.target].value_counts(normalize=True)\n",
    "\n",
    "        # Normalize the review texts\n",
    "        if normalize:\n",
    "            self.data[\"text\"] = self.normalize()\n",
    "\n",
    "        # Initialize container for experiment results\n",
    "        self.experiment_results = {}\n",
    "        self.__run = 1\n",
    "\n",
    "        # Set up experiments to conduct\n",
    "        self.experiments = [\n",
    "            {\n",
    "                'name': 'Logistic Regression',\n",
    "                'model': LogisticRegression(n_jobs = os.cpu_count() -1, solver=\"saga\", l1_ratio=0.5),\n",
    "                'params': {\n",
    "                    'preprocessor': [\n",
    "                        #CountVectorizer(min_df=1), \n",
    "                        #CountVectorizer(min_df=2), \n",
    "                        #CountVectorizer(min_df=3), \n",
    "                        #CountVectorizer(ngram_range=(1,2)),\n",
    "                        #TfidfVectorizer(norm= 'l1'),\n",
    "                        #TfidfVectorizer(norm = 'l2'),\n",
    "                        FunctionTransformer(self.produce_embeddings, kw_args={'wv':\"wv_glove_wiki\"}),\n",
    "                        FunctionTransformer(self.produce_embeddings, kw_args={'wv':\"wv_glove_twitter\"}),\n",
    "                        FunctionTransformer(self.produce_embeddings, kw_args={'wv':\"wv_word2vec\"})\n",
    "                        ],\n",
    "                    'oversampler': ['passthrough', SMOTE(random_state=33)],\n",
    "                    'estimator__penalty': [\"none\", \"l2\", \"l1\", \"elasticnet\"],\n",
    "                    'estimator__C': [0.5, 1, 2]\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'name': 'KNeighbors',\n",
    "                'model': KNeighborsClassifier(algorithm = \"auto\", n_jobs = os.cpu_count() -1),\n",
    "                'params': {\n",
    "                    'preprocessor': [\n",
    "                        CountVectorizer(min_df=1), \n",
    "                        CountVectorizer(min_df=2), \n",
    "                        CountVectorizer(min_df=3), \n",
    "                        CountVectorizer(ngram_range=(1,2)),\n",
    "                        TfidfVectorizer(norm= 'l1'),\n",
    "                        TfidfVectorizer(norm = 'l2'),\n",
    "                        FunctionTransformer(self.produce_embeddings, kw_args={'wv':\"wv_glove_wiki\"}),\n",
    "                        FunctionTransformer(self.produce_embeddings, kw_args={'wv':\"wv_glove_twitter\"}),\n",
    "                        FunctionTransformer(self.produce_embeddings, kw_args={'wv':\"wv_word2vec\"})\n",
    "                        ],\n",
    "                    'oversampler': ['passthrough', SMOTE(random_state=33)],\n",
    "                    'estimator__n_neighbors': randint(1, 10), \n",
    "                    'estimator__weights': [\"uniform\", \"distance\"], \n",
    "                    'estimator__metric': [\"euclidean\", \"manhattan\"]\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'name': 'Random Forest',\n",
    "                'model': RandomForestClassifier(n_jobs = os.cpu_count() -1, verbose = 1, random_state = 33),\n",
    "                'params': {\n",
    "                    'preprocessor': [\n",
    "                        CountVectorizer(min_df=1), \n",
    "                        CountVectorizer(min_df=2), \n",
    "                        CountVectorizer(min_df=3), \n",
    "                        CountVectorizer(ngram_range=(1,2)),\n",
    "                        TfidfVectorizer(norm= 'l1'),\n",
    "                        TfidfVectorizer(norm = 'l2'),\n",
    "                        FunctionTransformer(self.produce_embeddings, kw_args={'wv':\"wv_glove_wiki\"}),\n",
    "                        FunctionTransformer(self.produce_embeddings, kw_args={'wv':\"wv_glove_twitter\"}),\n",
    "                        FunctionTransformer(self.produce_embeddings, kw_args={'wv':\"wv_word2vec\"})\n",
    "                        ],\n",
    "                    'oversampler': ['passthrough', SMOTE(random_state=33)],\n",
    "                    'estimator__criterion': ['gini', 'entropy'],\n",
    "                    'estimator__n_estimators': randint(1, 100), \n",
    "                    'estimator__max_features': uniform(0.1,0.9),\n",
    "                    'estimator__class_weight': [None, 'balanced', 'balanced_subsample'],\n",
    "                    'estimator__ccp_alpha': uniform(0,0.1)\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'name': 'SVM',\n",
    "                'model': SVC(verbose = True, random_state = 33),\n",
    "                'params': {\n",
    "                    'preprocessor': [\n",
    "                        CountVectorizer(min_df=1), \n",
    "                        CountVectorizer(min_df=2), \n",
    "                        CountVectorizer(min_df=3), \n",
    "                        CountVectorizer(ngram_range=(1,2)),\n",
    "                        TfidfVectorizer(norm= 'l1'),\n",
    "                        TfidfVectorizer(norm = 'l2'),\n",
    "                        FunctionTransformer(self.produce_embeddings, kw_args={'wv':\"wv_glove_wiki\"}),\n",
    "                        FunctionTransformer(self.produce_embeddings, kw_args={'wv':\"wv_glove_twitter\"}),\n",
    "                        FunctionTransformer(self.produce_embeddings, kw_args={'wv':\"wv_word2vec\"})\n",
    "                        ],\n",
    "                    'oversampler': ['passthrough', SMOTE(random_state=33)],\n",
    "                    'estimator__C': uniform(0.001, 1),\n",
    "                    'estimator__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "                    'estimator__degree': randint(1, 5),\n",
    "                    'estimator__gamma': uniform(0.0001, 1),\n",
    "                    'estimator__class_weight': [None, 'balanced']\n",
    "                }\n",
    "            },\n",
    "            # When in doubt use XGBoost\n",
    "            {\n",
    "                'name': 'XGBoost',\n",
    "                'model': XGBClassifier(\n",
    "                    objective = \"multi:softprob\",\n",
    "                    num_class = 5, \n",
    "                    tree_method = \"auto\",\n",
    "                    eval_metric = \"auc\", \n",
    "                    verbosity = 1,\n",
    "                    use_label_encoder = False,\n",
    "                    random_state = 33),\n",
    "                'params': {\n",
    "                    'preprocessor': [\n",
    "                        CountVectorizer(min_df=1), \n",
    "                        CountVectorizer(min_df=2), \n",
    "                        CountVectorizer(min_df=3), \n",
    "                        CountVectorizer(ngram_range=(1,2)),\n",
    "                        TfidfVectorizer(norm= 'l1'),\n",
    "                        TfidfVectorizer(norm = 'l2'),\n",
    "                        FunctionTransformer(self.produce_embeddings, kw_args={'wv':\"wv_glove_wiki\"}),\n",
    "                        FunctionTransformer(self.produce_embeddings, kw_args={'wv':\"wv_glove_twitter\"}),\n",
    "                        FunctionTransformer(self.produce_embeddings, kw_args={'wv':\"wv_word2vec\"})\n",
    "                        ],\n",
    "                    'oversampler': ['passthrough', SMOTE(random_state=33)],\n",
    "                    'estimator__learning_rate': uniform(0.001, 0.5),\n",
    "                    'estimator__gamma': uniform(0, 0.5),\n",
    "                    'estimator__max_depth': randint(3, 10),\n",
    "                    'estimator__colsample_bytree': uniform(0.5, 0.5)\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "\n",
    "    def __normalize_document(self, doc, tokenizer, stop_words):\n",
    "        doc = re.sub(r'@[\\w]+', '', doc)          # replace user mentions\n",
    "        doc = re.sub(r'http[\\S]+', 'URL', doc)    # replace URLs\n",
    "        doc = re.sub(r'[^\\w\\s]', '', doc)         # keep words and spaces\n",
    "        doc = doc.lower()\n",
    "        doc = doc.strip()\n",
    "        tokens = tokenizer.tokenize(doc)\n",
    "        filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "        doc = ' '.join(filtered_tokens)\n",
    "        return doc\n",
    "\n",
    "    def normalize(self):\n",
    "        stop_words = nltk.corpus.stopwords.words('english')\n",
    "        wpt = nltk.WordPunctTokenizer()\n",
    "\n",
    "        return self.data.text.apply(lambda x: self.__normalize_document(x, wpt, stop_words))\n",
    "\n",
    "    def naive_baseline(self):\n",
    "        self.data[\"naive_baseline_prediction\"] = np.random.choice(self.target_distribution.index.to_numpy(), size = len(yelp), replace = True, p = self.target_distribution.values)\n",
    "\n",
    "    def vader_sentiment(self):\n",
    "        vader = SentimentIntensityAnalyzer()\n",
    "        self.data['vader_scores'] = self.data.text.apply(lambda x: vader.polarity_scores(x))\n",
    "        self.data['vader_compound'] = self.data.vader_scores.apply(lambda x: x['compound'])\n",
    "        self.data['vader_prediction'] = self.data.vader_compound.apply(lambda x: 1 if x >= 0 else 0)\n",
    "        pd.crosstab(self.data[self.target], self.data.vader_prediction, normalize='all')\n",
    "\n",
    "    def textblob_sentiment(self):\n",
    "        self.data['textblob_score'] = self.data.text.apply(\n",
    "            lambda review: TextBlob(review).sentiment[0])\n",
    "        self.data['textblob_prediction'] = self.data.textblob_score.apply(lambda x: 1 if x >= 0 else 0)\n",
    "        pd.crosstab(self.data[self.target], self.data.textblob_prediction, normalize='all')\n",
    "\n",
    "    def produce_embeddings(self, X, wv):\n",
    "        print(wv)\n",
    "        if wv not in [\"wv_glove_wiki\",\"wv_glove_twitter\",\"wv_word2vec\"]:\n",
    "            print(\"Not a valid set of word vectors - Choose one of the following\")\n",
    "            print([\"wv_glove_wiki\",\"wv_glove_twitter\",\"wv_word2vec\"])\n",
    "\n",
    "        vocabulary = set(potential_wv[wv].index_to_key)\n",
    "        wpt = nltk.WordPunctTokenizer()\n",
    "\n",
    "        def avg_embeddings(document):\n",
    "            words = wpt.tokenize(document)\n",
    "            invocab = [word for word in words if word in vocabulary]\n",
    "            avg = np.mean(potential_wv[wv][invocab], axis=0) if len(invocab) >= 1 else []\n",
    "            return avg\n",
    "\n",
    "        doc_embeddings = [avg_embeddings(doc) for doc in X.values]\n",
    "        print(doc_embeddings)\n",
    "        return doc_embeddings\n",
    "\n",
    "\n",
    "    def evaluate_classifier(self, name, model, params, iterations, cv_splits, cv_repeats, use_word_vectors = False):\n",
    "\n",
    "        class Debug(BaseEstimator, TransformerMixin):\n",
    "\n",
    "            def transform(self, X):\n",
    "                print(X)\n",
    "                print(X.shape)\n",
    "                self.shape = X.shape\n",
    "                # what other output you want\n",
    "                return X\n",
    "\n",
    "            def fit(self, X, y=None, **fit_params):\n",
    "                return self\n",
    "                \n",
    "        # Setting up the pipeline\n",
    "        if False:\n",
    "            pipeline = Pipeline([('oversampler', SMOTE()), ('estimator', model)])\n",
    "            input = self.produce_embeddings(use_word_vectors)\n",
    "            params.pop('wordcount__min_df')\n",
    "            params.pop('wordcount__ngram_range')\n",
    "            params.pop('tfidf')\n",
    "\n",
    "        # The preprocessor set here is just a default and gets overwritten by the possible preprocessors in the parameter space\n",
    "        pipeline = Pipeline([('preprocessor', CountVectorizer()), (\"debug\", Debug()), ('oversampler', SMOTE()), ('estimator', model)])\n",
    "\n",
    "        # Setting up the Cross validation\n",
    "        inner_cv = RepeatedStratifiedKFold(n_splits = cv_splits, n_repeats = cv_repeats, random_state = 33)\n",
    "        outer_cv = RepeatedStratifiedKFold(n_splits = cv_splits, n_repeats = cv_repeats, random_state = 33)\n",
    "\n",
    "        # Doing hyperparameter optimization\n",
    "        optimization = RandomizedSearchCV(\n",
    "            estimator = pipeline,\n",
    "            param_distributions = params,\n",
    "            scoring = ['accuracy', 'balanced_accuracy', 'f1_weighted', 'roc_auc_ovo_weighted'],\n",
    "            cv = inner_cv,\n",
    "            refit = 'roc_auc_ovo_weighted',\n",
    "            n_iter = iterations,\n",
    "            n_jobs = os.cpu_count() -1,\n",
    "            verbose = 1,\n",
    "            random_state = 33)\n",
    "\n",
    "        #print(input.shape)\n",
    "        #print(input)\n",
    "        #print(self.data[self.target].shape)\n",
    "        #print(self.data[self.target])\n",
    "        optimization.fit(self.data.text, self.data[self.target])\n",
    "\n",
    "        # Evaluating the best model on the outer cross validation\n",
    "        performance_estimation = cross_validate(\n",
    "            estimator = optimization,\n",
    "            X = self.data.text,\n",
    "            y = self.data[self.target],\n",
    "            scoring = ['accuracy', 'balanced_accuracy', 'f1_weighted', 'roc_auc_ovo_weighted'],\n",
    "            cv = outer_cv,\n",
    "            n_jobs = os.cpu_count() -1)\n",
    "\n",
    "        return(\n",
    "        {\n",
    "            'name': name,\n",
    "            'optimization_cv_results': pd.DataFrame(optimization.cv_results_),\n",
    "            'best_params': optimization.best_params_,\n",
    "            'best_model': optimization.best_estimator_,\n",
    "            'acc': performance_estimation['test_accuracy'],\n",
    "            'balanced_acc': performance_estimation['test_balanced_accuracy'],\n",
    "            'f1': performance_estimation['test_f1_weighted'],\n",
    "            'roc_auc': performance_estimation['test_roc_auc_ovo_weighted']\n",
    "        }\n",
    "        )\n",
    "\n",
    "    def run_experiments(self, iterations, cv_splits, cv_repeats, models = None):\n",
    "\n",
    "        self.experiment_results[f\"run_{self.__run}\"] = {}\n",
    "\n",
    "        # Determine which models to test\n",
    "        if models:\n",
    "            experiments = [experiment for experiment in self.experiments if experiment[\"name\"] in models]\n",
    "        else:\n",
    "            experiments = self.experiments\n",
    "\n",
    "        # Run experiment per model type\n",
    "        for experiment in experiments:\n",
    "\n",
    "            # Skip SVM\n",
    "            if experiment['name'] == 'SVM':\n",
    "                continue\n",
    "\n",
    "            start_time= time.time()\n",
    "\n",
    "            print()\n",
    "            print()\n",
    "            print(experiment['name'])\n",
    "            print(\"-----------------\")\n",
    "\n",
    "            self.experiment_results[f\"run_{self.__run}\"][experiment['name']] = self.evaluate_classifier(\n",
    "                    name = experiment['name'],\n",
    "                    model = experiment['model'],\n",
    "                    params = experiment['params'],\n",
    "                    iterations = iterations,\n",
    "                    cv_splits = cv_splits,\n",
    "                    cv_repeats = cv_repeats\n",
    "                )\n",
    "\n",
    "            end_time = time.time() - start_time\n",
    "            print(f'Time: {int(round(end_time, 1))} seconds ({int(round(end_time/60, 1))} minutes)')\n",
    "\n",
    "        self.__run += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages of pipeline\n",
    "\n",
    "Convenience and encapsulation\n",
    "You only have to call fit and predict once on your data to fit a whole sequence of estimators.\n",
    "\n",
    "Joint parameter selection\n",
    "You can grid search over parameters of all estimators in the pipeline at once.\n",
    "\n",
    "Safety\n",
    "Pipelines help avoid leaking statistics from your test data into the trained model in cross-validation, by ensuring that the same samples are used to train the transformers and predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Sentiment Analyser\n",
    "sentiment = SentimentAnalyser(yelp, \"stars\", normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment.naive_baseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment.vader_sentiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment.textblob_sentiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Logistic Regression\n",
      "-----------------\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n"
     ]
    }
   ],
   "source": [
    "sentiment.run_experiments(3, 2, 1, [\"Logistic Regression\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ValentinStudium\\Envs\\sma\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "c:\\Users\\ValentinStudium\\Envs\\sma\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(l1_ratio=0.5, n_jobs=7, solver=&#x27;saga&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(l1_ratio=0.5, n_jobs=7, solver=&#x27;saga&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(l1_ratio=0.5, n_jobs=7, solver='saga')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_data = CountVectorizer().fit_transform(yelp.text)\n",
    "model = LogisticRegression(n_jobs = os.cpu_count() -1, solver=\"saga\", l1_ratio=0.5)\n",
    "model.fit(X = transformed_data, y = yelp.stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9550413223140496"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X = transformed_data, y = yelp.stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ValentinStudium\\Envs\\sma\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "c:\\Users\\ValentinStudium\\Envs\\sma\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-11 {color: black;background-color: white;}#sk-container-id-11 pre{padding: 0;}#sk-container-id-11 div.sk-toggleable {background-color: white;}#sk-container-id-11 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-11 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-11 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-11 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-11 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-11 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-11 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-11 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-11 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-11 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-11 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-11 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-11 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-11 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-11 div.sk-item {position: relative;z-index: 1;}#sk-container-id-11 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-11 div.sk-item::before, #sk-container-id-11 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-11 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-11 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-11 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-11 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-11 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-11 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-11 div.sk-label-container {text-align: center;}#sk-container-id-11 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-11 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-11\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;, CountVectorizer()), (&#x27;oversampler&#x27;, SMOTE()),\n",
       "                (&#x27;estimator&#x27;,\n",
       "                 LogisticRegression(l1_ratio=0.5, max_iter=1000, n_jobs=7,\n",
       "                                    solver=&#x27;saga&#x27;))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-40\" type=\"checkbox\" ><label for=\"sk-estimator-id-40\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;, CountVectorizer()), (&#x27;oversampler&#x27;, SMOTE()),\n",
       "                (&#x27;estimator&#x27;,\n",
       "                 LogisticRegression(l1_ratio=0.5, max_iter=1000, n_jobs=7,\n",
       "                                    solver=&#x27;saga&#x27;))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-41\" type=\"checkbox\" ><label for=\"sk-estimator-id-41\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-42\" type=\"checkbox\" ><label for=\"sk-estimator-id-42\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SMOTE</label><div class=\"sk-toggleable__content\"><pre>SMOTE()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-43\" type=\"checkbox\" ><label for=\"sk-estimator-id-43\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(l1_ratio=0.5, max_iter=1000, n_jobs=7, solver=&#x27;saga&#x27;)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('preprocessor', CountVectorizer()), ('oversampler', SMOTE()),\n",
       "                ('estimator',\n",
       "                 LogisticRegression(l1_ratio=0.5, max_iter=1000, n_jobs=7,\n",
       "                                    solver='saga'))])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([('preprocessor', CountVectorizer()), ('oversampler', SMOTE()), ('estimator',  LogisticRegression(n_jobs = os.cpu_count() -1, solver=\"saga\", l1_ratio=0.5, max_iter=1000))])\n",
    "pipeline.fit(yelp.text, yelp.stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9920661157024794"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.score(yelp.text, yelp.stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "multi_class must be in ('ovo', 'ovr')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ValentinStudium\\Nextcloud\\Studium\\09 Social Media Analytics\\Sentiment-Analysis\\Sentiment_Analysis.ipynb Cell 25'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000054?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m roc_auc_score\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000054?line=1'>2</a>\u001b[0m roc_auc_score(yelp\u001b[39m.\u001b[39;49mstars, pipeline\u001b[39m.\u001b[39;49mpredict_proba(yelp\u001b[39m.\u001b[39;49mtext))\n",
      "File \u001b[1;32mc:\\Users\\ValentinStudium\\Envs\\sma\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:562\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[1;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/metrics/_ranking.py?line=554'>555</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/metrics/_ranking.py?line=555'>556</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mPartial AUC computation not available in \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/metrics/_ranking.py?line=556'>557</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mmulticlass setting, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mmax_fpr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/metrics/_ranking.py?line=557'>558</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m set to `None`, received `max_fpr=\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/metrics/_ranking.py?line=558'>559</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39minstead\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(max_fpr)\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/metrics/_ranking.py?line=559'>560</a>\u001b[0m         )\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/metrics/_ranking.py?line=560'>561</a>\u001b[0m     \u001b[39mif\u001b[39;00m multi_class \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/metrics/_ranking.py?line=561'>562</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mmulti_class must be in (\u001b[39m\u001b[39m'\u001b[39m\u001b[39movo\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39movr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/metrics/_ranking.py?line=562'>563</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _multiclass_roc_auc_score(\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/metrics/_ranking.py?line=563'>564</a>\u001b[0m         y_true, y_score, labels, multi_class, average, sample_weight\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/metrics/_ranking.py?line=564'>565</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/metrics/_ranking.py?line=565'>566</a>\u001b[0m \u001b[39melif\u001b[39;00m y_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: multi_class must be in ('ovo', 'ovr')"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(yelp.stars, pipeline.predict_proba(yelp.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ValentinStudium\\Envs\\sma\\lib\\site-packages\\sklearn\\model_selection\\_search.py:306: UserWarning: The total space of parameters 2 is smaller than n_iter=5. Running 2 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ValentinStudium\\Envs\\sma\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-13 {color: black;background-color: white;}#sk-container-id-13 pre{padding: 0;}#sk-container-id-13 div.sk-toggleable {background-color: white;}#sk-container-id-13 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-13 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-13 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-13 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-13 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-13 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-13 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-13 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-13 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-13 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-13 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-13 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-13 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-13 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-13 div.sk-item {position: relative;z-index: 1;}#sk-container-id-13 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-13 div.sk-item::before, #sk-container-id-13 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-13 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-13 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-13 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-13 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-13 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-13 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-13 div.sk-label-container {text-align: center;}#sk-container-id-13 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-13 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-13\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=RepeatedKFold(n_repeats=1, n_splits=2, random_state=33),\n",
       "                   estimator=Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                                              CountVectorizer()),\n",
       "                                             (&#x27;oversampler&#x27;, SMOTE()),\n",
       "                                             (&#x27;estimator&#x27;,\n",
       "                                              LogisticRegression(l1_ratio=0.5,\n",
       "                                                                 max_iter=1000,\n",
       "                                                                 n_jobs=7,\n",
       "                                                                 solver=&#x27;saga&#x27;))]),\n",
       "                   n_iter=5, n_jobs=7,\n",
       "                   param_distributions={&#x27;oversampler&#x27;: [&#x27;passthrough&#x27;,\n",
       "                                                        SMOTE(random_state=33)]},\n",
       "                   random_state=33, refit=&#x27;roc_auc_ovo&#x27;,\n",
       "                   scoring=[&#x27;accuracy&#x27;, &#x27;balanced_accuracy&#x27;, &#x27;f1_weighted&#x27;,\n",
       "                            &#x27;roc_auc_ovo&#x27;],\n",
       "                   verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-49\" type=\"checkbox\" ><label for=\"sk-estimator-id-49\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=RepeatedKFold(n_repeats=1, n_splits=2, random_state=33),\n",
       "                   estimator=Pipeline(steps=[(&#x27;preprocessor&#x27;,\n",
       "                                              CountVectorizer()),\n",
       "                                             (&#x27;oversampler&#x27;, SMOTE()),\n",
       "                                             (&#x27;estimator&#x27;,\n",
       "                                              LogisticRegression(l1_ratio=0.5,\n",
       "                                                                 max_iter=1000,\n",
       "                                                                 n_jobs=7,\n",
       "                                                                 solver=&#x27;saga&#x27;))]),\n",
       "                   n_iter=5, n_jobs=7,\n",
       "                   param_distributions={&#x27;oversampler&#x27;: [&#x27;passthrough&#x27;,\n",
       "                                                        SMOTE(random_state=33)]},\n",
       "                   random_state=33, refit=&#x27;roc_auc_ovo&#x27;,\n",
       "                   scoring=[&#x27;accuracy&#x27;, &#x27;balanced_accuracy&#x27;, &#x27;f1_weighted&#x27;,\n",
       "                            &#x27;roc_auc_ovo&#x27;],\n",
       "                   verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-50\" type=\"checkbox\" ><label for=\"sk-estimator-id-50\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;, CountVectorizer()), (&#x27;oversampler&#x27;, SMOTE()),\n",
       "                (&#x27;estimator&#x27;,\n",
       "                 LogisticRegression(l1_ratio=0.5, max_iter=1000, n_jobs=7,\n",
       "                                    solver=&#x27;saga&#x27;))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-51\" type=\"checkbox\" ><label for=\"sk-estimator-id-51\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-52\" type=\"checkbox\" ><label for=\"sk-estimator-id-52\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SMOTE</label><div class=\"sk-toggleable__content\"><pre>SMOTE()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-53\" type=\"checkbox\" ><label for=\"sk-estimator-id-53\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(l1_ratio=0.5, max_iter=1000, n_jobs=7, solver=&#x27;saga&#x27;)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=RepeatedKFold(n_repeats=1, n_splits=2, random_state=33),\n",
       "                   estimator=Pipeline(steps=[('preprocessor',\n",
       "                                              CountVectorizer()),\n",
       "                                             ('oversampler', SMOTE()),\n",
       "                                             ('estimator',\n",
       "                                              LogisticRegression(l1_ratio=0.5,\n",
       "                                                                 max_iter=1000,\n",
       "                                                                 n_jobs=7,\n",
       "                                                                 solver='saga'))]),\n",
       "                   n_iter=5, n_jobs=7,\n",
       "                   param_distributions={'oversampler': ['passthrough',\n",
       "                                                        SMOTE(random_state=33)]},\n",
       "                   random_state=33, refit='roc_auc_ovo',\n",
       "                   scoring=['accuracy', 'balanced_accuracy', 'f1_weighted',\n",
       "                            'roc_auc_ovo'],\n",
       "                   verbose=1)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    'oversampler': ['passthrough', SMOTE(random_state=33)]\n",
    "}\n",
    "\n",
    "inner_cv = RepeatedKFold(n_splits = 2, n_repeats = 1, random_state = 33)\n",
    "outer_cv = RepeatedKFold(n_splits = 2, n_repeats = 1, random_state = 33)\n",
    "\n",
    "# Doing hyperparameter optimization\n",
    "optimization = RandomizedSearchCV(\n",
    "    estimator = pipeline,\n",
    "    param_distributions = params,\n",
    "    scoring = ['accuracy', 'balanced_accuracy', 'f1_weighted', 'roc_auc_ovo_weighted'],\n",
    "    cv = inner_cv,\n",
    "    refit = 'roc_auc_ovo_weighted',\n",
    "    n_iter = 5,\n",
    "    n_jobs = os.cpu_count() -1,\n",
    "    verbose = 1,\n",
    "    random_state = 33)\n",
    "\n",
    "#print(input.shape)\n",
    "#print(input)\n",
    "#print(self.data[self.target].shape)\n",
    "#print(self.data[self.target])\n",
    "optimization.fit(yelp.text, yelp.stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([3.50885952, 9.40877807]),\n",
       " 'std_fit_time': array([0.62279022, 0.69137371]),\n",
       " 'mean_score_time': array([0.22923148, 0.17882931]),\n",
       " 'std_score_time': array([0.06071699, 0.02544558]),\n",
       " 'param_oversampler': masked_array(data=['passthrough', SMOTE(random_state=33)],\n",
       "              mask=[False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'oversampler': 'passthrough'},\n",
       "  {'oversampler': SMOTE(random_state=33)}],\n",
       " 'split0_test_accuracy': array([0.53139458, 0.48182419]),\n",
       " 'split1_test_accuracy': array([0.52910053, 0.49801587]),\n",
       " 'mean_test_accuracy': array([0.53024755, 0.48992003]),\n",
       " 'std_test_accuracy': array([0.00114703, 0.00809584]),\n",
       " 'rank_test_accuracy': array([1, 2]),\n",
       " 'split0_test_balanced_accuracy': array([0.38302699, 0.4135917 ]),\n",
       " 'split1_test_balanced_accuracy': array([0.35831764, 0.38671666]),\n",
       " 'mean_test_balanced_accuracy': array([0.37067231, 0.40015418]),\n",
       " 'std_test_balanced_accuracy': array([0.01235468, 0.01343752]),\n",
       " 'rank_test_balanced_accuracy': array([2, 1]),\n",
       " 'split0_test_f1_weighted': array([0.51624354, 0.48843744]),\n",
       " 'split1_test_f1_weighted': array([0.50788505, 0.49736243]),\n",
       " 'mean_test_f1_weighted': array([0.5120643 , 0.49289994]),\n",
       " 'std_test_f1_weighted': array([0.00417925, 0.00446249]),\n",
       " 'rank_test_f1_weighted': array([1, 2]),\n",
       " 'split0_test_roc_auc_ovo': array([0.72573007, 0.70266252]),\n",
       " 'split1_test_roc_auc_ovo': array([0.69427455, 0.67347176]),\n",
       " 'mean_test_roc_auc_ovo': array([0.71000231, 0.68806714]),\n",
       " 'std_test_roc_auc_ovo': array([0.01572776, 0.01459538]),\n",
       " 'rank_test_roc_auc_ovo': array([1, 2])}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimization.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ValentinStudium\\Envs\\sma\\lib\\site-packages\\sklearn\\model_selection\\_search.py:306: UserWarning: The total space of parameters 2 is smaller than n_iter=5. Running 2 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ValentinStudium\\Envs\\sma\\lib\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [nan nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\ValentinStudium\\Envs\\sma\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "c:\\Users\\ValentinStudium\\Envs\\sma\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def produce_embeddings(X, wv):\n",
    "    print(wv)\n",
    "    if wv not in [\"wv_glove_wiki\",\"wv_glove_twitter\",\"wv_word2vec\"]:\n",
    "        print(\"Not a valid set of word vectors - Choose one of the following\")\n",
    "        print([\"wv_glove_wiki\",\"wv_glove_twitter\",\"wv_word2vec\"])\n",
    "\n",
    "    vocabulary = set(potential_wv[wv].index_to_key)\n",
    "    wpt = nltk.WordPunctTokenizer()\n",
    "\n",
    "    def avg_embeddings(document):\n",
    "        words = wpt.tokenize(document)\n",
    "        invocab = [word for word in words if word in vocabulary]\n",
    "        avg = np.mean(potential_wv[wv][invocab], axis=0) if len(invocab) >= 1 else []\n",
    "        return avg\n",
    "\n",
    "    doc_embeddings = [avg_embeddings(doc) for doc in X.values]\n",
    "    return doc_embeddings\n",
    "\n",
    "params = {\n",
    "    'preprocessor': [\n",
    "        CountVectorizer(min_df=1), \n",
    "        CountVectorizer(min_df=2), \n",
    "        CountVectorizer(min_df=3), \n",
    "        CountVectorizer(ngram_range=(1,2)),\n",
    "        TfidfVectorizer(norm= 'l1'),\n",
    "        TfidfVectorizer(norm = 'l2'),\n",
    "        FunctionTransformer(produce_embeddings, kw_args={'wv':\"wv_glove_wiki\"}),\n",
    "        FunctionTransformer(produce_embeddings, kw_args={'wv':\"wv_glove_twitter\"}),\n",
    "        FunctionTransformer(produce_embeddings, kw_args={'wv':\"wv_word2vec\"})\n",
    "        ],\n",
    "    'oversampler': ['passthrough', SMOTE(random_state=33)],\n",
    "    'estimator__penalty': [\"none\", \"l2\", \"l1\", \"elasticnet\"],\n",
    "    'estimator__C': [0.5, 1, 2]\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'oversampler': ['passthrough', SMOTE(random_state=33)]\n",
    "}\n",
    "\n",
    "inner_cv = RepeatedStratifiedKFold(n_splits = 2, n_repeats = 1, random_state = 33)\n",
    "outer_cv = RepeatedStratifiedKFold(n_splits = 2, n_repeats = 1, random_state = 33)\n",
    "\n",
    "# Doing hyperparameter optimization\n",
    "optimization = RandomizedSearchCV(\n",
    "    estimator = pipeline,\n",
    "    param_distributions = params,\n",
    "    scoring = ['accuracy', 'balanced_accuracy', 'f1', 'roc_auc'],\n",
    "    cv = inner_cv,\n",
    "    refit = 'roc_auc',\n",
    "    n_iter = 5,\n",
    "    n_jobs = os.cpu_count() -1,\n",
    "    verbose = 1,\n",
    "    random_state = 33)\n",
    "\n",
    "#print(input.shape)\n",
    "#print(input)\n",
    "#print(self.data[self.target].shape)\n",
    "#print(self.data[self.target])\n",
    "optimization.fit(yelp.text, yelp.stars)\n",
    "\n",
    "# Evaluating the best model on the outer cross validation\n",
    "performance_estimation = cross_validate(\n",
    "    estimator = optimization,\n",
    "    X = yelp.text,\n",
    "    y = yelp.stars,\n",
    "    scoring = ['accuracy', 'balanced_accuracy', 'f1', 'roc_auc'],\n",
    "    cv = outer_cv,\n",
    "    n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.5789814 , 1.25538003]),\n",
       " 'std_fit_time': array([0.13088441, 0.00751984]),\n",
       " 'mean_score_time': array([0.08575821, 0.0890162 ]),\n",
       " 'std_score_time': array([0.00524211, 0.00301719]),\n",
       " 'param_oversampler': masked_array(data=['passthrough', SMOTE(random_state=33)],\n",
       "              mask=[False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'oversampler': 'passthrough'},\n",
       "  {'oversampler': SMOTE(random_state=33)}],\n",
       " 'split0_test_accuracy': array([nan, nan]),\n",
       " 'split1_test_accuracy': array([nan, nan]),\n",
       " 'mean_test_accuracy': array([nan, nan]),\n",
       " 'std_test_accuracy': array([nan, nan]),\n",
       " 'rank_test_accuracy': array([1, 2]),\n",
       " 'split0_test_balanced_accuracy': array([nan, nan]),\n",
       " 'split1_test_balanced_accuracy': array([nan, nan]),\n",
       " 'mean_test_balanced_accuracy': array([nan, nan]),\n",
       " 'std_test_balanced_accuracy': array([nan, nan]),\n",
       " 'rank_test_balanced_accuracy': array([1, 2]),\n",
       " 'split0_test_f1': array([nan, nan]),\n",
       " 'split1_test_f1': array([nan, nan]),\n",
       " 'mean_test_f1': array([nan, nan]),\n",
       " 'std_test_f1': array([nan, nan]),\n",
       " 'rank_test_f1': array([1, 2]),\n",
       " 'split0_test_roc_auc': array([nan, nan]),\n",
       " 'split1_test_roc_auc': array([nan, nan]),\n",
       " 'mean_test_roc_auc': array([nan, nan]),\n",
       " 'std_test_roc_auc': array([nan, nan]),\n",
       " 'rank_test_roc_auc': array([1, 2])}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimization.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_preprocessor</th>\n",
       "      <th>param_oversampler</th>\n",
       "      <th>param_estimator__penalty</th>\n",
       "      <th>param_estimator__C</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_accuracy</th>\n",
       "      <th>...</th>\n",
       "      <th>split0_test_f1</th>\n",
       "      <th>split1_test_f1</th>\n",
       "      <th>mean_test_f1</th>\n",
       "      <th>std_test_f1</th>\n",
       "      <th>rank_test_f1</th>\n",
       "      <th>split0_test_roc_auc</th>\n",
       "      <th>split1_test_roc_auc</th>\n",
       "      <th>mean_test_roc_auc</th>\n",
       "      <th>std_test_roc_auc</th>\n",
       "      <th>rank_test_roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.717501</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>0.117499</td>\n",
       "      <td>0.003503</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>passthrough</td>\n",
       "      <td>none</td>\n",
       "      <td>2</td>\n",
       "      <td>{'preprocessor': CountVectorizer(), 'oversampl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19.886458</td>\n",
       "      <td>0.154257</td>\n",
       "      <td>0.125498</td>\n",
       "      <td>0.019500</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>SMOTE(random_state=33)</td>\n",
       "      <td>l1</td>\n",
       "      <td>2</td>\n",
       "      <td>{'preprocessor': CountVectorizer(), 'oversampl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.337158</td>\n",
       "      <td>0.070497</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>FunctionTransformer(func=&lt;bound method Sentime...</td>\n",
       "      <td>SMOTE(random_state=33)</td>\n",
       "      <td>elasticnet</td>\n",
       "      <td>2</td>\n",
       "      <td>{'preprocessor': FunctionTransformer(func=&lt;bou...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.607074</td>\n",
       "      <td>1.314542</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>FunctionTransformer(func=&lt;bound method Sentime...</td>\n",
       "      <td>SMOTE(random_state=33)</td>\n",
       "      <td>none</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'preprocessor': FunctionTransformer(func=&lt;bou...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.117513</td>\n",
       "      <td>1.867869</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>FunctionTransformer(func=&lt;bound method Sentime...</td>\n",
       "      <td>passthrough</td>\n",
       "      <td>none</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'preprocessor': FunctionTransformer(func=&lt;bou...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.996702</td>\n",
       "      <td>0.037999</td>\n",
       "      <td>0.209499</td>\n",
       "      <td>0.017500</td>\n",
       "      <td>TfidfVectorizer(norm='l1')</td>\n",
       "      <td>passthrough</td>\n",
       "      <td>l1</td>\n",
       "      <td>2</td>\n",
       "      <td>{'preprocessor': TfidfVectorizer(norm='l1'), '...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20.287348</td>\n",
       "      <td>0.243974</td>\n",
       "      <td>0.104000</td>\n",
       "      <td>0.019998</td>\n",
       "      <td>CountVectorizer(min_df=3)</td>\n",
       "      <td>SMOTE(random_state=33)</td>\n",
       "      <td>elasticnet</td>\n",
       "      <td>1</td>\n",
       "      <td>{'preprocessor': CountVectorizer(min_df=3), 'o...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.538431</td>\n",
       "      <td>0.144711</td>\n",
       "      <td>0.286506</td>\n",
       "      <td>0.023013</td>\n",
       "      <td>TfidfVectorizer(norm='l1')</td>\n",
       "      <td>SMOTE(random_state=33)</td>\n",
       "      <td>l2</td>\n",
       "      <td>2</td>\n",
       "      <td>{'preprocessor': TfidfVectorizer(norm='l1'), '...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>33.105094</td>\n",
       "      <td>0.957821</td>\n",
       "      <td>0.070483</td>\n",
       "      <td>0.007513</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>SMOTE(random_state=33)</td>\n",
       "      <td>elasticnet</td>\n",
       "      <td>2</td>\n",
       "      <td>{'preprocessor': CountVectorizer(), 'oversampl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.031324</td>\n",
       "      <td>0.361381</td>\n",
       "      <td>0.193499</td>\n",
       "      <td>0.022498</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>passthrough</td>\n",
       "      <td>elasticnet</td>\n",
       "      <td>2</td>\n",
       "      <td>{'preprocessor': TfidfVectorizer(), 'oversampl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       0.717501      0.000503         0.117499        0.003503   \n",
       "1      19.886458      0.154257         0.125498        0.019500   \n",
       "2       9.337158      0.070497         0.000000        0.000000   \n",
       "3      13.607074      1.314542         0.000000        0.000000   \n",
       "4      13.117513      1.867869         0.000000        0.000000   \n",
       "5       1.996702      0.037999         0.209499        0.017500   \n",
       "6      20.287348      0.243974         0.104000        0.019998   \n",
       "7       1.538431      0.144711         0.286506        0.023013   \n",
       "8      33.105094      0.957821         0.070483        0.007513   \n",
       "9       4.031324      0.361381         0.193499        0.022498   \n",
       "\n",
       "                                  param_preprocessor       param_oversampler  \\\n",
       "0                                  CountVectorizer()             passthrough   \n",
       "1                                  CountVectorizer()  SMOTE(random_state=33)   \n",
       "2  FunctionTransformer(func=<bound method Sentime...  SMOTE(random_state=33)   \n",
       "3  FunctionTransformer(func=<bound method Sentime...  SMOTE(random_state=33)   \n",
       "4  FunctionTransformer(func=<bound method Sentime...             passthrough   \n",
       "5                         TfidfVectorizer(norm='l1')             passthrough   \n",
       "6                          CountVectorizer(min_df=3)  SMOTE(random_state=33)   \n",
       "7                         TfidfVectorizer(norm='l1')  SMOTE(random_state=33)   \n",
       "8                                  CountVectorizer()  SMOTE(random_state=33)   \n",
       "9                                  TfidfVectorizer()             passthrough   \n",
       "\n",
       "  param_estimator__penalty param_estimator__C  \\\n",
       "0                     none                  2   \n",
       "1                       l1                  2   \n",
       "2               elasticnet                  2   \n",
       "3                     none                0.5   \n",
       "4                     none                0.5   \n",
       "5                       l1                  2   \n",
       "6               elasticnet                  1   \n",
       "7                       l2                  2   \n",
       "8               elasticnet                  2   \n",
       "9               elasticnet                  2   \n",
       "\n",
       "                                              params  split0_test_accuracy  \\\n",
       "0  {'preprocessor': CountVectorizer(), 'oversampl...                   NaN   \n",
       "1  {'preprocessor': CountVectorizer(), 'oversampl...                   NaN   \n",
       "2  {'preprocessor': FunctionTransformer(func=<bou...                   NaN   \n",
       "3  {'preprocessor': FunctionTransformer(func=<bou...                   NaN   \n",
       "4  {'preprocessor': FunctionTransformer(func=<bou...                   NaN   \n",
       "5  {'preprocessor': TfidfVectorizer(norm='l1'), '...                   NaN   \n",
       "6  {'preprocessor': CountVectorizer(min_df=3), 'o...                   NaN   \n",
       "7  {'preprocessor': TfidfVectorizer(norm='l1'), '...                   NaN   \n",
       "8  {'preprocessor': CountVectorizer(), 'oversampl...                   NaN   \n",
       "9  {'preprocessor': TfidfVectorizer(), 'oversampl...                   NaN   \n",
       "\n",
       "   ...  split0_test_f1  split1_test_f1  mean_test_f1  std_test_f1  \\\n",
       "0  ...             NaN             NaN           NaN          NaN   \n",
       "1  ...             NaN             NaN           NaN          NaN   \n",
       "2  ...             NaN             NaN           NaN          NaN   \n",
       "3  ...             NaN             NaN           NaN          NaN   \n",
       "4  ...             NaN             NaN           NaN          NaN   \n",
       "5  ...             NaN             NaN           NaN          NaN   \n",
       "6  ...             NaN             NaN           NaN          NaN   \n",
       "7  ...             NaN             NaN           NaN          NaN   \n",
       "8  ...             NaN             NaN           NaN          NaN   \n",
       "9  ...             NaN             NaN           NaN          NaN   \n",
       "\n",
       "   rank_test_f1  split0_test_roc_auc  split1_test_roc_auc  mean_test_roc_auc  \\\n",
       "0             1                  NaN                  NaN                NaN   \n",
       "1             2                  NaN                  NaN                NaN   \n",
       "2             3                  NaN                  NaN                NaN   \n",
       "3             4                  NaN                  NaN                NaN   \n",
       "4             5                  NaN                  NaN                NaN   \n",
       "5             6                  NaN                  NaN                NaN   \n",
       "6             7                  NaN                  NaN                NaN   \n",
       "7             8                  NaN                  NaN                NaN   \n",
       "8             9                  NaN                  NaN                NaN   \n",
       "9            10                  NaN                  NaN                NaN   \n",
       "\n",
       "   std_test_roc_auc  rank_test_roc_auc  \n",
       "0               NaN                  1  \n",
       "1               NaN                  2  \n",
       "2               NaN                  3  \n",
       "3               NaN                  4  \n",
       "4               NaN                  5  \n",
       "5               NaN                  6  \n",
       "6               NaN                  7  \n",
       "7               NaN                  8  \n",
       "8               NaN                  9  \n",
       "9               NaN                 10  \n",
       "\n",
       "[10 rows x 29 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment.experiment_results[\"run_1\"][\"Logistic Regression\"][\"optimization_cv_results\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment.run_experiments(10, 3, 1, [\"\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment.experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment analysis on the document level\n",
    "\n",
    "# 1. Rule based\n",
    "    # Vader\n",
    "    # Text Blob\n",
    "# 2. ML based - Classification or Regression?\n",
    "    # own feature engineering plus defined models\n",
    "        # bag of words\n",
    "        # TD-IF\n",
    "        # word vectors\n",
    "    # Transformers"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2822d77ee51dbb640527abe329425380abdeab763840ac762f0d0bc769537fcd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('sma')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
