{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portfolio-Exam Part I - Sentiment Analysis\n",
    "\n",
    "* Social Media Analytics - MADS-SMA\n",
    "* Valentin Werger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different approaches\n",
    "# Train own word embedding\n",
    "# Use model trained on yelp for other data\n",
    "# Try out sentiments towards types of entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading required ML packages and functions\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, MinMaxScaler, FunctionTransformer\n",
    "from sklearn.metrics import roc_curve, RocCurveDisplay, confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold, GridSearchCV, RandomizedSearchCV, RepeatedKFold\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from scipy.stats import uniform, randint\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import gensim.downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Hamburg Yelp reviews\n",
    "yelp = pd.read_csv(\"data/yelp_reviews_hamburg_en.csv\", parse_dates=[\"date\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3025 entries, 0 to 3024\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype         \n",
      "---  ------  --------------  -----         \n",
      " 0   url     3025 non-null   object        \n",
      " 1   stars   3025 non-null   float64       \n",
      " 2   text    3025 non-null   object        \n",
      " 3   date    3025 non-null   datetime64[ns]\n",
      "dtypes: datetime64[ns](1), float64(1), object(2)\n",
      "memory usage: 94.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# Overview of the data\n",
    "yelp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.yelp.com/biz/il-buco-hamburg</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Fantastic little restaurant!Great staff and fo...</td>\n",
       "      <td>2017-08-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.yelp.com/biz/campus-suite-hamburg-7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>We went there to grab some breakfast. They are...</td>\n",
       "      <td>2015-09-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.yelp.com/biz/campus-suite-hamburg-7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>Good coffee,  sandwiches,  and yogurts close t...</td>\n",
       "      <td>2016-01-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.yelp.com/biz/campus-suite-hamburg-7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>When XING handed out coupons for free coffee d...</td>\n",
       "      <td>2008-04-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.yelp.com/biz/campus-suite-hamburg-7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>I love Campus Suite franchise. after the Balza...</td>\n",
       "      <td>2010-01-15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               url  stars  \\\n",
       "0         https://www.yelp.com/biz/il-buco-hamburg    5.0   \n",
       "1  https://www.yelp.com/biz/campus-suite-hamburg-7    4.0   \n",
       "2  https://www.yelp.com/biz/campus-suite-hamburg-7    4.0   \n",
       "3  https://www.yelp.com/biz/campus-suite-hamburg-7    3.0   \n",
       "4  https://www.yelp.com/biz/campus-suite-hamburg-7    4.0   \n",
       "\n",
       "                                                text       date  \n",
       "0  Fantastic little restaurant!Great staff and fo... 2017-08-12  \n",
       "1  We went there to grab some breakfast. They are... 2015-09-29  \n",
       "2  Good coffee,  sandwiches,  and yogurts close t... 2016-01-13  \n",
       "3  When XING handed out coupons for free coffee d... 2008-04-24  \n",
       "4  I love Campus Suite franchise. after the Balza... 2010-01-15  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the top 5 rows\n",
    "yelp.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract name of the location from url\n",
    "yelp[\"name\"] = yelp.apply(lambda row: re.sub(\"https://www.yelp.com/biz/\", \"\", row[\"url\"]), axis=1)\n",
    "yelp = yelp.drop(columns=\"url\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Count'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inspect length of review text\n",
    "text_length_distribution = np.array([len(text) for text in yelp.text])\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=[15,8])\n",
    "sns.histplot(data = text_length_distribution)\n",
    "\n",
    "# Problem: Maximum length of Bert is 512\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check distribution of ratings\n",
    "rating_counts = yelp.stars.value_counts()\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=[15,8])\n",
    "sns.histplot(data = text_length_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading glove-wiki-gigaword-50\n",
      "Downloading glove-twitter-50\n",
      "Downloading word2vec-google-news-300\n"
     ]
    }
   ],
   "source": [
    "# Load potential sets of word vectors\n",
    "#list(gensim.downloader.info()['models'].keys())\n",
    "potential_wv = {}\n",
    "print(\"Downloading glove-wiki-gigaword-50\")\n",
    "potential_wv[\"wv_glove_wiki\"] = gensim.downloader.load('glove-wiki-gigaword-300')\n",
    "print(\"Downloading glove-twitter-50\")\n",
    "potential_wv[\"wv_glove_twitter\"] = gensim.downloader.load('glove-twitter-200')\n",
    "print(\"Downloading word2vec-google-news-300\")\n",
    "potential_wv[\"wv_word2vec\"] = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analyser class\n",
    "class SentimentAnalyser():\n",
    "\n",
    "    def __init__(self, data, target, normalize = True):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.target_distribution = self.data[self.target].value_counts(normalize=True)\n",
    "\n",
    "        # Normalize the review texts\n",
    "        if normalize:\n",
    "            self.data[\"text\"] = self.normalize()\n",
    "\n",
    "        # Initialize container for experiment results\n",
    "        self.experiment_results = {}\n",
    "        self.__run = 1\n",
    "\n",
    "        # Produce word embeddings for the whole dataset\n",
    "        self.word_embeddings = {}\n",
    "        for vector_set in [\"wv_glove_wiki\", \"wv_glove_twitter\", \"wv_word2vec\"]:\n",
    "            self.word_embeddings[vector_set] = self.__produce_embeddings(self.data.text, vector_set)\n",
    "\n",
    "        # Set up experiments to conduct\n",
    "        self.experiments = [\n",
    "            {\n",
    "                'name': 'Logistic Regression',\n",
    "                'model': LogisticRegression(n_jobs = os.cpu_count() -1, solver=\"saga\", l1_ratio=0.5),\n",
    "                'params': {\n",
    "                    'preprocessor': [\n",
    "                        self.__WordCountTransformerHelper(min_df=1, ngram_range=(1,1)),\n",
    "                        self.__WordCountTransformerHelper(min_df=2, ngram_range=(1,1)),\n",
    "                        self.__WordCountTransformerHelper(min_df=3, ngram_range=(1,1)),\n",
    "                        self.__WordCountTransformerHelper(min_df=1, ngram_range=(1,2)),\n",
    "                        self.__TfidfTransformerHelper(norm=\"l2\"),\n",
    "                        self.__TfidfTransformerHelper(norm=\"l1\"),\n",
    "                        FunctionTransformer(self.__word_embedding_helper_function, kw_args={'wv':\"wv_glove_wiki\"}),\n",
    "                        FunctionTransformer(self.__word_embedding_helper_function, kw_args={'wv':\"wv_glove_twitter\"}),\n",
    "                        FunctionTransformer(self.__word_embedding_helper_function, kw_args={'wv':\"wv_word2vec\"})\n",
    "                        ],\n",
    "                    'oversampler': ['passthrough', SMOTE(random_state=33)],\n",
    "                    'estimator__penalty': [\"none\", \"l2\", \"l1\", \"elasticnet\"],\n",
    "                    'estimator__C': [0.5, 1, 2]\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'name': 'KNeighbors',\n",
    "                'model': KNeighborsClassifier(algorithm = \"auto\", n_jobs = os.cpu_count() -1),\n",
    "                'params': {\n",
    "                    'preprocessor': [\n",
    "                        CountVectorizer(min_df=1), \n",
    "                        CountVectorizer(min_df=2), \n",
    "                        CountVectorizer(min_df=3), \n",
    "                        CountVectorizer(ngram_range=(1,2)),\n",
    "                        TfidfVectorizer(norm='l1'),\n",
    "                        TfidfVectorizer(norm='l2'),\n",
    "                        FunctionTransformer(self.__produce_embeddings, kw_args={'wv':\"wv_glove_wiki\"}),\n",
    "                        FunctionTransformer(self.__produce_embeddings, kw_args={'wv':\"wv_glove_twitter\"}),\n",
    "                        FunctionTransformer(self.__produce_embeddings, kw_args={'wv':\"wv_word2vec\"})\n",
    "                        ],\n",
    "                    'oversampler': ['passthrough', SMOTE(random_state=33)],\n",
    "                    'estimator__n_neighbors': randint(1, 10), \n",
    "                    'estimator__weights': [\"uniform\", \"distance\"], \n",
    "                    'estimator__metric': [\"euclidean\", \"manhattan\"]\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'name': 'Random Forest',\n",
    "                'model': RandomForestClassifier(n_jobs = os.cpu_count() -1, verbose = 1, random_state = 33),\n",
    "                'params': {\n",
    "                    'preprocessor': [\n",
    "                        CountVectorizer(min_df=1), \n",
    "                        CountVectorizer(min_df=2), \n",
    "                        CountVectorizer(min_df=3), \n",
    "                        CountVectorizer(ngram_range=(1,2)),\n",
    "                        TfidfVectorizer(norm='l1'),\n",
    "                        TfidfVectorizer(norm='l2'),\n",
    "                        FunctionTransformer(self.__produce_embeddings, kw_args={'wv':\"wv_glove_wiki\"}),\n",
    "                        FunctionTransformer(self.__produce_embeddings, kw_args={'wv':\"wv_glove_twitter\"}),\n",
    "                        FunctionTransformer(self.__produce_embeddings, kw_args={'wv':\"wv_word2vec\"})\n",
    "                        ],\n",
    "                    'oversampler': ['passthrough', SMOTE(random_state=33)],\n",
    "                    'estimator__criterion': ['gini', 'entropy'],\n",
    "                    'estimator__n_estimators': randint(1, 100), \n",
    "                    'estimator__max_features': uniform(0.1,0.9),\n",
    "                    'estimator__class_weight': [None, 'balanced', 'balanced_subsample'],\n",
    "                    'estimator__ccp_alpha': uniform(0,0.1)\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'name': 'SVM',\n",
    "                'model': SVC(verbose = True, random_state = 33),\n",
    "                'params': {\n",
    "                    'preprocessor': [\n",
    "                        CountVectorizer(min_df=1), \n",
    "                        CountVectorizer(min_df=2), \n",
    "                        CountVectorizer(min_df=3), \n",
    "                        CountVectorizer(ngram_range=(1,2)),\n",
    "                        TfidfVectorizer(norm='l1'),\n",
    "                        TfidfVectorizer(norm='l2'),\n",
    "                        FunctionTransformer(self.__produce_embeddings, kw_args={'wv':\"wv_glove_wiki\"}),\n",
    "                        FunctionTransformer(self.__produce_embeddings, kw_args={'wv':\"wv_glove_twitter\"}),\n",
    "                        FunctionTransformer(self.__produce_embeddings, kw_args={'wv':\"wv_word2vec\"})\n",
    "                        ],\n",
    "                    'oversampler': ['passthrough', SMOTE(random_state=33)],\n",
    "                    'estimator__C': uniform(0.001, 1),\n",
    "                    'estimator__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "                    'estimator__degree': randint(1, 5),\n",
    "                    'estimator__gamma': uniform(0.0001, 1),\n",
    "                    'estimator__class_weight': [None, 'balanced']\n",
    "                }\n",
    "            },\n",
    "            # When in doubt use XGBoost\n",
    "            {\n",
    "                'name': 'XGBoost',\n",
    "                'model': XGBClassifier(\n",
    "                    objective = \"multi:softprob\",\n",
    "                    num_class = 5, \n",
    "                    tree_method = \"auto\",\n",
    "                    eval_metric = \"auc\", \n",
    "                    verbosity = 1,\n",
    "                    use_label_encoder = False,\n",
    "                    random_state = 33),\n",
    "                'params': {\n",
    "                    'preprocessor': [\n",
    "                        CountVectorizer(min_df=1), \n",
    "                        CountVectorizer(min_df=2), \n",
    "                        CountVectorizer(min_df=3), \n",
    "                        CountVectorizer(ngram_range=(1,2)),\n",
    "                        TfidfVectorizer(norm='l1'),\n",
    "                        TfidfVectorizer(norm='l2'),\n",
    "                        FunctionTransformer(self.__produce_embeddings, kw_args={'wv':\"wv_glove_wiki\"}),\n",
    "                        FunctionTransformer(self.__produce_embeddings, kw_args={'wv':\"wv_glove_twitter\"}),\n",
    "                        FunctionTransformer(self.__produce_embeddings, kw_args={'wv':\"wv_word2vec\"})\n",
    "                        ],\n",
    "                    'oversampler': ['passthrough', SMOTE(random_state=33)],\n",
    "                    'estimator__learning_rate': uniform(0.001, 0.5),\n",
    "                    'estimator__gamma': uniform(0, 0.5),\n",
    "                    'estimator__max_depth': randint(3, 10),\n",
    "                    'estimator__colsample_bytree': uniform(0.5, 0.5)\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "\n",
    "    def __normalize_document(self, doc, tokenizer, stop_words):\n",
    "        doc = re.sub(r'@[\\w]+', '', doc)          # replace user mentions\n",
    "        doc = re.sub(r'http[\\S]+', 'URL', doc)    # replace URLs\n",
    "        doc = re.sub(r'[^\\w\\s]', '', doc)         # keep words and spaces\n",
    "        doc = doc.lower()\n",
    "        doc = doc.strip()\n",
    "        tokens = tokenizer.tokenize(doc)\n",
    "        filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "        doc = ' '.join(filtered_tokens)\n",
    "        return doc\n",
    "\n",
    "    def normalize(self):\n",
    "        stop_words = nltk.corpus.stopwords.words('english')\n",
    "        wpt = nltk.WordPunctTokenizer()\n",
    "\n",
    "        return self.data.text.apply(lambda x: self.__normalize_document(x, wpt, stop_words))\n",
    "\n",
    "    def naive_baseline(self):\n",
    "        self.data[\"naive_baseline_prediction\"] = np.random.choice(self.target_distribution.index.to_numpy(), size = len(self.data), replace = True, p = self.target_distribution.values)\n",
    "\n",
    "    def vader_sentiment(self):\n",
    "        vader = SentimentIntensityAnalyzer()\n",
    "        self.data['vader_scores'] = self.data.text.apply(lambda x: vader.polarity_scores(x))\n",
    "        self.data['vader_compound'] = self.data.vader_scores.apply(lambda x: x['compound'])\n",
    "        self.data['vader_prediction'] = self.data.vader_compound.apply(lambda x: 1 if x >= 0 else 0)\n",
    "        pd.crosstab(self.data[self.target], self.data.vader_prediction, normalize='all')\n",
    "\n",
    "    def textblob_sentiment(self):\n",
    "        self.data['textblob_score'] = self.data.text.apply(\n",
    "            lambda review: TextBlob(review).sentiment[0])\n",
    "        self.data['textblob_prediction'] = self.data.textblob_score.apply(lambda x: 1 if x >= 0 else 0)\n",
    "        pd.crosstab(self.data[self.target], self.data.textblob_prediction, normalize='all')\n",
    "\n",
    "    def __produce_embeddings(self, X, wv):\n",
    "        print(f\"Produce word embeddings for - {wv}\")\n",
    "\n",
    "        vocabulary = set(potential_wv[wv].index_to_key)\n",
    "        wpt = nltk.WordPunctTokenizer()\n",
    "\n",
    "        def avg_embeddings(document):\n",
    "            words = wpt.tokenize(document)\n",
    "            invocab = [word for word in words if word in vocabulary]\n",
    "            avg = np.mean(potential_wv[wv][invocab], axis=0) if len(invocab) >= 1 else []\n",
    "            return avg\n",
    "\n",
    "        doc_embeddings = np.array([avg_embeddings(doc) for doc in X.values])\n",
    "        return doc_embeddings\n",
    "\n",
    "    def __word_embedding_helper_function(self, X, wv):\n",
    "        embedding = self.word_embeddings[wv]\n",
    "        return embedding[X]\n",
    "\n",
    "    class __WordCountTransformerHelper(TransformerMixin, BaseEstimator):\n",
    "\n",
    "        def __init__(self, min_df=1, ngram_range=(1,1)):\n",
    "            self.min_df = min_df\n",
    "            self.ngram_range = ngram_range\n",
    "            self.transformer = CountVectorizer(min_df=self.min_df, ngram_range=self.ngram_range)\n",
    "\n",
    "        def fit(self, X, y=None):\n",
    "            self.transformer.fit(self.data.text[X])\n",
    "            return self\n",
    "\n",
    "        def transform(self, X):\n",
    "            return self.transformer.transform(self.data.text[X])\n",
    "\n",
    "    class __TfidfTransformerHelper(TransformerMixin, BaseEstimator):\n",
    "\n",
    "        def __init__(self, norm=\"l2\"):\n",
    "            self.norm = norm\n",
    "            self.transformer = TfidfTransformer(norm=self.norm)\n",
    "\n",
    "        def fit(self, X, y=None):\n",
    "            self.transformer.fit(self.data.text[X])\n",
    "            return self\n",
    "\n",
    "        def transform(self, X):\n",
    "            return self.transformer.transform(self.data.text[X])\n",
    "\n",
    "\n",
    "    def evaluate_classifier(self, name, model, params, iterations, cv_splits, cv_repeats):\n",
    "\n",
    "        class Debug(BaseEstimator, TransformerMixin):\n",
    "\n",
    "            def transform(self, X):\n",
    "                print(X.shape)\n",
    "                # what other output you want\n",
    "                return X\n",
    "\n",
    "            def fit(self, X, y=None, **fit_params):\n",
    "                return self\n",
    "\n",
    "        # The preprocessor set here is just a default and gets overwritten by the possible preprocessors in the parameter space\n",
    "        pipeline = Pipeline([(\"debug1\", Debug()), ('preprocessor', CountVectorizer()), (\"debug2\", Debug()), ('oversampler', SMOTE()), ('estimator', model)])\n",
    "\n",
    "        # Setting up the Cross validation\n",
    "        inner_cv = RepeatedStratifiedKFold(n_splits = cv_splits, n_repeats = cv_repeats, random_state = 33)\n",
    "        outer_cv = RepeatedStratifiedKFold(n_splits = cv_splits, n_repeats = cv_repeats, random_state = 33)\n",
    "\n",
    "        # Doing hyperparameter optimization\n",
    "        optimization = RandomizedSearchCV(\n",
    "            estimator = pipeline,\n",
    "            param_distributions = params,\n",
    "            scoring = ['accuracy', 'balanced_accuracy', 'f1_weighted', 'roc_auc_ovo_weighted'],\n",
    "            cv = inner_cv,\n",
    "            refit = 'roc_auc_ovo_weighted',\n",
    "            n_iter = iterations,\n",
    "            n_jobs = os.cpu_count() -1,\n",
    "            verbose = 1,\n",
    "            random_state = 33)\n",
    "\n",
    "        optimization.fit(np.array(range(len(self.data))), self.data[self.target])\n",
    "\n",
    "        # Evaluating the best model on the outer cross validation\n",
    "        performance_estimation = cross_validate(\n",
    "            estimator = optimization,\n",
    "            X = np.array(range(len(self.data))),\n",
    "            y = self.data[self.target],\n",
    "            scoring = ['accuracy', 'balanced_accuracy', 'f1_weighted', 'roc_auc_ovo_weighted'],\n",
    "            cv = outer_cv,\n",
    "            n_jobs = os.cpu_count() -1)\n",
    "\n",
    "        return(\n",
    "        {\n",
    "            'name': name,\n",
    "            'optimization_cv_results': pd.DataFrame(optimization.cv_results_),\n",
    "            'best_params': optimization.best_params_,\n",
    "            'best_model': optimization.best_estimator_,\n",
    "            'acc': performance_estimation['test_accuracy'],\n",
    "            'balanced_acc': performance_estimation['test_balanced_accuracy'],\n",
    "            'f1': performance_estimation['test_f1_weighted'],\n",
    "            'roc_auc': performance_estimation['test_roc_auc_ovo_weighted']\n",
    "        }\n",
    "        )\n",
    "\n",
    "    def run_experiments(self, iterations, cv_splits, cv_repeats, models = None):\n",
    "\n",
    "        self.experiment_results[f\"run_{self.__run}\"] = {}\n",
    "\n",
    "        # Determine which models to test\n",
    "        if models:\n",
    "            experiments = [experiment for experiment in self.experiments if experiment[\"name\"] in models]\n",
    "        else:\n",
    "            experiments = self.experiments\n",
    "\n",
    "        # Run experiment per model type\n",
    "        for experiment in experiments:\n",
    "\n",
    "            # Skip SVM\n",
    "            if experiment['name'] == 'SVM':\n",
    "                continue\n",
    "\n",
    "            start_time= time.time()\n",
    "\n",
    "            print()\n",
    "            print()\n",
    "            print(experiment['name'])\n",
    "            print(\"-----------------\")\n",
    "\n",
    "            self.experiment_results[f\"run_{self.__run}\"][experiment['name']] = self.evaluate_classifier(\n",
    "                    name = experiment['name'],\n",
    "                    model = experiment['model'],\n",
    "                    params = experiment['params'],\n",
    "                    iterations = iterations,\n",
    "                    cv_splits = cv_splits,\n",
    "                    cv_repeats = cv_repeats\n",
    "                )\n",
    "\n",
    "            end_time = time.time() - start_time\n",
    "            print(f'Time: {int(round(end_time, 1))} seconds ({int(round(end_time/60, 1))} minutes)')\n",
    "\n",
    "        self.__run += 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class word_count_transformer_helper(TransformerMixin, BaseEstimator):\n",
    "\n",
    "    def __init__(self, min_df, ngram_range):\n",
    "        self.transformer = CountVectorizer(min_df=min_df, ngram_range=ngram_range)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.transformer.fit(yelp.text[X])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.transformer.transform(yelp.text[X])\n",
    "\n",
    "test = word_count_transformer_helper(min_df = 1, ngram_range = (1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'preprocessor': [\n",
    "        word_count_transformer_helper(min_df = 1, ngram_range = (1,1)), \n",
    "        #FunctionTransformer(word_count_helper_function, kw_args={'min_df':2}), \n",
    "        #FunctionTransformer(word_count_helper_function, kw_args={'min_df':3}), \n",
    "        #FunctionTransformer(word_count_helper_function, kw_args={'ngram_range':(1,2)})\n",
    "        #FunctionTransformer(self.tfidf_helper_function, kw_args={'norm':\"l1\"}),\n",
    "        #FunctionTransformer(self.tfidf_helper_function, kw_args={'norm':\"l2\"})\n",
    "        #FunctionTransformer(self.word_embedding_helper_function, kw_args={'wv':\"wv_glove_wiki\"}),\n",
    "        #FunctionTransformer(self.word_embedding_helper_function, kw_args={'wv':\"wv_glove_twitter\"}),\n",
    "        #FunctionTransformer(self.word_embedding_helper_function, kw_args={'wv':\"wv_word2vec\"})\n",
    "        ],\n",
    "    'oversampler': ['passthrough'], #SMOTE(random_state=33)],\n",
    "    'estimator__penalty': [\"none\", \"l2\", \"l1\", \"elasticnet\"],\n",
    "    'estimator__C': [0.5, 1, 2]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n",
      "(3025,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ValentinStudium\\Envs\\sma\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-13 {color: black;background-color: white;}#sk-container-id-13 pre{padding: 0;}#sk-container-id-13 div.sk-toggleable {background-color: white;}#sk-container-id-13 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-13 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-13 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-13 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-13 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-13 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-13 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-13 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-13 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-13 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-13 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-13 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-13 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-13 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-13 div.sk-item {position: relative;z-index: 1;}#sk-container-id-13 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-13 div.sk-item::before, #sk-container-id-13 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-13 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-13 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-13 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-13 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-13 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-13 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-13 div.sk-label-container {text-align: center;}#sk-container-id-13 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-13 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-13\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=RepeatedKFold(n_repeats=1, n_splits=3, random_state=33),\n",
       "                   error_score=&#x27;raise&#x27;,\n",
       "                   estimator=Pipeline(steps=[(&#x27;debug1&#x27;, Debug()),\n",
       "                                             (&#x27;preprocessor&#x27;,\n",
       "                                              CountVectorizer()),\n",
       "                                             (&#x27;oversampler&#x27;, SMOTE()),\n",
       "                                             (&#x27;estimator&#x27;,\n",
       "                                              LogisticRegression(l1_ratio=0.5,\n",
       "                                                                 n_jobs=7,\n",
       "                                                                 solver=&#x27;saga&#x27;))]),\n",
       "                   n_iter=3, n_jobs=7,\n",
       "                   param_distributions={&#x27;estimator__C&#x27;: [0.5, 1, 2],\n",
       "                                        &#x27;estimator__penalty&#x27;: [&#x27;none&#x27;, &#x27;l2&#x27;,\n",
       "                                                               &#x27;l1&#x27;,\n",
       "                                                               &#x27;elasticnet&#x27;],\n",
       "                                        &#x27;oversampler&#x27;: [&#x27;passthrough&#x27;],\n",
       "                                        &#x27;preprocessor&#x27;: [word_count_transformer_helper(min_df=1,\n",
       "                                                                                       ngram_range=(1,\n",
       "                                                                                                    1))]},\n",
       "                   random_state=33, refit=&#x27;roc_auc_ovo_weighted&#x27;,\n",
       "                   scoring=[&#x27;accuracy&#x27;, &#x27;balanced_accuracy&#x27;, &#x27;f1_weighted&#x27;,\n",
       "                            &#x27;roc_auc_ovo_weighted&#x27;],\n",
       "                   verbose=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-42\" type=\"checkbox\" ><label for=\"sk-estimator-id-42\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=RepeatedKFold(n_repeats=1, n_splits=3, random_state=33),\n",
       "                   error_score=&#x27;raise&#x27;,\n",
       "                   estimator=Pipeline(steps=[(&#x27;debug1&#x27;, Debug()),\n",
       "                                             (&#x27;preprocessor&#x27;,\n",
       "                                              CountVectorizer()),\n",
       "                                             (&#x27;oversampler&#x27;, SMOTE()),\n",
       "                                             (&#x27;estimator&#x27;,\n",
       "                                              LogisticRegression(l1_ratio=0.5,\n",
       "                                                                 n_jobs=7,\n",
       "                                                                 solver=&#x27;saga&#x27;))]),\n",
       "                   n_iter=3, n_jobs=7,\n",
       "                   param_distributions={&#x27;estimator__C&#x27;: [0.5, 1, 2],\n",
       "                                        &#x27;estimator__penalty&#x27;: [&#x27;none&#x27;, &#x27;l2&#x27;,\n",
       "                                                               &#x27;l1&#x27;,\n",
       "                                                               &#x27;elasticnet&#x27;],\n",
       "                                        &#x27;oversampler&#x27;: [&#x27;passthrough&#x27;],\n",
       "                                        &#x27;preprocessor&#x27;: [word_count_transformer_helper(min_df=1,\n",
       "                                                                                       ngram_range=(1,\n",
       "                                                                                                    1))]},\n",
       "                   random_state=33, refit=&#x27;roc_auc_ovo_weighted&#x27;,\n",
       "                   scoring=[&#x27;accuracy&#x27;, &#x27;balanced_accuracy&#x27;, &#x27;f1_weighted&#x27;,\n",
       "                            &#x27;roc_auc_ovo_weighted&#x27;],\n",
       "                   verbose=1)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-43\" type=\"checkbox\" ><label for=\"sk-estimator-id-43\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;debug1&#x27;, Debug()), (&#x27;preprocessor&#x27;, CountVectorizer()),\n",
       "                (&#x27;oversampler&#x27;, SMOTE()),\n",
       "                (&#x27;estimator&#x27;,\n",
       "                 LogisticRegression(l1_ratio=0.5, n_jobs=7, solver=&#x27;saga&#x27;))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-44\" type=\"checkbox\" ><label for=\"sk-estimator-id-44\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Debug</label><div class=\"sk-toggleable__content\"><pre>Debug()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-45\" type=\"checkbox\" ><label for=\"sk-estimator-id-45\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-46\" type=\"checkbox\" ><label for=\"sk-estimator-id-46\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SMOTE</label><div class=\"sk-toggleable__content\"><pre>SMOTE()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-47\" type=\"checkbox\" ><label for=\"sk-estimator-id-47\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(l1_ratio=0.5, n_jobs=7, solver=&#x27;saga&#x27;)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=RepeatedKFold(n_repeats=1, n_splits=3, random_state=33),\n",
       "                   error_score='raise',\n",
       "                   estimator=Pipeline(steps=[('debug1', Debug()),\n",
       "                                             ('preprocessor',\n",
       "                                              CountVectorizer()),\n",
       "                                             ('oversampler', SMOTE()),\n",
       "                                             ('estimator',\n",
       "                                              LogisticRegression(l1_ratio=0.5,\n",
       "                                                                 n_jobs=7,\n",
       "                                                                 solver='saga'))]),\n",
       "                   n_iter=3, n_jobs=7,\n",
       "                   param_distributions={'estimator__C': [0.5, 1, 2],\n",
       "                                        'estimator__penalty': ['none', 'l2',\n",
       "                                                               'l1',\n",
       "                                                               'elasticnet'],\n",
       "                                        'oversampler': ['passthrough'],\n",
       "                                        'preprocessor': [word_count_transformer_helper(min_df=1,\n",
       "                                                                                       ngram_range=(1,\n",
       "                                                                                                    1))]},\n",
       "                   random_state=33, refit='roc_auc_ovo_weighted',\n",
       "                   scoring=['accuracy', 'balanced_accuracy', 'f1_weighted',\n",
       "                            'roc_auc_ovo_weighted'],\n",
       "                   verbose=1)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def word_count_helper_function(X, min_df=1, ngram_range=(1,1)):\n",
    "    text_input = yelp.text[X]\n",
    "    transformer = CountVectorizer(min_df=min_df, ngram_range=ngram_range)\n",
    "    counts = transformer.fit_transform(text_input)\n",
    "    return counts\n",
    "\n",
    "class word_count_transformer_helper(TransformerMixin, BaseEstimator):\n",
    "\n",
    "    def __init__(self, min_df, ngram_range):\n",
    "        self.min_df = min_df\n",
    "        self.ngram_range = ngram_range\n",
    "        self.transformer = CountVectorizer(min_df=self.min_df, ngram_range=self.ngram_range)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.transformer.fit(yelp.text[X])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.transformer.transform(yelp.text[X])\n",
    "\n",
    "params = {\n",
    "    'preprocessor': [\n",
    "        word_count_transformer_helper(min_df = 1, ngram_range = (1,1)), \n",
    "        #FunctionTransformer(word_count_helper_function, kw_args={'min_df':2}), \n",
    "        #FunctionTransformer(word_count_helper_function, kw_args={'min_df':3}), \n",
    "        #FunctionTransformer(word_count_helper_function, kw_args={'ngram_range':(1,2)})\n",
    "        #FunctionTransformer(self.tfidf_helper_function, kw_args={'norm':\"l1\"}),\n",
    "        #FunctionTransformer(self.tfidf_helper_function, kw_args={'norm':\"l2\"})\n",
    "        #FunctionTransformer(self.word_embedding_helper_function, kw_args={'wv':\"wv_glove_wiki\"}),\n",
    "        #FunctionTransformer(self.word_embedding_helper_function, kw_args={'wv':\"wv_glove_twitter\"}),\n",
    "        #FunctionTransformer(self.word_embedding_helper_function, kw_args={'wv':\"wv_word2vec\"})\n",
    "        ],\n",
    "    'oversampler': ['passthrough'], #SMOTE(random_state=33)],\n",
    "    'estimator__penalty': [\"none\", \"l2\", \"l1\", \"elasticnet\"],\n",
    "    'estimator__C': [0.5, 1, 2]\n",
    "}\n",
    "\n",
    "class Debug(BaseEstimator, TransformerMixin):\n",
    "\n",
    "    def transform(self, X):\n",
    "        print(X.shape)\n",
    "        # what other output you want\n",
    "        return X\n",
    "\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        return self\n",
    "\n",
    "# The preprocessor set here is just a default and gets overwritten by the possible preprocessors in the parameter space\n",
    "pipeline = Pipeline([(\"debug1\", Debug()), ('preprocessor', CountVectorizer()), ('oversampler', SMOTE()), ('estimator', LogisticRegression(n_jobs = os.cpu_count() -1, solver=\"saga\", l1_ratio=0.5))])\n",
    "#pipeline.fit(np.array(range(len(yelp))), yelp[\"stars\"])\n",
    "#print(pipeline.score(np.array(range(len(yelp))), yelp[\"stars\"]))\n",
    "\n",
    "# Setting up the Cross validation\n",
    "inner_cv = RepeatedKFold(n_splits = 3, n_repeats = 1, random_state = 33)\n",
    "outer_cv = RepeatedKFold(n_splits = 3, n_repeats = 1, random_state = 33)\n",
    "\n",
    "# Doing hyperparameter optimization\n",
    "optimization = RandomizedSearchCV(\n",
    "    estimator = pipeline,\n",
    "    param_distributions = params,\n",
    "    scoring = ['accuracy', 'balanced_accuracy', 'f1_weighted', 'roc_auc_ovo_weighted'],\n",
    "    cv = inner_cv,\n",
    "    refit = 'roc_auc_ovo_weighted',\n",
    "    n_iter = 3,\n",
    "    n_jobs = os.cpu_count() -1,\n",
    "    verbose = 1,\n",
    "    random_state = 33,\n",
    "    error_score=\"raise\")\n",
    "\n",
    "optimization.fit(np.array(range(len(yelp))), yelp[\"stars\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 1.39322535, 18.0493141 ,  1.25904115]),\n",
       " 'std_fit_time': array([0.0288245 , 2.02765123, 0.0911937 ]),\n",
       " 'mean_score_time': array([0.31859668, 0.13458967, 0.22503209]),\n",
       " 'std_score_time': array([0.03788537, 0.03249213, 0.08643384]),\n",
       " 'param_preprocessor': masked_array(data=[word_count_transformer_helper(min_df=1, ngram_range=(1, 1)),\n",
       "                    word_count_transformer_helper(min_df=1, ngram_range=(1, 1)),\n",
       "                    word_count_transformer_helper(min_df=1, ngram_range=(1, 1))],\n",
       "              mask=[False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_oversampler': masked_array(data=['passthrough', 'passthrough', 'passthrough'],\n",
       "              mask=[False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_estimator__penalty': masked_array(data=['none', 'elasticnet', 'l2'],\n",
       "              mask=[False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_estimator__C': masked_array(data=[0.5, 2, 1],\n",
       "              mask=[False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'preprocessor': word_count_transformer_helper(min_df=1, ngram_range=(1, 1)),\n",
       "   'oversampler': 'passthrough',\n",
       "   'estimator__penalty': 'none',\n",
       "   'estimator__C': 0.5},\n",
       "  {'preprocessor': word_count_transformer_helper(min_df=1, ngram_range=(1, 1)),\n",
       "   'oversampler': 'passthrough',\n",
       "   'estimator__penalty': 'elasticnet',\n",
       "   'estimator__C': 2},\n",
       "  {'preprocessor': word_count_transformer_helper(min_df=1, ngram_range=(1, 1)),\n",
       "   'oversampler': 'passthrough',\n",
       "   'estimator__penalty': 'l2',\n",
       "   'estimator__C': 1}],\n",
       " 'split0_test_accuracy': array([0.55302279, 0.54905847, 0.55401388]),\n",
       " 'split1_test_accuracy': array([0.53869048, 0.53869048, 0.53769841]),\n",
       " 'split2_test_accuracy': array([0.52083333, 0.51388889, 0.51984127]),\n",
       " 'mean_test_accuracy': array([0.53751553, 0.53387928, 0.53718452]),\n",
       " 'std_test_accuracy': array([0.01316753, 0.01475547, 0.01395564]),\n",
       " 'rank_test_accuracy': array([1, 3, 2]),\n",
       " 'split0_test_balanced_accuracy': array([0.391624  , 0.38959543, 0.39008359]),\n",
       " 'split1_test_balanced_accuracy': array([0.3892489 , 0.39555161, 0.38860582]),\n",
       " 'split2_test_balanced_accuracy': array([0.35810233, 0.34258861, 0.34600445]),\n",
       " 'mean_test_balanced_accuracy': array([0.37965841, 0.37591188, 0.37489795]),\n",
       " 'std_test_balanced_accuracy': array([0.01527326, 0.02368825, 0.0204397 ]),\n",
       " 'rank_test_balanced_accuracy': array([1, 2, 3]),\n",
       " 'split0_test_f1_weighted': array([0.53594569, 0.53292412, 0.53676172]),\n",
       " 'split1_test_f1_weighted': array([0.52917247, 0.52994709, 0.52809632]),\n",
       " 'split2_test_f1_weighted': array([0.50351716, 0.49652811, 0.49928055]),\n",
       " 'mean_test_f1_weighted': array([0.52287844, 0.51979977, 0.52137953]),\n",
       " 'std_test_f1_weighted': array([0.01396695, 0.01650037, 0.01602178]),\n",
       " 'rank_test_f1_weighted': array([1, 3, 2]),\n",
       " 'split0_test_roc_auc_ovo_weighted': array([0.74770786, 0.75013443, 0.74834046]),\n",
       " 'split1_test_roc_auc_ovo_weighted': array([0.76272321, 0.76463403, 0.76331044]),\n",
       " 'split2_test_roc_auc_ovo_weighted': array([0.7421428 , 0.74388464, 0.74350147]),\n",
       " 'mean_test_roc_auc_ovo_weighted': array([0.75085796, 0.75288436, 0.75171745]),\n",
       " 'std_test_roc_auc_ovo_weighted': array([0.00869216, 0.00869122, 0.00843216]),\n",
       " 'rank_test_roc_auc_ovo_weighted': array([3, 1, 2])}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimization.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_embedding_helper_function(self, X, wv):\n",
    "    embedding = self.word_embeddings[wv]\n",
    "    return embedding[X]\n",
    "\n",
    "def word_count_helper_function(self, X, min_df=1, ngram_range=(1,1)):\n",
    "    text_input = np.array(self.data.text)[X]\n",
    "    transformer = CountVectorizer(min_df=min_df, ngram_range=ngram_range)\n",
    "    return transformer.fit_transform(text_input)\n",
    "\n",
    "def tfidf_helper_function(self, X, norm):\n",
    "    text_input = np.array(self.data.text)[X]\n",
    "    transformer = TfidfVectorizer(norm=norm)\n",
    "    return transformer.fit_transform(text_input)\n",
    "\n",
    "\n",
    "params = {\n",
    "'oversampler': ['passthrough', SMOTE(random_state=33)],\n",
    "'estimator__penalty': [\"none\", \"l2\", \"l1\", \"elasticnet\"],\n",
    "'estimator__C': [0.5, 1, 2]\n",
    "}\n",
    "\n",
    "\n",
    "# The preprocessor set here is just a default and gets overwritten by the possible preprocessors in the parameter space\n",
    "pipeline = Pipeline([('preprocessor', FunctionTransformer(word_count_helper_function)), ('oversampler', SMOTE()), ('estimator', LogisticRegression(n_jobs = os.cpu_count() -1, solver=\"saga\", l1_ratio=0.5))])\n",
    "\n",
    "# Setting up the Cross validation\n",
    "inner_cv = RepeatedStratifiedKFold(n_splits = 2, n_repeats = 1, random_state = 33)\n",
    "outer_cv = RepeatedStratifiedKFold(n_splits = 2, n_repeats = 1, random_state = 33)\n",
    "\n",
    "# Doing hyperparameter optimization\n",
    "optimization = RandomizedSearchCV(\n",
    "    estimator = pipeline,\n",
    "    param_distributions = params,\n",
    "    scoring = ['accuracy', 'balanced_accuracy', 'f1_weighted', 'roc_auc_ovo_weighted'],\n",
    "    cv = inner_cv,\n",
    "    refit = 'roc_auc_ovo_weighted',\n",
    "    n_iter = 5,\n",
    "    n_jobs = os.cpu_count() -1,\n",
    "    verbose = 1,\n",
    "    random_state = 33)\n",
    "\n",
    "optimization.fit(np.array(range(len(yelp))), yelp.stars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing needs to be part of pipeline to prevent data leakage or cheating the out of vocabulary problem - additional computational cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages of pipeline\n",
    "\n",
    "Convenience and encapsulation\n",
    "You only have to call fit and predict once on your data to fit a whole sequence of estimators.\n",
    "\n",
    "Joint parameter selection\n",
    "You can grid search over parameters of all estimators in the pipeline at once.\n",
    "\n",
    "Safety\n",
    "Pipelines help avoid leaking statistics from your test data into the trained model in cross-validation, by ensuring that the same samples are used to train the transformers and predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Produce word embeddings for - wv_glove_wiki\n",
      "Produce word embeddings for - wv_glove_twitter\n",
      "Produce word embeddings for - wv_word2vec\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Sentiment Analyser\n",
    "sentiment = SentimentAnalyser(yelp, \"stars\", normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment.naive_baseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment.vader_sentiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment.textblob_sentiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Logistic Regression\n",
      "-----------------\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n"
     ]
    },
    {
     "ename": "BrokenProcessPool",
     "evalue": "A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"c:\\Users\\ValentinStudium\\Envs\\sma\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 407, in _process_worker\n    call_item = call_queue.get(block=True, timeout=timeout)\n  File \"C:\\Users\\ValentinStudium\\AppData\\Local\\Programs\\Python\\Python39\\lib\\multiprocessing\\queues.py\", line 122, in get\n    return _ForkingPickler.loads(res)\nAttributeError: 'SentimentAnalyser' object has no attribute '__word_embedding_helper_function'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ValentinStudium\\Nextcloud\\Studium\\09 Social Media Analytics\\Sentiment-Analysis\\Sentiment_Analysis.ipynb Cell 24'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000050?line=0'>1</a>\u001b[0m sentiment\u001b[39m.\u001b[39;49mrun_experiments(\u001b[39m3\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39m1\u001b[39;49m, [\u001b[39m\"\u001b[39;49m\u001b[39mLogistic Regression\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n",
      "\u001b[1;32mc:\\Users\\ValentinStudium\\Nextcloud\\Studium\\09 Social Media Analytics\\Sentiment-Analysis\\Sentiment_Analysis.ipynb Cell 12'\u001b[0m in \u001b[0;36mSentimentAnalyser.run_experiments\u001b[1;34m(self, iterations, cv_splits, cv_repeats, models)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000013?line=298'>299</a>\u001b[0m \u001b[39mprint\u001b[39m(experiment[\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000013?line=299'>300</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m-----------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000013?line=301'>302</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperiment_results[\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrun_\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__run\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m][experiment[\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate_classifier(\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000013?line=302'>303</a>\u001b[0m         name \u001b[39m=\u001b[39;49m experiment[\u001b[39m'\u001b[39;49m\u001b[39mname\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000013?line=303'>304</a>\u001b[0m         model \u001b[39m=\u001b[39;49m experiment[\u001b[39m'\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000013?line=304'>305</a>\u001b[0m         params \u001b[39m=\u001b[39;49m experiment[\u001b[39m'\u001b[39;49m\u001b[39mparams\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000013?line=305'>306</a>\u001b[0m         iterations \u001b[39m=\u001b[39;49m iterations,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000013?line=306'>307</a>\u001b[0m         cv_splits \u001b[39m=\u001b[39;49m cv_splits,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000013?line=307'>308</a>\u001b[0m         cv_repeats \u001b[39m=\u001b[39;49m cv_repeats\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000013?line=308'>309</a>\u001b[0m     )\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000013?line=310'>311</a>\u001b[0m end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000013?line=311'>312</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTime: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mint\u001b[39m(\u001b[39mround\u001b[39m(end_time, \u001b[39m1\u001b[39m))\u001b[39m}\u001b[39;00m\u001b[39m seconds (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mint\u001b[39m(\u001b[39mround\u001b[39m(end_time\u001b[39m/\u001b[39m\u001b[39m60\u001b[39m, \u001b[39m1\u001b[39m))\u001b[39m}\u001b[39;00m\u001b[39m minutes)\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\ValentinStudium\\Nextcloud\\Studium\\09 Social Media Analytics\\Sentiment-Analysis\\Sentiment_Analysis.ipynb Cell 12'\u001b[0m in \u001b[0;36mSentimentAnalyser.evaluate_classifier\u001b[1;34m(self, name, model, params, iterations, cv_splits, cv_repeats)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000013?line=241'>242</a>\u001b[0m \u001b[39m# Doing hyperparameter optimization\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000013?line=242'>243</a>\u001b[0m optimization \u001b[39m=\u001b[39m RandomizedSearchCV(\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000013?line=243'>244</a>\u001b[0m     estimator \u001b[39m=\u001b[39m pipeline,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000013?line=244'>245</a>\u001b[0m     param_distributions \u001b[39m=\u001b[39m params,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000013?line=250'>251</a>\u001b[0m     verbose \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000013?line=251'>252</a>\u001b[0m     random_state \u001b[39m=\u001b[39m \u001b[39m33\u001b[39m)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000013?line=253'>254</a>\u001b[0m optimization\u001b[39m.\u001b[39;49mfit(np\u001b[39m.\u001b[39;49marray(\u001b[39mrange\u001b[39;49m(\u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata))), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget])\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000013?line=255'>256</a>\u001b[0m \u001b[39m# Evaluating the best model on the outer cross validation\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000013?line=256'>257</a>\u001b[0m performance_estimation \u001b[39m=\u001b[39m cross_validate(\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000013?line=257'>258</a>\u001b[0m     estimator \u001b[39m=\u001b[39m optimization,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000013?line=258'>259</a>\u001b[0m     X \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata))),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000013?line=261'>262</a>\u001b[0m     cv \u001b[39m=\u001b[39m outer_cv,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000013?line=262'>263</a>\u001b[0m     n_jobs \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mcpu_count() \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ValentinStudium\\Envs\\sma\\lib\\site-packages\\sklearn\\model_selection\\_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=868'>869</a>\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=869'>870</a>\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=870'>871</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=872'>873</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=874'>875</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=876'>877</a>\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=877'>878</a>\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=878'>879</a>\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\ValentinStudium\\Envs\\sma\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1749\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=1746'>1747</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=1747'>1748</a>\u001b[0m     \u001b[39m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=1748'>1749</a>\u001b[0m     evaluate_candidates(\n\u001b[0;32m   <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=1749'>1750</a>\u001b[0m         ParameterSampler(\n\u001b[0;32m   <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=1750'>1751</a>\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_distributions, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_iter, random_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state\n\u001b[0;32m   <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=1751'>1752</a>\u001b[0m         )\n\u001b[0;32m   <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=1752'>1753</a>\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ValentinStudium\\Envs\\sma\\lib\\site-packages\\sklearn\\model_selection\\_search.py:822\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=813'>814</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=814'>815</a>\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=815'>816</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=816'>817</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=817'>818</a>\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=818'>819</a>\u001b[0m         )\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=819'>820</a>\u001b[0m     )\n\u001b[1;32m--> <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=821'>822</a>\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=822'>823</a>\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=823'>824</a>\u001b[0m         clone(base_estimator),\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=824'>825</a>\u001b[0m         X,\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=825'>826</a>\u001b[0m         y,\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=826'>827</a>\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=827'>828</a>\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=828'>829</a>\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=829'>830</a>\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=830'>831</a>\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=831'>832</a>\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=832'>833</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=833'>834</a>\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=834'>835</a>\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=835'>836</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=836'>837</a>\u001b[0m )\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=838'>839</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=839'>840</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=840'>841</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=841'>842</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=842'>843</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=843'>844</a>\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ValentinStudium\\Envs\\sma\\lib\\site-packages\\joblib\\parallel.py:1056\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/joblib/parallel.py?line=1052'>1053</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/joblib/parallel.py?line=1054'>1055</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[1;32m-> <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/joblib/parallel.py?line=1055'>1056</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[0;32m   <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/joblib/parallel.py?line=1056'>1057</a>\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[0;32m   <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/joblib/parallel.py?line=1057'>1058</a>\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[1;32mc:\\Users\\ValentinStudium\\Envs\\sma\\lib\\site-packages\\joblib\\parallel.py:935\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/joblib/parallel.py?line=932'>933</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/joblib/parallel.py?line=933'>934</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m--> <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/joblib/parallel.py?line=934'>935</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/joblib/parallel.py?line=935'>936</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/joblib/parallel.py?line=936'>937</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[1;32mc:\\Users\\ValentinStudium\\Envs\\sma\\lib\\site-packages\\joblib\\_parallel_backends.py:542\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/joblib/_parallel_backends.py?line=538'>539</a>\u001b[0m \u001b[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/joblib/_parallel_backends.py?line=539'>540</a>\u001b[0m \u001b[39mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/joblib/_parallel_backends.py?line=540'>541</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/joblib/_parallel_backends.py?line=541'>542</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/joblib/_parallel_backends.py?line=542'>543</a>\u001b[0m \u001b[39mexcept\u001b[39;00m CfTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/joblib/_parallel_backends.py?line=543'>544</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\concurrent\\futures\\_base.py:445\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/AppData/Local/Programs/Python/Python39/lib/concurrent/futures/_base.py?line=442'>443</a>\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/AppData/Local/Programs/Python/Python39/lib/concurrent/futures/_base.py?line=443'>444</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m--> <a href='file:///c%3A/Users/ValentinStudium/AppData/Local/Programs/Python/Python39/lib/concurrent/futures/_base.py?line=444'>445</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/AppData/Local/Programs/Python/Python39/lib/concurrent/futures/_base.py?line=445'>446</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/AppData/Local/Programs/Python/Python39/lib/concurrent/futures/_base.py?line=446'>447</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\concurrent\\futures\\_base.py:390\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/AppData/Local/Programs/Python/Python39/lib/concurrent/futures/_base.py?line=387'>388</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/AppData/Local/Programs/Python/Python39/lib/concurrent/futures/_base.py?line=388'>389</a>\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/ValentinStudium/AppData/Local/Programs/Python/Python39/lib/concurrent/futures/_base.py?line=389'>390</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/AppData/Local/Programs/Python/Python39/lib/concurrent/futures/_base.py?line=390'>391</a>\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/AppData/Local/Programs/Python/Python39/lib/concurrent/futures/_base.py?line=391'>392</a>\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/AppData/Local/Programs/Python/Python39/lib/concurrent/futures/_base.py?line=392'>393</a>\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;31mBrokenProcessPool\u001b[0m: A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable."
     ]
    }
   ],
   "source": [
    "sentiment.run_experiments(3, 2, 1, [\"Logistic Regression\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Logistic Regression\n",
      "-----------------\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ValentinStudium\\Envs\\sma\\lib\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [0.50578736        nan        nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\ValentinStudium\\Envs\\sma\\lib\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [0.26541201        nan        nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\ValentinStudium\\Envs\\sma\\lib\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [0.44338571        nan        nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\ValentinStudium\\Envs\\sma\\lib\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [0.76371486        nan        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    1    2 ... 3022 3023 3024]\n",
      "[[-0.0231393   0.04741233  0.01614293 ... -0.02514256  0.04803576\n",
      "   0.00268555]\n",
      " [-0.02278455  0.04631694 -0.02104378 ... -0.00500965  0.05541738\n",
      "  -0.00547345]\n",
      " [ 0.00243906  0.02151313 -0.00636644 ... -0.03804838  0.00852438\n",
      "  -0.01802885]\n",
      " ...\n",
      " [ 0.00030435  0.04760577  0.02483141 ... -0.01444306  0.03530873\n",
      "   0.00304269]\n",
      " [ 0.0184044   0.04695504  0.02700057 ... -0.04469659  0.01335864\n",
      "   0.0114487 ]\n",
      " [ 0.02122582  0.03601752  0.05741713 ... -0.09107801  0.01724688\n",
      "   0.0259196 ]]\n",
      "Time: 65 seconds (1 minutes)\n"
     ]
    }
   ],
   "source": [
    "sentiment.run_experiments(3, 2, 1, [\"Logistic Regression\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_preprocessor</th>\n",
       "      <th>param_oversampler</th>\n",
       "      <th>param_estimator__penalty</th>\n",
       "      <th>param_estimator__C</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_accuracy</th>\n",
       "      <th>...</th>\n",
       "      <th>split0_test_f1_weighted</th>\n",
       "      <th>split1_test_f1_weighted</th>\n",
       "      <th>mean_test_f1_weighted</th>\n",
       "      <th>std_test_f1_weighted</th>\n",
       "      <th>rank_test_f1_weighted</th>\n",
       "      <th>split0_test_roc_auc_ovo_weighted</th>\n",
       "      <th>split1_test_roc_auc_ovo_weighted</th>\n",
       "      <th>mean_test_roc_auc_ovo_weighted</th>\n",
       "      <th>std_test_roc_auc_ovo_weighted</th>\n",
       "      <th>rank_test_roc_auc_ovo_weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.780407</td>\n",
       "      <td>0.543702</td>\n",
       "      <td>0.096499</td>\n",
       "      <td>0.007501</td>\n",
       "      <td>FunctionTransformer(func=&lt;bound method Sentime...</td>\n",
       "      <td>passthrough</td>\n",
       "      <td>elasticnet</td>\n",
       "      <td>1</td>\n",
       "      <td>{'preprocessor': FunctionTransformer(func=&lt;bou...</td>\n",
       "      <td>0.499009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.441208</td>\n",
       "      <td>0.445563</td>\n",
       "      <td>0.443386</td>\n",
       "      <td>0.002178</td>\n",
       "      <td>1</td>\n",
       "      <td>0.759383</td>\n",
       "      <td>0.768054</td>\n",
       "      <td>0.763719</td>\n",
       "      <td>0.004336</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.893004</td>\n",
       "      <td>0.129515</td>\n",
       "      <td>0.182261</td>\n",
       "      <td>0.056265</td>\n",
       "      <td>FunctionTransformer(func=&lt;bound method Sentime...</td>\n",
       "      <td>passthrough</td>\n",
       "      <td>none</td>\n",
       "      <td>2</td>\n",
       "      <td>{'preprocessor': FunctionTransformer(func=&lt;bou...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.571842</td>\n",
       "      <td>0.112156</td>\n",
       "      <td>0.127510</td>\n",
       "      <td>0.001489</td>\n",
       "      <td>FunctionTransformer(func=&lt;bound method Sentime...</td>\n",
       "      <td>passthrough</td>\n",
       "      <td>none</td>\n",
       "      <td>1</td>\n",
       "      <td>{'preprocessor': FunctionTransformer(func=&lt;bou...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       4.780407      0.543702         0.096499        0.007501   \n",
       "1       0.893004      0.129515         0.182261        0.056265   \n",
       "2       0.571842      0.112156         0.127510        0.001489   \n",
       "\n",
       "                                  param_preprocessor param_oversampler  \\\n",
       "0  FunctionTransformer(func=<bound method Sentime...       passthrough   \n",
       "1  FunctionTransformer(func=<bound method Sentime...       passthrough   \n",
       "2  FunctionTransformer(func=<bound method Sentime...       passthrough   \n",
       "\n",
       "  param_estimator__penalty param_estimator__C  \\\n",
       "0               elasticnet                  1   \n",
       "1                     none                  2   \n",
       "2                     none                  1   \n",
       "\n",
       "                                              params  split0_test_accuracy  \\\n",
       "0  {'preprocessor': FunctionTransformer(func=<bou...              0.499009   \n",
       "1  {'preprocessor': FunctionTransformer(func=<bou...                   NaN   \n",
       "2  {'preprocessor': FunctionTransformer(func=<bou...                   NaN   \n",
       "\n",
       "   ...  split0_test_f1_weighted  split1_test_f1_weighted  \\\n",
       "0  ...                 0.441208                 0.445563   \n",
       "1  ...                      NaN                      NaN   \n",
       "2  ...                      NaN                      NaN   \n",
       "\n",
       "   mean_test_f1_weighted  std_test_f1_weighted  rank_test_f1_weighted  \\\n",
       "0               0.443386              0.002178                      1   \n",
       "1                    NaN                   NaN                      2   \n",
       "2                    NaN                   NaN                      3   \n",
       "\n",
       "   split0_test_roc_auc_ovo_weighted  split1_test_roc_auc_ovo_weighted  \\\n",
       "0                          0.759383                          0.768054   \n",
       "1                               NaN                               NaN   \n",
       "2                               NaN                               NaN   \n",
       "\n",
       "   mean_test_roc_auc_ovo_weighted  std_test_roc_auc_ovo_weighted  \\\n",
       "0                        0.763719                       0.004336   \n",
       "1                             NaN                            NaN   \n",
       "2                             NaN                            NaN   \n",
       "\n",
       "   rank_test_roc_auc_ovo_weighted  \n",
       "0                               1  \n",
       "1                               2  \n",
       "2                               3  \n",
       "\n",
       "[3 rows x 29 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment.experiment_results[\"run_1\"][\"Logistic Regression\"][\"optimization_cv_results\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Logistic Regression\n",
      "-----------------\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ValentinStudium\\Envs\\sma\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:702: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         15940 function calls (15439 primitive calls) in 59.862 seconds\n",
      "\n",
      "   Ordered by: standard name\n",
      "\n",
      "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
      "        1    0.001    0.001   59.861   59.861 3837047588.py:214(evaluate_classifier)\n",
      "        1    0.000    0.000    0.000    0.000 3837047588.py:216(Debug)\n",
      "        1    0.000    0.000   59.861   59.861 3837047588.py:269(run_experiments)\n",
      "        1    0.000    0.000    0.000    0.000 3837047588.py:275(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 <__array_function__ internals>:177(all)\n",
      "        1    0.000    0.000    0.000    0.000 <__array_function__ internals>:177(amin)\n",
      "        1    0.000    0.000    0.000    0.000 <__array_function__ internals>:177(any)\n",
      "        3    0.000    0.000    0.000    0.000 <__array_function__ internals>:177(bincount)\n",
      "        3    0.000    0.000    0.000    0.000 <__array_function__ internals>:177(concatenate)\n",
      "        2    0.000    0.000    0.000    0.000 <__array_function__ internals>:177(cumsum)\n",
      "        3    0.000    0.000    0.000    0.000 <__array_function__ internals>:177(prod)\n",
      "        3    0.000    0.000    0.000    0.000 <__array_function__ internals>:177(product)\n",
      "        1    0.000    0.000    0.000    0.000 <__array_function__ internals>:177(ravel)\n",
      "        1    0.000    0.000    0.000    0.000 <__array_function__ internals>:177(shape)\n",
      "        1    0.000    0.000    0.000    0.000 <__array_function__ internals>:177(sort)\n",
      "        1    0.000    0.000    0.000    0.000 <__array_function__ internals>:177(sum)\n",
      "        6    0.000    0.000    0.002    0.000 <__array_function__ internals>:177(unique)\n",
      "        1    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:1033(_handle_fromlist)\n",
      "        2    0.000    0.000    0.000    0.000 <frozen importlib._bootstrap>:398(parent)\n",
      "        1    0.000    0.000    0.000    0.000 <string>:1(<module>)\n",
      "        1    0.000    0.000    0.001    0.001 __init__.py:581(__init__)\n",
      "        1    0.001    0.001    0.001    0.001 __init__.py:649(update)\n",
      "       11    0.000    0.000    0.000    0.000 _base.py:1291(isspmatrix)\n",
      "        6    0.000    0.000    0.000    0.000 _base.py:317(__init__)\n",
      "        6    0.000    0.000    0.000    0.000 _base.py:387(__get_result)\n",
      "        6    0.000    0.000    0.000    0.000 _base.py:397(add_done_callback)\n",
      "        6    0.000    0.000   59.268    9.878 _base.py:417(result)\n",
      "       10    0.000    0.000    0.000    0.000 _collections_abc.py:760(get)\n",
      "        1    0.000    0.000    0.000    0.000 _collections_abc.py:767(__contains__)\n",
      "        7    0.000    0.000    0.000    0.000 _config.py:20(_get_threadlocal_config)\n",
      "        7    0.000    0.000    0.000    0.000 _config.py:28(get_config)\n",
      "        7    0.000    0.000    0.000    0.000 _logistic.py:1026(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 _memmapping_reducer.py:115(__init__)\n",
      "        2    0.000    0.000    0.001    0.000 _memmapping_reducer.py:168(_get_temp_dir)\n",
      "        1    0.000    0.000    0.000    0.000 _memmapping_reducer.py:338(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 _memmapping_reducer.py:473(get_memmapping_reducers)\n",
      "        1    0.000    0.000    0.001    0.001 _memmapping_reducer.py:518(__init__)\n",
      "        1    0.000    0.000    0.001    0.001 _memmapping_reducer.py:533(set_current_context)\n",
      "        2    0.000    0.000    0.001    0.001 _memmapping_reducer.py:537(register_new_context)\n",
      "        2    0.000    0.000    0.001    0.000 _memmapping_reducer.py:583(register_folder_finalizer)\n",
      "        1    0.000    0.000    0.556    0.556 _memmapping_reducer.py:612(_unlink_temporary_resources)\n",
      "        1    0.000    0.000    0.556    0.556 _memmapping_reducer.py:644(_try_delete_folder)\n",
      "        6    0.000    0.000    0.000    0.000 _parallel_backends.py:124(get_nested_backend)\n",
      "        2    0.000    0.000    0.000    0.000 _parallel_backends.py:137(retrieval_context)\n",
      "        1    0.000    0.000    0.001    0.001 _parallel_backends.py:154(_prepare_worker_env)\n",
      "        2    0.000    0.000    0.000    0.000 _parallel_backends.py:185(in_main_thread)\n",
      "        1    0.000    0.000    0.000    0.000 _parallel_backends.py:280(__init__)\n",
      "        7    0.000    0.000    0.000    0.000 _parallel_backends.py:285(compute_batch_size)\n",
      "        7    0.000    0.000    0.000    0.000 _parallel_backends.py:34(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 _parallel_backends.py:361(reset_batch_stats)\n",
      "        1    0.000    0.000    0.002    0.002 _parallel_backends.py:486(configure)\n",
      "        2    0.000    0.000    0.000    0.000 _parallel_backends.py:501(effective_n_jobs)\n",
      "        6    0.000    0.000    0.001    0.000 _parallel_backends.py:529(apply_async)\n",
      "        6    0.000    0.000   59.268    9.878 _parallel_backends.py:537(wrap_future_result)\n",
      "        1    0.000    0.000    0.557    0.557 _parallel_backends.py:546(terminate)\n",
      "        6    0.000    0.000    0.000    0.000 _parallel_backends.py:590(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 _parallel_backends.py:80(start_call)\n",
      "        1    0.000    0.000    0.000    0.000 _parallel_backends.py:83(stop_call)\n",
      "        4    0.000    0.000    0.001    0.000 _scorer.py:390(get_scorer)\n",
      "        4    0.000    0.000    0.002    0.000 _scorer.py:432(check_scoring)\n",
      "        1    0.000    0.000    0.002    0.002 _scorer.py:505(_check_multimetric_scoring)\n",
      "        5    0.000    0.000    0.000    0.000 _scorer.py:552(<genexpr>)\n",
      "        1    0.000    0.000    0.002    0.002 _scorer.py:565(<dictcomp>)\n",
      "        3    0.000    0.000    0.000    0.000 _search.py:152(__len__)\n",
      "        6    0.000    0.000    0.000    0.000 _search.py:156(<genexpr>)\n",
      "       15    0.000    0.000    0.000    0.000 _search.py:157(<genexpr>)\n",
      "        3    0.000    0.000    0.000    0.000 _search.py:160(__getitem__)\n",
      "        1    0.000    0.000    0.000    0.000 _search.py:1716(__init__)\n",
      "        1    0.000    0.000   59.286   59.286 _search.py:1747(_run_search)\n",
      "        3    0.000    0.000    0.000    0.000 _search.py:186(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 _search.py:258(__init__)\n",
      "        3    0.000    0.000    0.000    0.000 _search.py:288(_is_all_lists)\n",
      "        6    0.000    0.000    0.000    0.000 _search.py:289(<genexpr>)\n",
      "       15    0.000    0.000    0.000    0.000 _search.py:290(<genexpr>)\n",
      "        4    0.001    0.000    0.001    0.000 _search.py:294(__iter__)\n",
      "        2    0.000    0.000    0.001    0.000 _search.py:329(__len__)\n",
      "        1    0.000    0.000    0.000    0.000 _search.py:375(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 _search.py:703(_check_refit_for_multimetric)\n",
      "        1    0.001    0.001   59.859   59.859 _search.py:738(fit)\n",
      "        1    0.000    0.000   59.286   59.286 _search.py:809(evaluate_candidates)\n",
      "        7    0.000    0.000    0.006    0.001 _search.py:822(<genexpr>)\n",
      "        3    0.000    0.000    0.000    0.000 _search.py:93(__init__)\n",
      "        2    0.000    0.000    0.001    0.001 _split.py:1380(__init__)\n",
      "        6    0.000    0.000    0.000    0.000 _split.py:1387(<genexpr>)\n",
      "        3    0.000    0.000    0.004    0.001 _split.py:1395(split)\n",
      "        1    0.000    0.000    0.004    0.004 _split.py:1427(get_n_splits)\n",
      "        2    0.000    0.000    0.001    0.001 _split.py:1561(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 _split.py:2275(check_cv)\n",
      "        2    0.000    0.000    0.000    0.000 _split.py:276(__init__)\n",
      "        3    0.000    0.000    0.002    0.001 _split.py:306(split)\n",
      "        1    0.000    0.000    0.000    0.000 _split.py:343(get_n_splits)\n",
      "        3    0.000    0.000    0.002    0.001 _split.py:60(split)\n",
      "        2    0.000    0.000    0.000    0.000 _split.py:647(__init__)\n",
      "        1    0.001    0.001    0.002    0.002 _split.py:650(_make_test_folds)\n",
      "        1    0.000    0.000    0.000    0.000 _split.py:692(<listcomp>)\n",
      "        3    0.000    0.000    0.002    0.001 _split.py:712(_iter_test_masks)\n",
      "        1    0.000    0.000    0.001    0.001 _split.py:717(split)\n",
      "        6    0.000    0.000    0.000    0.000 _ufunc_config.py:131(geterr)\n",
      "        6    0.000    0.000    0.000    0.000 _ufunc_config.py:32(seterr)\n",
      "        3    0.000    0.000    0.000    0.000 _ufunc_config.py:429(__enter__)\n",
      "        3    0.000    0.000    0.000    0.000 _ufunc_config.py:434(__exit__)\n",
      "        1    0.001    0.001    0.002    0.002 _validation.py:346(_warn_or_raise_about_fit_failures)\n",
      "        1    0.000    0.000    0.000    0.000 _validation.py:347(<listcomp>)\n",
      "        2    0.000    0.000    0.000    0.000 _validation.py:355(<genexpr>)\n",
      "        8    0.000    0.000    0.000    0.000 _validation.py:577(inner_f)\n",
      "        8    0.000    0.000    0.000    0.000 _validation.py:592(<dictcomp>)\n",
      "       51    0.000    0.000    0.002    0.000 abc.py:117(__instancecheck__)\n",
      "       16    0.000    0.000    0.000    0.000 abc.py:121(__subclasscheck__)\n",
      "        2    0.000    0.000    0.000    0.000 accessor.py:178(__get__)\n",
      "        2    0.000    0.000    0.000    0.000 accessor.py:20(__init__)\n",
      "        2    0.000    0.000    0.000    0.000 accessor.py:36(_validate)\n",
      "        6    0.000    0.000    0.000    0.000 arraysetops.py:125(_unpack_tuple)\n",
      "        6    0.000    0.000    0.000    0.000 arraysetops.py:133(_unique_dispatcher)\n",
      "        6    0.000    0.000    0.002    0.000 arraysetops.py:138(unique)\n",
      "        6    0.001    0.000    0.001    0.000 arraysetops.py:320(_unique1d)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:1001(is_classifier)\n",
      "        8    0.000    0.000    0.000    0.000 base.py:125(__init__)\n",
      "       84    0.000    0.000    0.005    0.000 base.py:163(_get_param_names)\n",
      "       56    0.000    0.000    0.000    0.000 base.py:177(<listcomp>)\n",
      "       56    0.000    0.000    0.000    0.000 base.py:192(<listcomp>)\n",
      "       84    0.000    0.000    0.005    0.000 base.py:194(get_params)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:2432(is_object)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:2473(is_categorical)\n",
      "        8    0.000    0.000    0.000    0.000 base.py:295(__init__)\n",
      "        5    0.000    0.000    0.000    0.000 base.py:315(shape)\n",
      "    399/7    0.001    0.000    0.007    0.001 base.py:32(clone)\n",
      "        8    0.000    0.000    0.000    0.000 base.py:36(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:4973(__contains__)\n",
      "        1    0.000    0.000    0.000    0.000 base.py:5078(_can_hold_identifiers_and_holds_name)\n",
      "     49/7    0.000    0.000    0.005    0.001 base.py:65(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 common.py:145(classes)\n",
      "        1    0.000    0.000    0.000    0.000 common.py:147(<lambda>)\n",
      "        1    0.000    0.000    0.000    0.000 common.py:1587(_is_dtype_type)\n",
      "        1    0.000    0.000    0.000    0.000 common.py:161(is_object_dtype)\n",
      "        1    0.000    0.000    0.000    0.000 common.py:346(apply_if_callable)\n",
      "        6    0.000    0.000    0.000    0.000 connection.py:139(_check_closed)\n",
      "        6    0.000    0.000    0.000    0.000 connection.py:147(_check_writable)\n",
      "        6    0.000    0.000    0.000    0.000 connection.py:186(send_bytes)\n",
      "        6    0.000    0.000    0.000    0.000 connection.py:284(_send_bytes)\n",
      "        1    0.000    0.000    0.001    0.001 context.py:110(cpu_count)\n",
      "        1    0.000    0.000    0.001    0.001 context.py:169(_cpu_count_user)\n",
      "        1    0.000    0.000    0.000    0.000 context.py:233(get_context)\n",
      "        1    0.000    0.000    0.000    0.000 context.py:41(cpu_count)\n",
      "        1    0.000    0.000    0.000    0.000 contextlib.py:114(__enter__)\n",
      "        1    0.000    0.000    0.000    0.000 contextlib.py:123(__exit__)\n",
      "        1    0.000    0.000    0.000    0.000 contextlib.py:261(helper)\n",
      "        3    0.000    0.000    0.000    0.000 contextlib.py:63(_recreate_cm)\n",
      "        3    0.000    0.000    0.001    0.000 contextlib.py:76(inner)\n",
      "        1    0.000    0.000    0.000    0.000 contextlib.py:86(__init__)\n",
      "  352/312    0.001    0.000    0.002    0.000 copy.py:128(deepcopy)\n",
      "      340    0.000    0.000    0.000    0.000 copy.py:182(_deepcopy_atomic)\n",
      "      8/4    0.000    0.000    0.000    0.000 copy.py:226(_deepcopy_dict)\n",
      "       12    0.000    0.000    0.000    0.000 copy.py:242(_keep_alive)\n",
      "        4    0.000    0.000    0.001    0.000 copy.py:258(_reconstruct)\n",
      "        8    0.000    0.000    0.000    0.000 copy.py:263(<genexpr>)\n",
      "        4    0.000    0.000    0.000    0.000 copyreg.py:94(__newobj__)\n",
      "        1    0.000    0.000    0.555    0.555 disk.py:105(delete_folder)\n",
      "        1    0.000    0.000    0.000    0.000 disk.py:42(memstr_to_bytes)\n",
      "      602    0.000    0.000    0.000    0.000 enum.py:358(__call__)\n",
      "      602    0.000    0.000    0.000    0.000 enum.py:670(__new__)\n",
      "        1    0.000    0.000    0.002    0.002 executor.py:19(get_memmapping_executor)\n",
      "        1    0.000    0.000    0.002    0.002 executor.py:25(get_memmapping_executor)\n",
      "        1    0.000    0.000    0.000    0.000 extmath.py:870(_safe_accumulator_op)\n",
      "        6    0.000    0.000    0.000    0.000 fixes.py:100(delayed_function)\n",
      "        6    0.000    0.000    0.000    0.000 fixes.py:110(__init__)\n",
      "        6    0.000    0.000    0.000    0.000 fixes.py:97(delayed)\n",
      "        1    0.000    0.000    0.000    0.000 frame.py:1413(__len__)\n",
      "        1    0.000    0.000    0.000    0.000 frame.py:3463(__getitem__)\n",
      "        1    0.000    0.000    0.000    0.000 frame.py:3923(_get_item_cache)\n",
      "        1    0.000    0.000    0.000    0.000 fromnumeric.py:1751(_ravel_dispatcher)\n",
      "        1    0.000    0.000    0.000    0.000 fromnumeric.py:1755(ravel)\n",
      "        1    0.000    0.000    0.000    0.000 fromnumeric.py:1961(_shape_dispatcher)\n",
      "        1    0.000    0.000    0.000    0.000 fromnumeric.py:1965(shape)\n",
      "        1    0.000    0.000    0.000    0.000 fromnumeric.py:2155(_sum_dispatcher)\n",
      "        1    0.000    0.000    0.000    0.000 fromnumeric.py:2160(sum)\n",
      "        1    0.000    0.000    0.000    0.000 fromnumeric.py:2300(_any_dispatcher)\n",
      "        1    0.000    0.000    0.000    0.000 fromnumeric.py:2305(any)\n",
      "        1    0.000    0.000    0.000    0.000 fromnumeric.py:2399(_all_dispatcher)\n",
      "        1    0.000    0.000    0.000    0.000 fromnumeric.py:2404(all)\n",
      "        2    0.000    0.000    0.000    0.000 fromnumeric.py:2491(_cumsum_dispatcher)\n",
      "        2    0.000    0.000    0.000    0.000 fromnumeric.py:2495(cumsum)\n",
      "        1    0.000    0.000    0.000    0.000 fromnumeric.py:2795(_amin_dispatcher)\n",
      "        1    0.000    0.000    0.000    0.000 fromnumeric.py:2800(amin)\n",
      "        6    0.000    0.000    0.000    0.000 fromnumeric.py:2965(_prod_dispatcher)\n",
      "        3    0.000    0.000    0.000    0.000 fromnumeric.py:2970(prod)\n",
      "        3    0.000    0.000    0.000    0.000 fromnumeric.py:3776(product)\n",
      "        2    0.000    0.000    0.000    0.000 fromnumeric.py:51(_wrapfunc)\n",
      "        7    0.000    0.000    0.000    0.000 fromnumeric.py:69(_wrapreduction)\n",
      "        7    0.000    0.000    0.000    0.000 fromnumeric.py:70(<dictcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 fromnumeric.py:848(_sort_dispatcher)\n",
      "        1    0.000    0.000    0.000    0.000 fromnumeric.py:852(sort)\n",
      "       12    0.000    0.000    0.000    0.000 functools.py:35(update_wrapper)\n",
      "        6    0.000    0.000    0.000    0.000 functools.py:65(wraps)\n",
      "        4    0.000    0.000    0.000    0.000 generic.py:5569(__getattr__)\n",
      "        1    0.000    0.000    0.000    0.000 generic.py:636(_info_axis)\n",
      "        4    0.000    0.000    0.001    0.000 genericpath.py:16(exists)\n",
      "        1    0.001    0.001    0.001    0.001 genericpath.py:39(isdir)\n",
      "        1    0.000    0.000    0.000    0.000 indexing.py:2474(check_deprecated_indexers)\n",
      "        1    0.000    0.000    0.000    0.000 inference.py:321(is_hashable)\n",
      "      112    0.000    0.000    0.000    0.000 inspect.py:159(isfunction)\n",
      "       56    0.001    0.000    0.003    0.000 inspect.py:2150(_signature_from_function)\n",
      "       56    0.000    0.000    0.004    0.000 inspect.py:2244(_signature_from_callable)\n",
      "       14    0.000    0.000    0.000    0.000 inspect.py:2273(<lambda>)\n",
      "      602    0.001    0.000    0.001    0.000 inspect.py:2496(__init__)\n",
      "     1750    0.000    0.000    0.000    0.000 inspect.py:2546(name)\n",
      "     1092    0.000    0.000    0.000    0.000 inspect.py:2558(kind)\n",
      "       56    0.000    0.000    0.001    0.000 inspect.py:2779(__init__)\n",
      "      658    0.000    0.000    0.000    0.000 inspect.py:2828(<genexpr>)\n",
      "       56    0.000    0.000    0.004    0.000 inspect.py:2857(from_callable)\n",
      "       64    0.000    0.000    0.000    0.000 inspect.py:2863(parameters)\n",
      "       56    0.000    0.000    0.004    0.000 inspect.py:3109(signature)\n",
      "       56    0.000    0.000    0.000    0.000 inspect.py:494(unwrap)\n",
      "       70    0.000    0.000    0.000    0.000 inspect.py:514(_is_wrapper)\n",
      "        1    0.000    0.000    0.000    0.000 iostream.py:208(schedule)\n",
      "        8    0.000    0.000    0.000    0.000 iostream.py:421(_is_master_process)\n",
      "        8    0.000    0.000    0.000    0.000 iostream.py:440(_schedule_flush)\n",
      "        8    0.000    0.000    0.000    0.000 iostream.py:508(write)\n",
      "        1    0.000    0.000    0.000    0.000 iostream.py:97(_event_pipe)\n",
      "        1    0.000    0.000    0.000    0.000 logger.py:23(_squeeze_time)\n",
      "        1    0.000    0.000    0.000    0.000 logger.py:39(short_format_time)\n",
      "        5    0.000    0.000    0.000    0.000 managers.py:1837(dtype)\n",
      "        6    0.000    0.000    0.000    0.000 managers.py:1848(internal_values)\n",
      "       14    0.000    0.000    0.001    0.000 metaestimators.py:32(_get_params)\n",
      "        3    0.000    0.000    0.000    0.000 multiarray.py:148(concatenate)\n",
      "        3    0.000    0.000    0.000    0.000 multiarray.py:883(bincount)\n",
      "        1    0.000    0.000    0.000    0.000 multiclass.py:118(is_multilabel)\n",
      "        1    0.000    0.000    0.001    0.001 multiclass.py:203(type_of_target)\n",
      "       24    0.000    0.000    0.000    0.000 ntpath.py:124(splitdrive)\n",
      "        2    0.000    0.000    0.000    0.000 ntpath.py:289(expanduser)\n",
      "        2    0.000    0.000    0.000    0.000 ntpath.py:450(normpath)\n",
      "        2    0.000    0.000    0.000    0.000 ntpath.py:524(abspath)\n",
      "       11    0.000    0.000    0.000    0.000 ntpath.py:77(join)\n",
      "        2    0.000    0.000    0.000    0.000 numerictypes.py:282(issubclass_)\n",
      "        1    0.000    0.000    0.000    0.000 numerictypes.py:356(issubdtype)\n",
      "       11    0.000    0.000    0.000    0.000 os.py:674(__getitem__)\n",
      "       11    0.000    0.000    0.000    0.000 os.py:740(check_str)\n",
      "       11    0.000    0.000    0.000    0.000 os.py:746(encodekey)\n",
      "        6    0.000    0.000    0.000    0.000 parallel.py:245(__init__)\n",
      "       18    0.000    0.000    0.000    0.000 parallel.py:275(__len__)\n",
      "        6    0.000    0.000    0.000    0.000 parallel.py:345(__init__)\n",
      "        1    0.001    0.001    0.002    0.002 parallel.py:639(__init__)\n",
      "        1    0.000    0.000    0.002    0.002 parallel.py:723(__enter__)\n",
      "        1    0.000    0.000    0.557    0.557 parallel.py:728(__exit__)\n",
      "        1    0.000    0.000    0.002    0.002 parallel.py:732(_initialize_backend)\n",
      "        1    0.000    0.000    0.000    0.000 parallel.py:752(_effective_n_jobs)\n",
      "        1    0.000    0.000    0.557    0.557 parallel.py:757(_terminate_backend)\n",
      "        1    0.000    0.000    0.000    0.000 parallel.py:76(get_active_backend)\n",
      "        6    0.000    0.000    0.002    0.000 parallel.py:761(_dispatch)\n",
      "        7    0.000    0.000    0.008    0.001 parallel.py:798(dispatch_one_batch)\n",
      "        2    0.000    0.000    0.000    0.000 parallel.py:864(_print)\n",
      "        1    0.000    0.000   59.269   59.269 parallel.py:920(retrieve)\n",
      "        1    0.001    0.001   59.278   59.278 parallel.py:960(__call__)\n",
      "        2    0.000    0.000    0.000    0.000 pickle.py:335(whichmodule)\n",
      "        8    0.000    0.000    0.000    0.000 pipeline.py:145(__init__)\n",
      "       14    0.000    0.000    0.001    0.000 pipeline.py:150(get_params)\n",
      "        1    0.000    0.000    0.000    0.000 pipeline.py:271(_estimator_type)\n",
      "        2    0.000    0.000    0.000    0.000 process.py:198(daemon)\n",
      "        2    0.000    0.000    0.000    0.000 process.py:37(current_process)\n",
      "        6    0.000    0.000    0.000    0.000 process_executor.py:1047(_start_executor_manager_thread)\n",
      "        6    0.000    0.000    0.000    0.000 process_executor.py:1104(_ensure_executor_running)\n",
      "        6    0.000    0.000    0.001    0.000 process_executor.py:1112(submit)\n",
      "        6    0.000    0.000    0.001    0.000 process_executor.py:138(wakeup)\n",
      "        6    0.000    0.000    0.000    0.000 process_executor.py:260(__init__)\n",
      "       12    0.000    0.000    0.000    0.000 queue.py:122(put)\n",
      "        8    0.000    0.000    0.000    0.000 queue.py:154(get)\n",
      "        1    0.000    0.000    0.000    0.000 queue.py:206(_init)\n",
      "        8    0.000    0.000    0.000    0.000 queue.py:209(_qsize)\n",
      "       12    0.000    0.000    0.000    0.000 queue.py:213(_put)\n",
      "        6    0.000    0.000    0.000    0.000 queue.py:217(_get)\n",
      "        1    0.000    0.000    0.000    0.000 queue.py:34(__init__)\n",
      "        3    0.000    0.000    0.001    0.000 random.py:791(getrandbits)\n",
      "        1    0.000    0.000    0.000    0.000 range.py:347(dtype)\n",
      "        1    0.000    0.000    0.000    0.000 range.py:372(inferred_type)\n",
      "        1    0.000    0.000    0.000    0.000 range.py:909(__len__)\n",
      "       11    0.000    0.000    0.000    0.000 resource_tracker.py:179(_check_alive)\n",
      "        2    0.000    0.000    0.000    0.000 resource_tracker.py:188(register)\n",
      "        9    0.000    0.000    0.000    0.000 resource_tracker.py:198(maybe_unlink)\n",
      "       22    0.000    0.000    0.000    0.000 resource_tracker.py:203(_send)\n",
      "       11    0.000    0.000    0.000    0.000 resource_tracker.py:94(ensure_running)\n",
      "        1    0.000    0.000    0.000    0.000 reusable_executor.py:105(get_reusable_executor)\n",
      "        6    0.000    0.000    0.001    0.000 reusable_executor.py:175(submit)\n",
      "        1    0.000    0.000    0.000    0.000 reusable_executor.py:180(_resize)\n",
      "        5    0.000    0.000    0.000    0.000 series.py:575(dtype)\n",
      "        2    0.000    0.000    0.000    0.000 series.py:582(dtypes)\n",
      "        6    0.000    0.000    0.000    0.000 series.py:687(_values)\n",
      "        1    0.000    0.000    0.000    0.000 series.py:825(__array__)\n",
      "        1    0.000    0.000    0.000    0.000 socket.py:480(send)\n",
      "        6    0.000    0.000    0.000    0.000 synchronize.py:94(__enter__)\n",
      "        6    0.000    0.000    0.000    0.000 synchronize.py:97(__exit__)\n",
      "        2    0.000    0.000    0.000    0.000 tempfile.py:280(gettempdir)\n",
      "        8    0.000    0.000    0.000    0.000 text.py:1098(__init__)\n",
      "        1    0.000    0.000    0.000    0.000 threading.py:1059(_wait_for_tstate_lock)\n",
      "        1    0.000    0.000    0.000    0.000 threading.py:1113(is_alive)\n",
      "        2    0.000    0.000    0.000    0.000 threading.py:1338(current_thread)\n",
      "        9    0.000    0.000    0.000    0.000 threading.py:228(__init__)\n",
      "       32    0.000    0.000    0.000    0.000 threading.py:256(__enter__)\n",
      "       32    0.000    0.000    0.000    0.000 threading.py:259(__exit__)\n",
      "       18    0.000    0.000    0.000    0.000 threading.py:271(_is_owned)\n",
      "        6    0.000    0.000   59.268    9.878 threading.py:280(wait)\n",
      "       18    0.000    0.000    0.000    0.000 threading.py:351(notify)\n",
      "        1    0.000    0.000    0.000    0.000 threading.py:529(is_set)\n",
      "        7    0.000    0.000    0.000    0.000 threading.py:82(RLock)\n",
      "        1    0.000    0.000    0.000    0.000 util.py:48(debug)\n",
      "        3    0.000    0.000    0.000    0.000 uuid.py:138(__init__)\n",
      "        3    0.000    0.000    0.000    0.000 uuid.py:333(hex)\n",
      "        3    0.000    0.000    0.001    0.000 uuid.py:713(uuid4)\n",
      "        1    0.000    0.000    0.000    0.000 validation.py:1120(column_or_1d)\n",
      "        5    0.002    0.000    0.004    0.001 validation.py:1161(check_random_state)\n",
      "        1    0.000    0.000    0.001    0.001 validation.py:1784(_check_fit_params)\n",
      "        9    0.000    0.000    0.000    0.000 validation.py:310(_num_samples)\n",
      "        3    0.001    0.000    0.003    0.001 validation.py:373(check_consistent_length)\n",
      "        3    0.000    0.000    0.000    0.000 validation.py:384(<listcomp>)\n",
      "        9    0.000    0.000    0.000    0.000 validation.py:393(_make_indexable)\n",
      "        3    0.000    0.000    0.003    0.001 validation.py:413(indexable)\n",
      "        3    0.000    0.000    0.000    0.000 validation.py:432(<listcomp>)\n",
      "        1    0.000    0.000    0.000    0.000 validation.py:561(_ensure_no_complex_data)\n",
      "        1    0.000    0.000    0.000    0.000 validation.py:571(_check_estimator_name)\n",
      "        1    0.000    0.000    0.001    0.001 validation.py:619(check_array)\n",
      "        1    0.000    0.000    0.000    0.000 validation.py:93(_assert_all_finite)\n",
      "        3    0.000    0.000    0.000    0.000 warnings.py:165(simplefilter)\n",
      "        3    0.000    0.000    0.000    0.000 warnings.py:181(_add_filter)\n",
      "        3    0.000    0.000    0.000    0.000 warnings.py:437(__init__)\n",
      "        3    0.000    0.000    0.000    0.000 warnings.py:458(__enter__)\n",
      "        3    0.000    0.000    0.000    0.000 warnings.py:477(__exit__)\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method __new__ of type object at 0x00007FFBD2F5CC60}\n",
      "       51    0.002    0.000    0.002    0.000 {built-in method _abc._abc_instancecheck}\n",
      "       16    0.000    0.000    0.000    0.000 {built-in method _abc._abc_subclasscheck}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method _collections._count_elements}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method _stat.S_ISDIR}\n",
      "        7    0.000    0.000    0.000    0.000 {built-in method _thread.allocate_lock}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method _thread.get_ident}\n",
      "        9    0.000    0.000    0.000    0.000 {built-in method _warnings._filters_mutated}\n",
      "        6    0.000    0.000    0.000    0.000 {built-in method _winapi.WriteFile}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method atexit.register}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.__build_class__}\n",
      "      7/4    0.000    0.000    0.000    0.000 {built-in method builtins.all}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.any}\n",
      "       58    0.000    0.000    0.000    0.000 {built-in method builtins.callable}\n",
      "       12    0.000    0.000    0.000    0.000 {built-in method builtins.divmod}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.eval}\n",
      "        1    0.001    0.001   59.862   59.862 {built-in method builtins.exec}\n",
      "      804    0.000    0.000    0.000    0.000 {built-in method builtins.getattr}\n",
      "      517    0.001    0.000    0.001    0.000 {built-in method builtins.hasattr}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.hash}\n",
      "      450    0.000    0.000    0.000    0.000 {built-in method builtins.id}\n",
      "     1070    0.000    0.000    0.002    0.000 {built-in method builtins.isinstance}\n",
      "        8    0.000    0.000    0.000    0.000 {built-in method builtins.issubclass}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method builtins.iter}\n",
      "  293/279    0.000    0.000    0.000    0.000 {built-in method builtins.len}\n",
      "        5    0.000    0.000    0.000    0.000 {built-in method builtins.max}\n",
      "        4    0.000    0.000    0.000    0.000 {built-in method builtins.min}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method builtins.next}\n",
      "        5    0.000    0.000    0.000    0.000 {built-in method builtins.print}\n",
      "       60    0.000    0.000    0.000    0.000 {built-in method builtins.setattr}\n",
      "       59    0.000    0.000    0.000    0.000 {built-in method builtins.sorted}\n",
      "        3    0.000    0.000    0.000    0.000 {built-in method builtins.sum}\n",
      "        6    0.000    0.000    0.000    0.000 {built-in method from_bytes}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method nt._getfullpathname}\n",
      "        2    0.000    0.000    0.000    0.000 {built-in method nt.cpu_count}\n",
      "       39    0.000    0.000    0.000    0.000 {built-in method nt.fspath}\n",
      "       10    0.000    0.000    0.000    0.000 {built-in method nt.getpid}\n",
      "        7    0.002    0.000    0.002    0.000 {built-in method nt.listdir}\n",
      "        5    0.001    0.000    0.001    0.000 {built-in method nt.stat}\n",
      "        6    0.001    0.000    0.001    0.000 {built-in method nt.urandom}\n",
      "       22    0.000    0.000    0.000    0.000 {built-in method nt.write}\n",
      "        6    0.000    0.000    0.000    0.000 {built-in method numpy.arange}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method numpy.array}\n",
      "       15    0.000    0.000    0.000    0.000 {built-in method numpy.asanyarray}\n",
      "      7/6    0.000    0.000    0.000    0.000 {built-in method numpy.asarray}\n",
      "    27/22    0.000    0.000    0.002    0.000 {built-in method numpy.core._multiarray_umath.implement_array_function}\n",
      "        9    0.000    0.000    0.000    0.000 {built-in method numpy.empty}\n",
      "       12    0.000    0.000    0.000    0.000 {built-in method numpy.geterrobj}\n",
      "        6    0.000    0.000    0.000    0.000 {built-in method numpy.seterrobj}\n",
      "        1    0.000    0.000    0.000    0.000 {built-in method sklearn.utils._random.sample_without_replacement}\n",
      "       56    0.000    0.000    0.000    0.000 {built-in method sys.getrecursionlimit}\n",
      "        5    0.552    0.110    0.552    0.110 {built-in method time.sleep}\n",
      "        9    0.000    0.000    0.000    0.000 {built-in method time.time}\n",
      "        3    0.000    0.000    0.000    0.000 {function SeedSequence.generate_state at 0x0000028B9E82E3A0}\n",
      "        6    0.000    0.000    0.000    0.000 {method 'GetOverlappedResult' of '_winapi.Overlapped' objects}\n",
      "        6    0.000    0.000    0.000    0.000 {method '__enter__' of '_multiprocessing.SemLock' objects}\n",
      "       12    0.000    0.000    0.000    0.000 {method '__enter__' of '_thread.RLock' objects}\n",
      "       20    0.000    0.000    0.000    0.000 {method '__enter__' of '_thread.lock' objects}\n",
      "        6    0.000    0.000    0.000    0.000 {method '__exit__' of '_multiprocessing.SemLock' objects}\n",
      "       47    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.RLock' objects}\n",
      "       37    0.000    0.000    0.000    0.000 {method '__exit__' of '_thread.lock' objects}\n",
      "        4    0.000    0.000    0.000    0.000 {method '__reduce_ex__' of 'object' objects}\n",
      "        6    0.000    0.000    0.000    0.000 {method '_acquire_restore' of '_thread.RLock' objects}\n",
      "        6    0.000    0.000    0.000    0.000 {method '_is_owned' of '_thread.RLock' objects}\n",
      "        6    0.000    0.000    0.000    0.000 {method '_release_save' of '_thread.RLock' objects}\n",
      "       31   59.267    1.912   59.267    1.912 {method 'acquire' of '_thread.lock' objects}\n",
      "       19    0.000    0.000    0.000    0.000 {method 'append' of 'collections.deque' objects}\n",
      "      616    0.000    0.000    0.000    0.000 {method 'append' of 'list' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'argsort' of 'numpy.ndarray' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'astype' of 'numpy.ndarray' objects}\n",
      "        8    0.000    0.000    0.000    0.000 {method 'copy' of 'dict' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'copy' of 'numpy.ndarray' objects}\n",
      "        3    0.000    0.000    0.000    0.000 {method 'count' of 'list' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'cumsum' of 'numpy.ndarray' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'disable' of '_lsprof.Profiler' objects}\n",
      "       22    0.000    0.000    0.000    0.000 {method 'encode' of 'str' objects}\n",
      "        6    0.000    0.000    0.000    0.000 {method 'extend' of 'list' objects}\n",
      "        6    0.000    0.000    0.000    0.000 {method 'flatten' of 'numpy.ndarray' objects}\n",
      "       32    0.000    0.000    0.000    0.000 {method 'format' of 'str' objects}\n",
      "     1887    0.000    0.000    0.000    0.000 {method 'get' of 'dict' objects}\n",
      "        9    0.000    0.000    0.000    0.000 {method 'insert' of 'list' objects}\n",
      "      602    0.000    0.000    0.000    0.000 {method 'isidentifier' of 'str' objects}\n",
      "       65    0.000    0.000    0.000    0.000 {method 'items' of 'dict' objects}\n",
      "        3    0.000    0.000    0.000    0.000 {method 'join' of 'str' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'lstrip' of 'str' objects}\n",
      "        6    0.000    0.000    0.000    0.000 {method 'pop' of 'list' objects}\n",
      "        6    0.000    0.000    0.000    0.000 {method 'popleft' of 'collections.deque' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {method 'ravel' of 'numpy.ndarray' objects}\n",
      "        7    0.000    0.000    0.000    0.000 {method 'reduce' of 'numpy.ufunc' objects}\n",
      "        3    0.000    0.000    0.000    0.000 {method 'remove' of 'list' objects}\n",
      "        5    0.000    0.000    0.000    0.000 {method 'repeat' of 'numpy.ndarray' objects}\n",
      "       26    0.000    0.000    0.000    0.000 {method 'replace' of 'str' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'rpartition' of 'str' objects}\n",
      "        5    0.000    0.000    0.000    0.000 {method 'shuffle' of 'numpy.random.mtrand.RandomState' objects}\n",
      "        5    0.000    0.000    0.000    0.000 {method 'sort' of 'numpy.ndarray' objects}\n",
      "        2    0.000    0.000    0.000    0.000 {method 'split' of 'str' objects}\n",
      "        7    0.000    0.000    0.000    0.000 {method 'startswith' of 'str' objects}\n",
      "       26    0.000    0.000    0.000    0.000 {method 'update' of 'dict' objects}\n",
      "       11    0.000    0.000    0.000    0.000 {method 'upper' of 'str' objects}\n",
      "        6    0.000    0.000    0.000    0.000 {method 'values' of 'dict' objects}\n",
      "       56    0.000    0.000    0.000    0.000 {method 'values' of 'mappingproxy' objects}\n",
      "        8    0.000    0.000    0.000    0.000 {method 'write' of '_io.StringIO' objects}\n",
      "        1    0.000    0.000    0.000    0.000 {pandas._libs.lib.is_iterator}\n",
      "        1    0.000    0.000    0.000    0.000 {pandas._libs.lib.item_from_zerodim}\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "\nAll the 6 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n6 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\ValentinStudium\\Envs\\sma\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\ValentinStudium\\Envs\\sma\\lib\\site-packages\\imblearn\\pipeline.py\", line 268, in fit\n    Xt, yt = self._fit(X, y, **fit_params_steps)\n  File \"c:\\Users\\ValentinStudium\\Envs\\sma\\lib\\site-packages\\imblearn\\pipeline.py\", line 190, in _fit\n    self._validate_steps()\n  File \"c:\\Users\\ValentinStudium\\Envs\\sma\\lib\\site-packages\\imblearn\\pipeline.py\", line 125, in _validate_steps\n    self._validate_names(names)\n  File \"c:\\Users\\ValentinStudium\\Envs\\sma\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\", line 86, in _validate_names\n    raise ValueError(\"Names provided are not unique: {0!r}\".format(list(names)))\nValueError: Names provided are not unique: ['debug', 'preprocessor', 'debug', 'oversampler', 'estimator']\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ValentinStudium\\Nextcloud\\Studium\\09 Social Media Analytics\\Sentiment-Analysis\\Sentiment_Analysis.ipynb Cell 24'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000040?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcProfile\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000040?line=1'>2</a>\u001b[0m cProfile\u001b[39m.\u001b[39;49mrun(\u001b[39m'\u001b[39;49m\u001b[39msentiment.run_experiments(3, 2, 1, [\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mLogistic Regression\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m])\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\cProfile.py:16\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(statement, filename, sort)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/ValentinStudium/AppData/Local/Programs/Python/Python39/lib/cProfile.py?line=14'>15</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun\u001b[39m(statement, filename\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, sort\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[1;32m---> <a href='file:///c%3A/Users/ValentinStudium/AppData/Local/Programs/Python/Python39/lib/cProfile.py?line=15'>16</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _pyprofile\u001b[39m.\u001b[39;49m_Utils(Profile)\u001b[39m.\u001b[39;49mrun(statement, filename, sort)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\profile.py:53\u001b[0m, in \u001b[0;36m_Utils.run\u001b[1;34m(self, statement, filename, sort)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/ValentinStudium/AppData/Local/Programs/Python/Python39/lib/profile.py?line=50'>51</a>\u001b[0m prof \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler()\n\u001b[0;32m     <a href='file:///c%3A/Users/ValentinStudium/AppData/Local/Programs/Python/Python39/lib/profile.py?line=51'>52</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> <a href='file:///c%3A/Users/ValentinStudium/AppData/Local/Programs/Python/Python39/lib/profile.py?line=52'>53</a>\u001b[0m     prof\u001b[39m.\u001b[39;49mrun(statement)\n\u001b[0;32m     <a href='file:///c%3A/Users/ValentinStudium/AppData/Local/Programs/Python/Python39/lib/profile.py?line=53'>54</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mSystemExit\u001b[39;00m:\n\u001b[0;32m     <a href='file:///c%3A/Users/ValentinStudium/AppData/Local/Programs/Python/Python39/lib/profile.py?line=54'>55</a>\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\cProfile.py:95\u001b[0m, in \u001b[0;36mProfile.run\u001b[1;34m(self, cmd)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/ValentinStudium/AppData/Local/Programs/Python/Python39/lib/cProfile.py?line=92'>93</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39m__main__\u001b[39;00m\n\u001b[0;32m     <a href='file:///c%3A/Users/ValentinStudium/AppData/Local/Programs/Python/Python39/lib/cProfile.py?line=93'>94</a>\u001b[0m \u001b[39mdict\u001b[39m \u001b[39m=\u001b[39m __main__\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m\n\u001b[1;32m---> <a href='file:///c%3A/Users/ValentinStudium/AppData/Local/Programs/Python/Python39/lib/cProfile.py?line=94'>95</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunctx(cmd, \u001b[39mdict\u001b[39;49m, \u001b[39mdict\u001b[39;49m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\cProfile.py:100\u001b[0m, in \u001b[0;36mProfile.runctx\u001b[1;34m(self, cmd, globals, locals)\u001b[0m\n\u001b[0;32m     <a href='file:///c%3A/Users/ValentinStudium/AppData/Local/Programs/Python/Python39/lib/cProfile.py?line=97'>98</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menable()\n\u001b[0;32m     <a href='file:///c%3A/Users/ValentinStudium/AppData/Local/Programs/Python/Python39/lib/cProfile.py?line=98'>99</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/ValentinStudium/AppData/Local/Programs/Python/Python39/lib/cProfile.py?line=99'>100</a>\u001b[0m     exec(cmd, \u001b[39mglobals\u001b[39;49m, \u001b[39mlocals\u001b[39;49m)\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/AppData/Local/Programs/Python/Python39/lib/cProfile.py?line=100'>101</a>\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/AppData/Local/Programs/Python/Python39/lib/cProfile.py?line=101'>102</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisable()\n",
      "File \u001b[1;32m<string>:1\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\ValentinStudium\\Nextcloud\\Studium\\09 Social Media Analytics\\Sentiment-Analysis\\Sentiment_Analysis.ipynb Cell 12'\u001b[0m in \u001b[0;36mSentimentAnalyser.run_experiments\u001b[1;34m(self, iterations, cv_splits, cv_repeats, models)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000013?line=289'>290</a>\u001b[0m \u001b[39mprint\u001b[39m(experiment[\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000013?line=290'>291</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m-----------------\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000013?line=292'>293</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperiment_results[\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrun_\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__run\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m][experiment[\u001b[39m'\u001b[39m\u001b[39mname\u001b[39m\u001b[39m'\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevaluate_classifier(\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000013?line=293'>294</a>\u001b[0m         name \u001b[39m=\u001b[39;49m experiment[\u001b[39m'\u001b[39;49m\u001b[39mname\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000013?line=294'>295</a>\u001b[0m         model \u001b[39m=\u001b[39;49m experiment[\u001b[39m'\u001b[39;49m\u001b[39mmodel\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000013?line=295'>296</a>\u001b[0m         params \u001b[39m=\u001b[39;49m experiment[\u001b[39m'\u001b[39;49m\u001b[39mparams\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000013?line=296'>297</a>\u001b[0m         iterations \u001b[39m=\u001b[39;49m iterations,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000013?line=297'>298</a>\u001b[0m         cv_splits \u001b[39m=\u001b[39;49m cv_splits,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000013?line=298'>299</a>\u001b[0m         cv_repeats \u001b[39m=\u001b[39;49m cv_repeats\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000013?line=299'>300</a>\u001b[0m     )\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000013?line=301'>302</a>\u001b[0m end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000013?line=302'>303</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mTime: \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mint\u001b[39m(\u001b[39mround\u001b[39m(end_time, \u001b[39m1\u001b[39m))\u001b[39m}\u001b[39;00m\u001b[39m seconds (\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mint\u001b[39m(\u001b[39mround\u001b[39m(end_time\u001b[39m/\u001b[39m\u001b[39m60\u001b[39m, \u001b[39m1\u001b[39m))\u001b[39m}\u001b[39;00m\u001b[39m minutes)\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;32mc:\\Users\\ValentinStudium\\Nextcloud\\Studium\\09 Social Media Analytics\\Sentiment-Analysis\\Sentiment_Analysis.ipynb Cell 12'\u001b[0m in \u001b[0;36mSentimentAnalyser.evaluate_classifier\u001b[1;34m(self, name, model, params, iterations, cv_splits, cv_repeats)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000013?line=232'>233</a>\u001b[0m \u001b[39m# Doing hyperparameter optimization\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000013?line=233'>234</a>\u001b[0m optimization \u001b[39m=\u001b[39m RandomizedSearchCV(\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000013?line=234'>235</a>\u001b[0m     estimator \u001b[39m=\u001b[39m pipeline,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000013?line=235'>236</a>\u001b[0m     param_distributions \u001b[39m=\u001b[39m params,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000013?line=241'>242</a>\u001b[0m     verbose \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000013?line=242'>243</a>\u001b[0m     random_state \u001b[39m=\u001b[39m \u001b[39m33\u001b[39m)\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000013?line=244'>245</a>\u001b[0m optimization\u001b[39m.\u001b[39;49mfit(np\u001b[39m.\u001b[39;49marray(\u001b[39mrange\u001b[39;49m(\u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata))), \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget])\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000013?line=246'>247</a>\u001b[0m \u001b[39m# Evaluating the best model on the outer cross validation\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000013?line=247'>248</a>\u001b[0m performance_estimation \u001b[39m=\u001b[39m cross_validate(\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000013?line=248'>249</a>\u001b[0m     estimator \u001b[39m=\u001b[39m optimization,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000013?line=249'>250</a>\u001b[0m     X \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(\u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata))),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000013?line=252'>253</a>\u001b[0m     cv \u001b[39m=\u001b[39m outer_cv,\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000013?line=253'>254</a>\u001b[0m     n_jobs \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mcpu_count() \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\ValentinStudium\\Envs\\sma\\lib\\site-packages\\sklearn\\model_selection\\_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=868'>869</a>\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=869'>870</a>\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=870'>871</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=872'>873</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=874'>875</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=876'>877</a>\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=877'>878</a>\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=878'>879</a>\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\ValentinStudium\\Envs\\sma\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1749\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=1746'>1747</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=1747'>1748</a>\u001b[0m     \u001b[39m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[1;32m-> <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=1748'>1749</a>\u001b[0m     evaluate_candidates(\n\u001b[0;32m   <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=1749'>1750</a>\u001b[0m         ParameterSampler(\n\u001b[0;32m   <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=1750'>1751</a>\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_distributions, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_iter, random_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state\n\u001b[0;32m   <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=1751'>1752</a>\u001b[0m         )\n\u001b[0;32m   <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=1752'>1753</a>\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\ValentinStudium\\Envs\\sma\\lib\\site-packages\\sklearn\\model_selection\\_search.py:852\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=844'>845</a>\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m!=\u001b[39m n_candidates \u001b[39m*\u001b[39m n_splits:\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=845'>846</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=846'>847</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcv.split and cv.get_n_splits returned \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=847'>848</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minconsistent results. Expected \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=848'>849</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39msplits, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(n_splits, \u001b[39mlen\u001b[39m(out) \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m n_candidates)\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=849'>850</a>\u001b[0m     )\n\u001b[1;32m--> <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=851'>852</a>\u001b[0m _warn_or_raise_about_fit_failures(out, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merror_score)\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=853'>854</a>\u001b[0m \u001b[39m# For callable self.scoring, the return type is only know after\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=854'>855</a>\u001b[0m \u001b[39m# calling. If the return type is a dictionary, the error scores\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=855'>856</a>\u001b[0m \u001b[39m# can now be inserted with the correct key. The type checking\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=856'>857</a>\u001b[0m \u001b[39m# of out will be done in `_insert_error_scores`.\u001b[39;00m\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_search.py?line=857'>858</a>\u001b[0m \u001b[39mif\u001b[39;00m callable(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscoring):\n",
      "File \u001b[1;32mc:\\Users\\ValentinStudium\\Envs\\sma\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:367\u001b[0m, in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[1;34m(results, error_score)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_validation.py?line=359'>360</a>\u001b[0m \u001b[39mif\u001b[39;00m num_failed_fits \u001b[39m==\u001b[39m num_fits:\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_validation.py?line=360'>361</a>\u001b[0m     all_fits_failed_message \u001b[39m=\u001b[39m (\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_validation.py?line=361'>362</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mAll the \u001b[39m\u001b[39m{\u001b[39;00mnum_fits\u001b[39m}\u001b[39;00m\u001b[39m fits failed.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_validation.py?line=362'>363</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIt is very likely that your model is misconfigured.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_validation.py?line=363'>364</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou can try to debug the error by setting error_score=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_validation.py?line=364'>365</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBelow are more details about the failures:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfit_errors_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_validation.py?line=365'>366</a>\u001b[0m     )\n\u001b[1;32m--> <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_validation.py?line=366'>367</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(all_fits_failed_message)\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_validation.py?line=368'>369</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_validation.py?line=369'>370</a>\u001b[0m     some_fits_failed_message \u001b[39m=\u001b[39m (\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_validation.py?line=370'>371</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mnum_failed_fits\u001b[39m}\u001b[39;00m\u001b[39m fits failed out of a total of \u001b[39m\u001b[39m{\u001b[39;00mnum_fits\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_validation.py?line=371'>372</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mThe score on these train-test partitions for these parameters\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_validation.py?line=375'>376</a>\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBelow are more details about the failures:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mfit_errors_summary\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/model_selection/_validation.py?line=376'>377</a>\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: \nAll the 6 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n6 fits failed with the following error:\nTraceback (most recent call last):\n  File \"c:\\Users\\ValentinStudium\\Envs\\sma\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"c:\\Users\\ValentinStudium\\Envs\\sma\\lib\\site-packages\\imblearn\\pipeline.py\", line 268, in fit\n    Xt, yt = self._fit(X, y, **fit_params_steps)\n  File \"c:\\Users\\ValentinStudium\\Envs\\sma\\lib\\site-packages\\imblearn\\pipeline.py\", line 190, in _fit\n    self._validate_steps()\n  File \"c:\\Users\\ValentinStudium\\Envs\\sma\\lib\\site-packages\\imblearn\\pipeline.py\", line 125, in _validate_steps\n    self._validate_names(names)\n  File \"c:\\Users\\ValentinStudium\\Envs\\sma\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\", line 86, in _validate_names\n    raise ValueError(\"Names provided are not unique: {0!r}\".format(list(names)))\nValueError: Names provided are not unique: ['debug', 'preprocessor', 'debug', 'oversampler', 'estimator']\n"
     ]
    }
   ],
   "source": [
    "import cProfile\n",
    "cProfile.run('sentiment.run_experiments(3, 2, 1, [\"Logistic Regression\"])')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_preprocessor</th>\n",
       "      <th>param_oversampler</th>\n",
       "      <th>param_estimator__penalty</th>\n",
       "      <th>param_estimator__C</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_accuracy</th>\n",
       "      <th>...</th>\n",
       "      <th>split0_test_f1_weighted</th>\n",
       "      <th>split1_test_f1_weighted</th>\n",
       "      <th>mean_test_f1_weighted</th>\n",
       "      <th>std_test_f1_weighted</th>\n",
       "      <th>rank_test_f1_weighted</th>\n",
       "      <th>split0_test_roc_auc_ovo_weighted</th>\n",
       "      <th>split1_test_roc_auc_ovo_weighted</th>\n",
       "      <th>mean_test_roc_auc_ovo_weighted</th>\n",
       "      <th>std_test_roc_auc_ovo_weighted</th>\n",
       "      <th>rank_test_roc_auc_ovo_weighted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.885542</td>\n",
       "      <td>0.024955</td>\n",
       "      <td>3.880210</td>\n",
       "      <td>0.091795</td>\n",
       "      <td>FunctionTransformer(func=&lt;bound method Sentime...</td>\n",
       "      <td>passthrough</td>\n",
       "      <td>l1</td>\n",
       "      <td>1</td>\n",
       "      <td>{'preprocessor': FunctionTransformer(func=&lt;bou...</td>\n",
       "      <td>0.512227</td>\n",
       "      <td>...</td>\n",
       "      <td>0.487479</td>\n",
       "      <td>0.487753</td>\n",
       "      <td>0.487616</td>\n",
       "      <td>0.000137</td>\n",
       "      <td>1</td>\n",
       "      <td>0.762245</td>\n",
       "      <td>0.767468</td>\n",
       "      <td>0.764857</td>\n",
       "      <td>0.002612</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14.656642</td>\n",
       "      <td>0.489798</td>\n",
       "      <td>3.721914</td>\n",
       "      <td>0.740914</td>\n",
       "      <td>FunctionTransformer(func=&lt;bound method Sentime...</td>\n",
       "      <td>SMOTE(random_state=33)</td>\n",
       "      <td>l1</td>\n",
       "      <td>1</td>\n",
       "      <td>{'preprocessor': FunctionTransformer(func=&lt;bou...</td>\n",
       "      <td>0.456048</td>\n",
       "      <td>...</td>\n",
       "      <td>0.472759</td>\n",
       "      <td>0.494656</td>\n",
       "      <td>0.483707</td>\n",
       "      <td>0.010948</td>\n",
       "      <td>2</td>\n",
       "      <td>0.764429</td>\n",
       "      <td>0.794294</td>\n",
       "      <td>0.779361</td>\n",
       "      <td>0.014932</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.768894</td>\n",
       "      <td>0.743547</td>\n",
       "      <td>2.502676</td>\n",
       "      <td>0.046372</td>\n",
       "      <td>FunctionTransformer(func=&lt;bound method Sentime...</td>\n",
       "      <td>passthrough</td>\n",
       "      <td>l2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'preprocessor': FunctionTransformer(func=&lt;bou...</td>\n",
       "      <td>0.511566</td>\n",
       "      <td>...</td>\n",
       "      <td>0.474067</td>\n",
       "      <td>0.465805</td>\n",
       "      <td>0.469936</td>\n",
       "      <td>0.004131</td>\n",
       "      <td>3</td>\n",
       "      <td>0.758581</td>\n",
       "      <td>0.761004</td>\n",
       "      <td>0.759792</td>\n",
       "      <td>0.001211</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       6.885542      0.024955         3.880210        0.091795   \n",
       "1      14.656642      0.489798         3.721914        0.740914   \n",
       "2       3.768894      0.743547         2.502676        0.046372   \n",
       "\n",
       "                                  param_preprocessor       param_oversampler  \\\n",
       "0  FunctionTransformer(func=<bound method Sentime...             passthrough   \n",
       "1  FunctionTransformer(func=<bound method Sentime...  SMOTE(random_state=33)   \n",
       "2  FunctionTransformer(func=<bound method Sentime...             passthrough   \n",
       "\n",
       "  param_estimator__penalty param_estimator__C  \\\n",
       "0                       l1                  1   \n",
       "1                       l1                  1   \n",
       "2                       l2                0.5   \n",
       "\n",
       "                                              params  split0_test_accuracy  \\\n",
       "0  {'preprocessor': FunctionTransformer(func=<bou...              0.512227   \n",
       "1  {'preprocessor': FunctionTransformer(func=<bou...              0.456048   \n",
       "2  {'preprocessor': FunctionTransformer(func=<bou...              0.511566   \n",
       "\n",
       "   ...  split0_test_f1_weighted  split1_test_f1_weighted  \\\n",
       "0  ...                 0.487479                 0.487753   \n",
       "1  ...                 0.472759                 0.494656   \n",
       "2  ...                 0.474067                 0.465805   \n",
       "\n",
       "   mean_test_f1_weighted  std_test_f1_weighted  rank_test_f1_weighted  \\\n",
       "0               0.487616              0.000137                      1   \n",
       "1               0.483707              0.010948                      2   \n",
       "2               0.469936              0.004131                      3   \n",
       "\n",
       "   split0_test_roc_auc_ovo_weighted  split1_test_roc_auc_ovo_weighted  \\\n",
       "0                          0.762245                          0.767468   \n",
       "1                          0.764429                          0.794294   \n",
       "2                          0.758581                          0.761004   \n",
       "\n",
       "   mean_test_roc_auc_ovo_weighted  std_test_roc_auc_ovo_weighted  \\\n",
       "0                        0.764857                       0.002612   \n",
       "1                        0.779361                       0.014932   \n",
       "2                        0.759792                       0.001211   \n",
       "\n",
       "   rank_test_roc_auc_ovo_weighted  \n",
       "0                               2  \n",
       "1                               1  \n",
       "2                               3  \n",
       "\n",
       "[3 rows x 29 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment.experiment_results[\"run_1\"][\"Logistic Regression\"][\"optimization_cv_results\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ValentinStudium\\Envs\\sma\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "c:\\Users\\ValentinStudium\\Envs\\sma\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(l1_ratio=0.5, n_jobs=7, solver=&#x27;saga&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(l1_ratio=0.5, n_jobs=7, solver=&#x27;saga&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(l1_ratio=0.5, n_jobs=7, solver='saga')"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformed_data = CountVectorizer().fit_transform(yelp.text)\n",
    "model = LogisticRegression(n_jobs = os.cpu_count() -1, solver=\"saga\", l1_ratio=0.5)\n",
    "model.fit(X = transformed_data, y = yelp.stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9550413223140496"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X = transformed_data, y = yelp.stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ValentinStudium\\Envs\\sma\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "c:\\Users\\ValentinStudium\\Envs\\sma\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-15 {color: black;background-color: white;}#sk-container-id-15 pre{padding: 0;}#sk-container-id-15 div.sk-toggleable {background-color: white;}#sk-container-id-15 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-15 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-15 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-15 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-15 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-15 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-15 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-15 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-15 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-15 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-15 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-15 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-15 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-15 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-15 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-15 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-15 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-15 div.sk-item {position: relative;z-index: 1;}#sk-container-id-15 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-15 div.sk-item::before, #sk-container-id-15 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-15 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-15 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-15 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-15 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-15 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-15 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-15 div.sk-label-container {text-align: center;}#sk-container-id-15 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-15 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-15\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;, CountVectorizer()), (&#x27;oversampler&#x27;, SMOTE()),\n",
       "                (&#x27;estimator&#x27;,\n",
       "                 LogisticRegression(l1_ratio=0.5, max_iter=1000, n_jobs=7,\n",
       "                                    solver=&#x27;saga&#x27;))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-59\" type=\"checkbox\" ><label for=\"sk-estimator-id-59\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;preprocessor&#x27;, CountVectorizer()), (&#x27;oversampler&#x27;, SMOTE()),\n",
       "                (&#x27;estimator&#x27;,\n",
       "                 LogisticRegression(l1_ratio=0.5, max_iter=1000, n_jobs=7,\n",
       "                                    solver=&#x27;saga&#x27;))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-60\" type=\"checkbox\" ><label for=\"sk-estimator-id-60\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CountVectorizer</label><div class=\"sk-toggleable__content\"><pre>CountVectorizer()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-61\" type=\"checkbox\" ><label for=\"sk-estimator-id-61\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SMOTE</label><div class=\"sk-toggleable__content\"><pre>SMOTE()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-62\" type=\"checkbox\" ><label for=\"sk-estimator-id-62\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(l1_ratio=0.5, max_iter=1000, n_jobs=7, solver=&#x27;saga&#x27;)</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('preprocessor', CountVectorizer()), ('oversampler', SMOTE()),\n",
       "                ('estimator',\n",
       "                 LogisticRegression(l1_ratio=0.5, max_iter=1000, n_jobs=7,\n",
       "                                    solver='saga'))])"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline = Pipeline([('preprocessor', CountVectorizer()), ('oversampler', SMOTE()), ('estimator',  LogisticRegression(n_jobs = os.cpu_count() -1, solver=\"saga\", l1_ratio=0.5, max_iter=1000))])\n",
    "pipeline.fit(yelp.text, yelp.stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9920661157024794"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.score(yelp.text, yelp.stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "multi_class must be in ('ovo', 'ovr')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\ValentinStudium\\Nextcloud\\Studium\\09 Social Media Analytics\\Sentiment-Analysis\\Sentiment_Analysis.ipynb Cell 25'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000054?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m roc_auc_score\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/ValentinStudium/Nextcloud/Studium/09%20Social%20Media%20Analytics/Sentiment-Analysis/Sentiment_Analysis.ipynb#ch0000054?line=1'>2</a>\u001b[0m roc_auc_score(yelp\u001b[39m.\u001b[39;49mstars, pipeline\u001b[39m.\u001b[39;49mpredict_proba(yelp\u001b[39m.\u001b[39;49mtext))\n",
      "File \u001b[1;32mc:\\Users\\ValentinStudium\\Envs\\sma\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:562\u001b[0m, in \u001b[0;36mroc_auc_score\u001b[1;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/metrics/_ranking.py?line=554'>555</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/metrics/_ranking.py?line=555'>556</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mPartial AUC computation not available in \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/metrics/_ranking.py?line=556'>557</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mmulticlass setting, \u001b[39m\u001b[39m'\u001b[39m\u001b[39mmax_fpr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m must be\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/metrics/_ranking.py?line=557'>558</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39m set to `None`, received `max_fpr=\u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/metrics/_ranking.py?line=558'>559</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39minstead\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(max_fpr)\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/metrics/_ranking.py?line=559'>560</a>\u001b[0m         )\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/metrics/_ranking.py?line=560'>561</a>\u001b[0m     \u001b[39mif\u001b[39;00m multi_class \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/metrics/_ranking.py?line=561'>562</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mmulti_class must be in (\u001b[39m\u001b[39m'\u001b[39m\u001b[39movo\u001b[39m\u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39movr\u001b[39m\u001b[39m'\u001b[39m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/metrics/_ranking.py?line=562'>563</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m _multiclass_roc_auc_score(\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/metrics/_ranking.py?line=563'>564</a>\u001b[0m         y_true, y_score, labels, multi_class, average, sample_weight\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/metrics/_ranking.py?line=564'>565</a>\u001b[0m     )\n\u001b[0;32m    <a href='file:///c%3A/Users/ValentinStudium/Envs/sma/lib/site-packages/sklearn/metrics/_ranking.py?line=565'>566</a>\u001b[0m \u001b[39melif\u001b[39;00m y_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbinary\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[1;31mValueError\u001b[0m: multi_class must be in ('ovo', 'ovr')"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(yelp.stars, pipeline.predict_proba(yelp.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ValentinStudium\\Envs\\sma\\lib\\site-packages\\sklearn\\model_selection\\_search.py:306: UserWarning: The total space of parameters 2 is smaller than n_iter=5. Running 2 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    }
   ],
   "source": [
    "def produce_embeddings(X, wv):\n",
    "    print(wv)\n",
    "    if wv not in [\"wv_glove_wiki\",\"wv_glove_twitter\",\"wv_word2vec\"]:\n",
    "        print(\"Not a valid set of word vectors - Choose one of the following\")\n",
    "        print([\"wv_glove_wiki\",\"wv_glove_twitter\",\"wv_word2vec\"])\n",
    "\n",
    "    vocabulary = set(potential_wv[wv].index_to_key)\n",
    "    wpt = nltk.WordPunctTokenizer()\n",
    "\n",
    "    def avg_embeddings(document):\n",
    "        words = wpt.tokenize(document)\n",
    "        invocab = [word for word in words if word in vocabulary]\n",
    "        avg = np.mean(potential_wv[wv][invocab], axis=0) if len(invocab) >= 1 else []\n",
    "        return avg\n",
    "\n",
    "    doc_embeddings = [avg_embeddings(doc) for doc in X.values]\n",
    "    print(doc_embeddings)\n",
    "    return doc_embeddings\n",
    "\n",
    "params = {\n",
    "    'preprocessor': [FunctionTransformer(produce_embeddings, kw_args={'wv':\"wv_glove_wiki\"})],\n",
    "    'oversampler': ['passthrough', SMOTE(random_state=33)]\n",
    "}\n",
    "\n",
    "inner_cv = RepeatedKFold(n_splits = 2, n_repeats = 1, random_state = 33)\n",
    "outer_cv = RepeatedKFold(n_splits = 2, n_repeats = 1, random_state = 33)\n",
    "\n",
    "# Doing hyperparameter optimization\n",
    "optimization = RandomizedSearchCV(\n",
    "    estimator = pipeline,\n",
    "    param_distributions = params,\n",
    "    scoring = ['accuracy', 'balanced_accuracy', 'f1_weighted', 'roc_auc_ovo_weighted'],\n",
    "    cv = inner_cv,\n",
    "    refit = 'roc_auc_ovo_weighted',\n",
    "    n_iter = 5,\n",
    "    n_jobs = os.cpu_count() -1,\n",
    "    verbose = 1,\n",
    "    random_state = 33)\n",
    "\n",
    "#print(input.shape)\n",
    "#print(input)\n",
    "#print(self.data[self.target].shape)\n",
    "#print(self.data[self.target])\n",
    "optimization.fit(yelp.text, yelp.stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([5.44463897, 7.33878958]),\n",
       " 'std_fit_time': array([0.56282735, 0.98143685]),\n",
       " 'mean_score_time': array([5.38122666, 2.46106195]),\n",
       " 'std_score_time': array([0.95839632, 0.35805845]),\n",
       " 'param_preprocessor': masked_array(data=[FunctionTransformer(func=<function produce_embeddings at 0x00000263C45800D0>,\n",
       "                                        kw_args={'wv': 'wv_glove_wiki'})                         ,\n",
       "                    FunctionTransformer(func=<function produce_embeddings at 0x00000263C45800D0>,\n",
       "                                        kw_args={'wv': 'wv_glove_wiki'})                         ],\n",
       "              mask=[False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_oversampler': masked_array(data=['passthrough', SMOTE(random_state=33)],\n",
       "              mask=[False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'preprocessor': FunctionTransformer(func=<function produce_embeddings at 0x00000263C45800D0>,\n",
       "                       kw_args={'wv': 'wv_glove_wiki'}),\n",
       "   'oversampler': 'passthrough'},\n",
       "  {'preprocessor': FunctionTransformer(func=<function produce_embeddings at 0x00000263C45800D0>,\n",
       "                       kw_args={'wv': 'wv_glove_wiki'}),\n",
       "   'oversampler': SMOTE(random_state=33)}],\n",
       " 'split0_test_accuracy': array([0.51949769, 0.45406477]),\n",
       " 'split1_test_accuracy': array([0.51256614, 0.48082011]),\n",
       " 'mean_test_accuracy': array([0.51603191, 0.46744244]),\n",
       " 'std_test_accuracy': array([0.00346577, 0.01337767]),\n",
       " 'rank_test_accuracy': array([1, 2]),\n",
       " 'split0_test_balanced_accuracy': array([0.35715331, 0.43371024]),\n",
       " 'split1_test_balanced_accuracy': array([0.35482255, 0.4244776 ]),\n",
       " 'mean_test_balanced_accuracy': array([0.35598793, 0.42909392]),\n",
       " 'std_test_balanced_accuracy': array([0.00116538, 0.00461632]),\n",
       " 'rank_test_balanced_accuracy': array([2, 1]),\n",
       " 'split0_test_f1_weighted': array([0.49924435, 0.47224863]),\n",
       " 'split1_test_f1_weighted': array([0.47914585, 0.48809128]),\n",
       " 'mean_test_f1_weighted': array([0.4891951 , 0.48016996]),\n",
       " 'std_test_f1_weighted': array([0.01004925, 0.00792132]),\n",
       " 'rank_test_f1_weighted': array([1, 2]),\n",
       " 'split0_test_roc_auc_ovo_weighted': array([0.76903726, 0.76637362]),\n",
       " 'split1_test_roc_auc_ovo_weighted': array([0.75726426, 0.76299977]),\n",
       " 'mean_test_roc_auc_ovo_weighted': array([0.76315076, 0.76468669]),\n",
       " 'std_test_roc_auc_ovo_weighted': array([0.0058865 , 0.00168693]),\n",
       " 'rank_test_roc_auc_ovo_weighted': array([2, 1])}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimization.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([3.50885952, 9.40877807]),\n",
       " 'std_fit_time': array([0.62279022, 0.69137371]),\n",
       " 'mean_score_time': array([0.22923148, 0.17882931]),\n",
       " 'std_score_time': array([0.06071699, 0.02544558]),\n",
       " 'param_oversampler': masked_array(data=['passthrough', SMOTE(random_state=33)],\n",
       "              mask=[False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'oversampler': 'passthrough'},\n",
       "  {'oversampler': SMOTE(random_state=33)}],\n",
       " 'split0_test_accuracy': array([0.53139458, 0.48182419]),\n",
       " 'split1_test_accuracy': array([0.52910053, 0.49801587]),\n",
       " 'mean_test_accuracy': array([0.53024755, 0.48992003]),\n",
       " 'std_test_accuracy': array([0.00114703, 0.00809584]),\n",
       " 'rank_test_accuracy': array([1, 2]),\n",
       " 'split0_test_balanced_accuracy': array([0.38302699, 0.4135917 ]),\n",
       " 'split1_test_balanced_accuracy': array([0.35831764, 0.38671666]),\n",
       " 'mean_test_balanced_accuracy': array([0.37067231, 0.40015418]),\n",
       " 'std_test_balanced_accuracy': array([0.01235468, 0.01343752]),\n",
       " 'rank_test_balanced_accuracy': array([2, 1]),\n",
       " 'split0_test_f1_weighted': array([0.51624354, 0.48843744]),\n",
       " 'split1_test_f1_weighted': array([0.50788505, 0.49736243]),\n",
       " 'mean_test_f1_weighted': array([0.5120643 , 0.49289994]),\n",
       " 'std_test_f1_weighted': array([0.00417925, 0.00446249]),\n",
       " 'rank_test_f1_weighted': array([1, 2]),\n",
       " 'split0_test_roc_auc_ovo': array([0.72573007, 0.70266252]),\n",
       " 'split1_test_roc_auc_ovo': array([0.69427455, 0.67347176]),\n",
       " 'mean_test_roc_auc_ovo': array([0.71000231, 0.68806714]),\n",
       " 'std_test_roc_auc_ovo': array([0.01572776, 0.01459538]),\n",
       " 'rank_test_roc_auc_ovo': array([1, 2])}"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimization.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ValentinStudium\\Envs\\sma\\lib\\site-packages\\sklearn\\model_selection\\_search.py:306: UserWarning: The total space of parameters 2 is smaller than n_iter=5. Running 2 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ValentinStudium\\Envs\\sma\\lib\\site-packages\\sklearn\\model_selection\\_search.py:953: UserWarning: One or more of the test scores are non-finite: [nan nan]\n",
      "  warnings.warn(\n",
      "c:\\Users\\ValentinStudium\\Envs\\sma\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1106: UserWarning: l1_ratio parameter is only used when penalty is 'elasticnet'. Got (penalty=l2)\n",
      "  warnings.warn(\n",
      "c:\\Users\\ValentinStudium\\Envs\\sma\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def produce_embeddings(X, wv):\n",
    "    print(wv)\n",
    "    if wv not in [\"wv_glove_wiki\",\"wv_glove_twitter\",\"wv_word2vec\"]:\n",
    "        print(\"Not a valid set of word vectors - Choose one of the following\")\n",
    "        print([\"wv_glove_wiki\",\"wv_glove_twitter\",\"wv_word2vec\"])\n",
    "\n",
    "    vocabulary = set(potential_wv[wv].index_to_key)\n",
    "    wpt = nltk.WordPunctTokenizer()\n",
    "\n",
    "    def avg_embeddings(document):\n",
    "        words = wpt.tokenize(document)\n",
    "        invocab = [word for word in words if word in vocabulary]\n",
    "        avg = np.mean(potential_wv[wv][invocab], axis=0) if len(invocab) >= 1 else []\n",
    "        return avg\n",
    "\n",
    "    doc_embeddings = [avg_embeddings(doc) for doc in X.values]\n",
    "    return doc_embeddings\n",
    "\n",
    "params = {\n",
    "    'preprocessor': [\n",
    "        CountVectorizer(min_df=1), \n",
    "        CountVectorizer(min_df=2), \n",
    "        CountVectorizer(min_df=3), \n",
    "        CountVectorizer(ngram_range=(1,2)),\n",
    "        TfidfVectorizer(norm= 'l1'),\n",
    "        TfidfVectorizer(norm = 'l2'),\n",
    "        FunctionTransformer(produce_embeddings, kw_args={'wv':\"wv_glove_wiki\"}),\n",
    "        FunctionTransformer(produce_embeddings, kw_args={'wv':\"wv_glove_twitter\"}),\n",
    "        FunctionTransformer(produce_embeddings, kw_args={'wv':\"wv_word2vec\"})\n",
    "        ],\n",
    "    'oversampler': ['passthrough', SMOTE(random_state=33)],\n",
    "    'estimator__penalty': [\"none\", \"l2\", \"l1\", \"elasticnet\"],\n",
    "    'estimator__C': [0.5, 1, 2]\n",
    "}\n",
    "\n",
    "params = {\n",
    "    'oversampler': ['passthrough', SMOTE(random_state=33)]\n",
    "}\n",
    "\n",
    "inner_cv = RepeatedStratifiedKFold(n_splits = 2, n_repeats = 1, random_state = 33)\n",
    "outer_cv = RepeatedStratifiedKFold(n_splits = 2, n_repeats = 1, random_state = 33)\n",
    "\n",
    "# Doing hyperparameter optimization\n",
    "optimization = RandomizedSearchCV(\n",
    "    estimator = pipeline,\n",
    "    param_distributions = params,\n",
    "    scoring = ['accuracy', 'balanced_accuracy', 'f1', 'roc_auc'],\n",
    "    cv = inner_cv,\n",
    "    refit = 'roc_auc',\n",
    "    n_iter = 5,\n",
    "    n_jobs = os.cpu_count() -1,\n",
    "    verbose = 1,\n",
    "    random_state = 33)\n",
    "\n",
    "#print(input.shape)\n",
    "#print(input)\n",
    "#print(self.data[self.target].shape)\n",
    "#print(self.data[self.target])\n",
    "optimization.fit(yelp.text, yelp.stars)\n",
    "\n",
    "# Evaluating the best model on the outer cross validation\n",
    "performance_estimation = cross_validate(\n",
    "    estimator = optimization,\n",
    "    X = yelp.text,\n",
    "    y = yelp.stars,\n",
    "    scoring = ['accuracy', 'balanced_accuracy', 'f1', 'roc_auc'],\n",
    "    cv = outer_cv,\n",
    "    n_jobs = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([0.5789814 , 1.25538003]),\n",
       " 'std_fit_time': array([0.13088441, 0.00751984]),\n",
       " 'mean_score_time': array([0.08575821, 0.0890162 ]),\n",
       " 'std_score_time': array([0.00524211, 0.00301719]),\n",
       " 'param_oversampler': masked_array(data=['passthrough', SMOTE(random_state=33)],\n",
       "              mask=[False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'oversampler': 'passthrough'},\n",
       "  {'oversampler': SMOTE(random_state=33)}],\n",
       " 'split0_test_accuracy': array([nan, nan]),\n",
       " 'split1_test_accuracy': array([nan, nan]),\n",
       " 'mean_test_accuracy': array([nan, nan]),\n",
       " 'std_test_accuracy': array([nan, nan]),\n",
       " 'rank_test_accuracy': array([1, 2]),\n",
       " 'split0_test_balanced_accuracy': array([nan, nan]),\n",
       " 'split1_test_balanced_accuracy': array([nan, nan]),\n",
       " 'mean_test_balanced_accuracy': array([nan, nan]),\n",
       " 'std_test_balanced_accuracy': array([nan, nan]),\n",
       " 'rank_test_balanced_accuracy': array([1, 2]),\n",
       " 'split0_test_f1': array([nan, nan]),\n",
       " 'split1_test_f1': array([nan, nan]),\n",
       " 'mean_test_f1': array([nan, nan]),\n",
       " 'std_test_f1': array([nan, nan]),\n",
       " 'rank_test_f1': array([1, 2]),\n",
       " 'split0_test_roc_auc': array([nan, nan]),\n",
       " 'split1_test_roc_auc': array([nan, nan]),\n",
       " 'mean_test_roc_auc': array([nan, nan]),\n",
       " 'std_test_roc_auc': array([nan, nan]),\n",
       " 'rank_test_roc_auc': array([1, 2])}"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimization.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_preprocessor</th>\n",
       "      <th>param_oversampler</th>\n",
       "      <th>param_estimator__penalty</th>\n",
       "      <th>param_estimator__C</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_accuracy</th>\n",
       "      <th>...</th>\n",
       "      <th>split0_test_f1</th>\n",
       "      <th>split1_test_f1</th>\n",
       "      <th>mean_test_f1</th>\n",
       "      <th>std_test_f1</th>\n",
       "      <th>rank_test_f1</th>\n",
       "      <th>split0_test_roc_auc</th>\n",
       "      <th>split1_test_roc_auc</th>\n",
       "      <th>mean_test_roc_auc</th>\n",
       "      <th>std_test_roc_auc</th>\n",
       "      <th>rank_test_roc_auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.717501</td>\n",
       "      <td>0.000503</td>\n",
       "      <td>0.117499</td>\n",
       "      <td>0.003503</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>passthrough</td>\n",
       "      <td>none</td>\n",
       "      <td>2</td>\n",
       "      <td>{'preprocessor': CountVectorizer(), 'oversampl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19.886458</td>\n",
       "      <td>0.154257</td>\n",
       "      <td>0.125498</td>\n",
       "      <td>0.019500</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>SMOTE(random_state=33)</td>\n",
       "      <td>l1</td>\n",
       "      <td>2</td>\n",
       "      <td>{'preprocessor': CountVectorizer(), 'oversampl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.337158</td>\n",
       "      <td>0.070497</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>FunctionTransformer(func=&lt;bound method Sentime...</td>\n",
       "      <td>SMOTE(random_state=33)</td>\n",
       "      <td>elasticnet</td>\n",
       "      <td>2</td>\n",
       "      <td>{'preprocessor': FunctionTransformer(func=&lt;bou...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.607074</td>\n",
       "      <td>1.314542</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>FunctionTransformer(func=&lt;bound method Sentime...</td>\n",
       "      <td>SMOTE(random_state=33)</td>\n",
       "      <td>none</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'preprocessor': FunctionTransformer(func=&lt;bou...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.117513</td>\n",
       "      <td>1.867869</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>FunctionTransformer(func=&lt;bound method Sentime...</td>\n",
       "      <td>passthrough</td>\n",
       "      <td>none</td>\n",
       "      <td>0.5</td>\n",
       "      <td>{'preprocessor': FunctionTransformer(func=&lt;bou...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.996702</td>\n",
       "      <td>0.037999</td>\n",
       "      <td>0.209499</td>\n",
       "      <td>0.017500</td>\n",
       "      <td>TfidfVectorizer(norm='l1')</td>\n",
       "      <td>passthrough</td>\n",
       "      <td>l1</td>\n",
       "      <td>2</td>\n",
       "      <td>{'preprocessor': TfidfVectorizer(norm='l1'), '...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20.287348</td>\n",
       "      <td>0.243974</td>\n",
       "      <td>0.104000</td>\n",
       "      <td>0.019998</td>\n",
       "      <td>CountVectorizer(min_df=3)</td>\n",
       "      <td>SMOTE(random_state=33)</td>\n",
       "      <td>elasticnet</td>\n",
       "      <td>1</td>\n",
       "      <td>{'preprocessor': CountVectorizer(min_df=3), 'o...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.538431</td>\n",
       "      <td>0.144711</td>\n",
       "      <td>0.286506</td>\n",
       "      <td>0.023013</td>\n",
       "      <td>TfidfVectorizer(norm='l1')</td>\n",
       "      <td>SMOTE(random_state=33)</td>\n",
       "      <td>l2</td>\n",
       "      <td>2</td>\n",
       "      <td>{'preprocessor': TfidfVectorizer(norm='l1'), '...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>33.105094</td>\n",
       "      <td>0.957821</td>\n",
       "      <td>0.070483</td>\n",
       "      <td>0.007513</td>\n",
       "      <td>CountVectorizer()</td>\n",
       "      <td>SMOTE(random_state=33)</td>\n",
       "      <td>elasticnet</td>\n",
       "      <td>2</td>\n",
       "      <td>{'preprocessor': CountVectorizer(), 'oversampl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.031324</td>\n",
       "      <td>0.361381</td>\n",
       "      <td>0.193499</td>\n",
       "      <td>0.022498</td>\n",
       "      <td>TfidfVectorizer()</td>\n",
       "      <td>passthrough</td>\n",
       "      <td>elasticnet</td>\n",
       "      <td>2</td>\n",
       "      <td>{'preprocessor': TfidfVectorizer(), 'oversampl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0       0.717501      0.000503         0.117499        0.003503   \n",
       "1      19.886458      0.154257         0.125498        0.019500   \n",
       "2       9.337158      0.070497         0.000000        0.000000   \n",
       "3      13.607074      1.314542         0.000000        0.000000   \n",
       "4      13.117513      1.867869         0.000000        0.000000   \n",
       "5       1.996702      0.037999         0.209499        0.017500   \n",
       "6      20.287348      0.243974         0.104000        0.019998   \n",
       "7       1.538431      0.144711         0.286506        0.023013   \n",
       "8      33.105094      0.957821         0.070483        0.007513   \n",
       "9       4.031324      0.361381         0.193499        0.022498   \n",
       "\n",
       "                                  param_preprocessor       param_oversampler  \\\n",
       "0                                  CountVectorizer()             passthrough   \n",
       "1                                  CountVectorizer()  SMOTE(random_state=33)   \n",
       "2  FunctionTransformer(func=<bound method Sentime...  SMOTE(random_state=33)   \n",
       "3  FunctionTransformer(func=<bound method Sentime...  SMOTE(random_state=33)   \n",
       "4  FunctionTransformer(func=<bound method Sentime...             passthrough   \n",
       "5                         TfidfVectorizer(norm='l1')             passthrough   \n",
       "6                          CountVectorizer(min_df=3)  SMOTE(random_state=33)   \n",
       "7                         TfidfVectorizer(norm='l1')  SMOTE(random_state=33)   \n",
       "8                                  CountVectorizer()  SMOTE(random_state=33)   \n",
       "9                                  TfidfVectorizer()             passthrough   \n",
       "\n",
       "  param_estimator__penalty param_estimator__C  \\\n",
       "0                     none                  2   \n",
       "1                       l1                  2   \n",
       "2               elasticnet                  2   \n",
       "3                     none                0.5   \n",
       "4                     none                0.5   \n",
       "5                       l1                  2   \n",
       "6               elasticnet                  1   \n",
       "7                       l2                  2   \n",
       "8               elasticnet                  2   \n",
       "9               elasticnet                  2   \n",
       "\n",
       "                                              params  split0_test_accuracy  \\\n",
       "0  {'preprocessor': CountVectorizer(), 'oversampl...                   NaN   \n",
       "1  {'preprocessor': CountVectorizer(), 'oversampl...                   NaN   \n",
       "2  {'preprocessor': FunctionTransformer(func=<bou...                   NaN   \n",
       "3  {'preprocessor': FunctionTransformer(func=<bou...                   NaN   \n",
       "4  {'preprocessor': FunctionTransformer(func=<bou...                   NaN   \n",
       "5  {'preprocessor': TfidfVectorizer(norm='l1'), '...                   NaN   \n",
       "6  {'preprocessor': CountVectorizer(min_df=3), 'o...                   NaN   \n",
       "7  {'preprocessor': TfidfVectorizer(norm='l1'), '...                   NaN   \n",
       "8  {'preprocessor': CountVectorizer(), 'oversampl...                   NaN   \n",
       "9  {'preprocessor': TfidfVectorizer(), 'oversampl...                   NaN   \n",
       "\n",
       "   ...  split0_test_f1  split1_test_f1  mean_test_f1  std_test_f1  \\\n",
       "0  ...             NaN             NaN           NaN          NaN   \n",
       "1  ...             NaN             NaN           NaN          NaN   \n",
       "2  ...             NaN             NaN           NaN          NaN   \n",
       "3  ...             NaN             NaN           NaN          NaN   \n",
       "4  ...             NaN             NaN           NaN          NaN   \n",
       "5  ...             NaN             NaN           NaN          NaN   \n",
       "6  ...             NaN             NaN           NaN          NaN   \n",
       "7  ...             NaN             NaN           NaN          NaN   \n",
       "8  ...             NaN             NaN           NaN          NaN   \n",
       "9  ...             NaN             NaN           NaN          NaN   \n",
       "\n",
       "   rank_test_f1  split0_test_roc_auc  split1_test_roc_auc  mean_test_roc_auc  \\\n",
       "0             1                  NaN                  NaN                NaN   \n",
       "1             2                  NaN                  NaN                NaN   \n",
       "2             3                  NaN                  NaN                NaN   \n",
       "3             4                  NaN                  NaN                NaN   \n",
       "4             5                  NaN                  NaN                NaN   \n",
       "5             6                  NaN                  NaN                NaN   \n",
       "6             7                  NaN                  NaN                NaN   \n",
       "7             8                  NaN                  NaN                NaN   \n",
       "8             9                  NaN                  NaN                NaN   \n",
       "9            10                  NaN                  NaN                NaN   \n",
       "\n",
       "   std_test_roc_auc  rank_test_roc_auc  \n",
       "0               NaN                  1  \n",
       "1               NaN                  2  \n",
       "2               NaN                  3  \n",
       "3               NaN                  4  \n",
       "4               NaN                  5  \n",
       "5               NaN                  6  \n",
       "6               NaN                  7  \n",
       "7               NaN                  8  \n",
       "8               NaN                  9  \n",
       "9               NaN                 10  \n",
       "\n",
       "[10 rows x 29 columns]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment.experiment_results[\"run_1\"][\"Logistic Regression\"][\"optimization_cv_results\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment.run_experiments(10, 3, 1, [\"\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment.experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment analysis on the document level\n",
    "\n",
    "# 1. Rule based\n",
    "    # Vader\n",
    "    # Text Blob\n",
    "# 2. ML based - Classification or Regression?\n",
    "    # own feature engineering plus defined models\n",
    "        # bag of words\n",
    "        # TD-IF\n",
    "        # word vectors\n",
    "    # Transformers"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2822d77ee51dbb640527abe329425380abdeab763840ac762f0d0bc769537fcd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('sma')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
