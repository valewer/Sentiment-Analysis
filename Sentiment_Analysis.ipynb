{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portfolio-Exam Part I - Sentiment Analysis\n",
    "\n",
    "* Social Media Analytics - MADS-SMA\n",
    "* Valentin Werger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different approaches\n",
    "# Train own word embedding\n",
    "# Use model trained on yelp for other data\n",
    "# Try out sentiments towards types of entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading base and Sentiment related packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import gensim.downloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading required ML packages and functions\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.preprocessing import FunctionTransformer, LabelBinarizer\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from scipy.stats import uniform, randint\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Hamburg Yelp reviews\n",
    "yelp = pd.read_csv(\"data/yelp_reviews_hamburg_en.csv\", parse_dates=[\"date\"], dtype={\"stars\":\"int64\"})\n",
    "# Subtract one from stars because some models (XGBoost, Transfomers) expects labels to be starting from 0\n",
    "yelp[\"stars\"] = yelp.stars - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3025 entries, 0 to 3024\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype         \n",
      "---  ------  --------------  -----         \n",
      " 0   url     3025 non-null   object        \n",
      " 1   stars   3025 non-null   int64         \n",
      " 2   text    3025 non-null   object        \n",
      " 3   date    3025 non-null   datetime64[ns]\n",
      "dtypes: datetime64[ns](1), int64(1), object(2)\n",
      "memory usage: 94.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# Overview of the data\n",
    "yelp.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>stars</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.yelp.com/biz/il-buco-hamburg</td>\n",
       "      <td>4</td>\n",
       "      <td>Fantastic little restaurant!Great staff and fo...</td>\n",
       "      <td>2017-08-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.yelp.com/biz/campus-suite-hamburg-7</td>\n",
       "      <td>3</td>\n",
       "      <td>We went there to grab some breakfast. They are...</td>\n",
       "      <td>2015-09-29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.yelp.com/biz/campus-suite-hamburg-7</td>\n",
       "      <td>3</td>\n",
       "      <td>Good coffee,  sandwiches,  and yogurts close t...</td>\n",
       "      <td>2016-01-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.yelp.com/biz/campus-suite-hamburg-7</td>\n",
       "      <td>2</td>\n",
       "      <td>When XING handed out coupons for free coffee d...</td>\n",
       "      <td>2008-04-24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.yelp.com/biz/campus-suite-hamburg-7</td>\n",
       "      <td>3</td>\n",
       "      <td>I love Campus Suite franchise. after the Balza...</td>\n",
       "      <td>2010-01-15</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               url  stars  \\\n",
       "0         https://www.yelp.com/biz/il-buco-hamburg      4   \n",
       "1  https://www.yelp.com/biz/campus-suite-hamburg-7      3   \n",
       "2  https://www.yelp.com/biz/campus-suite-hamburg-7      3   \n",
       "3  https://www.yelp.com/biz/campus-suite-hamburg-7      2   \n",
       "4  https://www.yelp.com/biz/campus-suite-hamburg-7      3   \n",
       "\n",
       "                                                text       date  \n",
       "0  Fantastic little restaurant!Great staff and fo... 2017-08-12  \n",
       "1  We went there to grab some breakfast. They are... 2015-09-29  \n",
       "2  Good coffee,  sandwiches,  and yogurts close t... 2016-01-13  \n",
       "3  When XING handed out coupons for free coffee d... 2008-04-24  \n",
       "4  I love Campus Suite franchise. after the Balza... 2010-01-15  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show the top 5 rows\n",
    "yelp.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract name of the location from url\n",
    "yelp[\"name\"] = yelp.apply(lambda row: re.sub(\"https://www.yelp.com/biz/\", \"\", row[\"url\"]), axis=1)\n",
    "yelp = yelp.drop(columns=\"url\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:ylabel='Count'>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Inspect length of review text\n",
    "text_length_distribution = np.array([len(text) for text in yelp.text])\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=[15,8])\n",
    "sns.histplot(data = text_length_distribution)\n",
    "\n",
    "# Problem: Maximum length of Bert is 512 and some reviews might go above that\n",
    "# one solution would be to summarize before\n",
    "# other to create overflows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='stars', ylabel='count'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check distribution of ratings\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "plt.figure(figsize=[15,8])\n",
    "sns.countplot(data = yelp, x = \"stars\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading glove-wiki-gigaword-50\n",
      "Downloading glove-twitter-50\n",
      "Downloading word2vec-google-news-300\n"
     ]
    }
   ],
   "source": [
    "# Load potential sets of word vectors\n",
    "#list(gensim.downloader.info()['models'].keys())\n",
    "potential_wv = {}\n",
    "print(\"Downloading glove-wiki-gigaword-50\")\n",
    "potential_wv[\"wv_glove_wiki\"] = gensim.downloader.load('glove-wiki-gigaword-300')\n",
    "print(\"Downloading glove-twitter-50\")\n",
    "potential_wv[\"wv_glove_twitter\"] = gensim.downloader.load('glove-twitter-200')\n",
    "print(\"Downloading word2vec-google-news-300\")\n",
    "potential_wv[\"wv_word2vec\"] = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next part three helper classes for preprocessing the data are configured to be used as part of the sklearn Pipeline. The reason to create such helper classes is that I want the feature engineering step of the text data to be part of the Machine Learning pipeline to conveniently compare the different possibilities and see how they influence the model performance. Besides that there is the problem of potential data leakage if we just transform the data one time through embeddings or word counts before repeatedly splitting into train and test sets for evaluation. In the case of text data this could also mean cheating the out of vocabulary problem that could arise in a real use of the model with new reviews. Including this step in the pipeline ensures that the same samples are used to train both the transformers and predictios. Additionally such a pipeline is convenient in that you only have to call fit and predict once on the data to fit a whole sequence of estimators as well as allowing parameter optimization for all estimators at once.\n",
    "\n",
    "In this case I have set up the following parameters for the feature engineering part of the pipeline:\n",
    "\n",
    "* WordCount/Tfidf\n",
    "    * min_df (minimum document frequency)\n",
    "        * 1\n",
    "        * 2\n",
    "        * 3\n",
    "    * ngram_range (only use unigrams or unigrams and bigrams)\n",
    "        * (1,1)\n",
    "        * (1,2)\n",
    "* Word Embeddings\n",
    "    * wv (set of word vectors to use)\n",
    "        * wv_glove_wiki\n",
    "        * wv_glove_twitter\n",
    "        * wv_word2vec\n",
    "\n",
    "However recalculating the word embeddings every time the model is fit, that means also on every cross validation step, seemed to have an intense computational effect making it unfeasible, because it made it harder to vary the actual model parameters. Therefore I decided to write the helper classes to still use both methods in the pipeline, while at the same time only producing the embeddings once in the beginning and then transfer them as a data parameter to the helper. The trick to make it work also for the other two methods was to specify the X on which the pipeline is actually fitted merely as the index of the data. This index then gets divided in cross validation and the preprocessing helpers forward the actual data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Helper classes for sklearn outside of the main class to not break parallelization\n",
    "class WordEmbeddingHelper(TransformerMixin, BaseEstimator):\n",
    "\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.data[X]\n",
    "\n",
    "class WordCountTransformerHelper(TransformerMixin, BaseEstimator):\n",
    "\n",
    "    def __init__(self, data, input, min_df=1, ngram_range=(1,1)):\n",
    "        self.data = data\n",
    "        self.input = input\n",
    "        self.min_df = min_df\n",
    "        self.ngram_range = ngram_range\n",
    "        self.transformer = CountVectorizer(min_df=self.min_df, ngram_range=self.ngram_range)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.transformer.fit(self.data[self.input][X])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.transformer.transform(self.data[self.input][X])\n",
    "\n",
    "class TfidfTransformerHelper(TransformerMixin, BaseEstimator):\n",
    "\n",
    "    def __init__(self, data, input, min_df=1, ngram_range=(1,1)):\n",
    "        self.data = data\n",
    "        self.input = input\n",
    "        self.min_df = min_df\n",
    "        self.ngram_range = ngram_range\n",
    "        self.transformer = TfidfVectorizer(min_df=self.min_df, ngram_range=self.ngram_range)\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self.transformer.fit(self.data[self.input][X])\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return self.transformer.transform(self.data[self.input][X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment Analyser class\n",
    "class SentimentAnalyser():\n",
    "\n",
    "    def __init__(self, data, target, input, normalize = True):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.input = input\n",
    "        self.target_distribution = self.data[self.target].value_counts(normalize=True)\n",
    "\n",
    "        # Normalize the review texts\n",
    "        if normalize:\n",
    "            self.data[\"text_normalized\"] = self.normalize()\n",
    "            self.input = \"text_normalized\"\n",
    "\n",
    "        # Initialize container for experiment results\n",
    "        self.experiment_results = {}\n",
    "        self.__run = 1\n",
    "\n",
    "        # Produce word embeddings for the whole dataset\n",
    "        self.word_embeddings = {}\n",
    "        for vector_set in [\"wv_glove_wiki\", \"wv_glove_twitter\", \"wv_word2vec\"]:\n",
    "            self.word_embeddings[vector_set] = self.produce_embeddings(self.data[self.input], vector_set)\n",
    "\n",
    "        # Set up experiments to conduct\n",
    "        self.experiments = [\n",
    "            {\n",
    "                'name': 'Logistic Regression',\n",
    "                'model': LogisticRegression(n_jobs = os.cpu_count() -1, solver=\"saga\", l1_ratio=0.5),\n",
    "                'params': {\n",
    "                    'preprocessor': [\n",
    "                        WordCountTransformerHelper(min_df=1, ngram_range=(1,1), data=self.data, input=self.input),\n",
    "                        WordCountTransformerHelper(min_df=2, ngram_range=(1,1), data=self.data, input=self.input),\n",
    "                        WordCountTransformerHelper(min_df=3, ngram_range=(1,1), data=self.data, input=self.input),\n",
    "                        WordCountTransformerHelper(min_df=1, ngram_range=(1,2), data=self.data, input=self.input),\n",
    "                        TfidfTransformerHelper(min_df=1, ngram_range=(1,1), data=self.data, input=self.input),\n",
    "                        TfidfTransformerHelper(min_df=2, ngram_range=(1,1), data=self.data, input=self.input),\n",
    "                        TfidfTransformerHelper(min_df=3, ngram_range=(1,1), data=self.data, input=self.input),\n",
    "                        TfidfTransformerHelper(min_df=1, ngram_range=(1,2), data=self.data, input=self.input),\n",
    "                        WordEmbeddingHelper(data=self.word_embeddings[\"wv_glove_wiki\"]),\n",
    "                        WordEmbeddingHelper(data=self.word_embeddings[\"wv_glove_twitter\"]),\n",
    "                        WordEmbeddingHelper(data=self.word_embeddings[\"wv_word2vec\"])\n",
    "                        ],\n",
    "                    'oversampler': ['passthrough', SMOTE(random_state=33)],\n",
    "                    'estimator__class_weight': [None, \"balanced\"],\n",
    "                    'estimator__penalty': [\"none\", \"l2\", \"l1\", \"elasticnet\"],\n",
    "                    'estimator__C': [0.5, 1, 2]\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'name': 'KNeighbors',\n",
    "                'model': KNeighborsClassifier(algorithm = \"auto\", n_jobs = os.cpu_count() -1),\n",
    "                'params': {\n",
    "                    'preprocessor': [\n",
    "                        WordCountTransformerHelper(min_df=1, ngram_range=(1,1), data=self.data, input=self.input),\n",
    "                        WordCountTransformerHelper(min_df=2, ngram_range=(1,1), data=self.data, input=self.input),\n",
    "                        WordCountTransformerHelper(min_df=3, ngram_range=(1,1), data=self.data, input=self.input),\n",
    "                        WordCountTransformerHelper(min_df=1, ngram_range=(1,2), data=self.data, input=self.input),\n",
    "                        TfidfTransformerHelper(min_df=1, ngram_range=(1,1), data=self.data, input=self.input),\n",
    "                        TfidfTransformerHelper(min_df=2, ngram_range=(1,1), data=self.data, input=self.input),\n",
    "                        TfidfTransformerHelper(min_df=3, ngram_range=(1,1), data=self.data, input=self.input),\n",
    "                        TfidfTransformerHelper(min_df=1, ngram_range=(1,2), data=self.data, input=self.input),\n",
    "                        WordEmbeddingHelper(data=self.word_embeddings[\"wv_glove_wiki\"]),\n",
    "                        WordEmbeddingHelper(data=self.word_embeddings[\"wv_glove_twitter\"]),\n",
    "                        WordEmbeddingHelper(data=self.word_embeddings[\"wv_word2vec\"])\n",
    "                        ],\n",
    "                    'oversampler': ['passthrough', SMOTE(random_state=33)],\n",
    "                    'estimator__n_neighbors': randint(1, 10), \n",
    "                    'estimator__weights': [\"uniform\", \"distance\"], \n",
    "                    'estimator__metric': [\"euclidean\", \"manhattan\"]\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'name': 'Random Forest',\n",
    "                'model': RandomForestClassifier(n_jobs = os.cpu_count() -1, verbose = 1, random_state = 33),\n",
    "                'params': {\n",
    "                    'preprocessor': [\n",
    "                        WordCountTransformerHelper(min_df=1, ngram_range=(1,1), data=self.data, input=self.input),\n",
    "                        WordCountTransformerHelper(min_df=2, ngram_range=(1,1), data=self.data, input=self.input),\n",
    "                        WordCountTransformerHelper(min_df=3, ngram_range=(1,1), data=self.data, input=self.input),\n",
    "                        WordCountTransformerHelper(min_df=1, ngram_range=(1,2), data=self.data, input=self.input),\n",
    "                        TfidfTransformerHelper(min_df=1, ngram_range=(1,1), data=self.data, input=self.input),\n",
    "                        TfidfTransformerHelper(min_df=2, ngram_range=(1,1), data=self.data, input=self.input),\n",
    "                        TfidfTransformerHelper(min_df=3, ngram_range=(1,1), data=self.data, input=self.input),\n",
    "                        TfidfTransformerHelper(min_df=1, ngram_range=(1,2), data=self.data, input=self.input),\n",
    "                        WordEmbeddingHelper(data=self.word_embeddings[\"wv_glove_wiki\"]),\n",
    "                        WordEmbeddingHelper(data=self.word_embeddings[\"wv_glove_twitter\"]),\n",
    "                        WordEmbeddingHelper(data=self.word_embeddings[\"wv_word2vec\"])\n",
    "                        ],\n",
    "                    'oversampler': ['passthrough', SMOTE(random_state=33)],\n",
    "                    'estimator__criterion': ['gini', 'entropy'],\n",
    "                    'estimator__n_estimators': randint(1, 100), \n",
    "                    'estimator__max_features': uniform(0.1,0.9),\n",
    "                    'estimator__class_weight': [None, 'balanced', 'balanced_subsample'],\n",
    "                    'estimator__ccp_alpha': uniform(0,0.1)\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'name': 'SVM',\n",
    "                'model': SVC(verbose = True, random_state = 33),\n",
    "                'params': {\n",
    "                    'preprocessor': [\n",
    "                        WordCountTransformerHelper(min_df=1, ngram_range=(1,1), data=self.data, input=self.input),\n",
    "                        WordCountTransformerHelper(min_df=2, ngram_range=(1,1), data=self.data, input=self.input),\n",
    "                        WordCountTransformerHelper(min_df=3, ngram_range=(1,1), data=self.data, input=self.input),\n",
    "                        WordCountTransformerHelper(min_df=1, ngram_range=(1,2), data=self.data, input=self.input),\n",
    "                        TfidfTransformerHelper(min_df=1, ngram_range=(1,1), data=self.data, input=self.input),\n",
    "                        TfidfTransformerHelper(min_df=2, ngram_range=(1,1), data=self.data, input=self.input),\n",
    "                        TfidfTransformerHelper(min_df=3, ngram_range=(1,1), data=self.data, input=self.input),\n",
    "                        TfidfTransformerHelper(min_df=1, ngram_range=(1,2), data=self.data, input=self.input),\n",
    "                        WordEmbeddingHelper(data=self.word_embeddings[\"wv_glove_wiki\"]),\n",
    "                        WordEmbeddingHelper(data=self.word_embeddings[\"wv_glove_twitter\"]),\n",
    "                        WordEmbeddingHelper(data=self.word_embeddings[\"wv_word2vec\"])\n",
    "                        ],\n",
    "                    'oversampler': ['passthrough', SMOTE(random_state=33)],\n",
    "                    'estimator__C': uniform(0.001, 1),\n",
    "                    'estimator__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "                    'estimator__degree': randint(1, 5),\n",
    "                    'estimator__gamma': uniform(0.0001, 1),\n",
    "                    'estimator__class_weight': [None, 'balanced']\n",
    "                }\n",
    "            },\n",
    "            # When in doubt use XGBoost\n",
    "            {\n",
    "                'name': 'XGBoost',\n",
    "                'model': XGBClassifier(\n",
    "                    objective = \"multi:softprob\",\n",
    "                    num_class = 5, \n",
    "                    tree_method = \"auto\",\n",
    "                    eval_metric = \"auc\", \n",
    "                    verbosity = 1,\n",
    "                    use_label_encoder = False,\n",
    "                    random_state = 33),\n",
    "                'params': {\n",
    "                    'preprocessor': [\n",
    "                        WordCountTransformerHelper(min_df=1, ngram_range=(1,1), data=self.data, input=self.input),\n",
    "                        WordCountTransformerHelper(min_df=2, ngram_range=(1,1), data=self.data, input=self.input),\n",
    "                        WordCountTransformerHelper(min_df=3, ngram_range=(1,1), data=self.data, input=self.input),\n",
    "                        WordCountTransformerHelper(min_df=1, ngram_range=(1,2), data=self.data, input=self.input),\n",
    "                        TfidfTransformerHelper(min_df=1, ngram_range=(1,1), data=self.data, input=self.input),\n",
    "                        TfidfTransformerHelper(min_df=2, ngram_range=(1,1), data=self.data, input=self.input),\n",
    "                        TfidfTransformerHelper(min_df=3, ngram_range=(1,1), data=self.data, input=self.input),\n",
    "                        TfidfTransformerHelper(min_df=1, ngram_range=(1,2), data=self.data, input=self.input),\n",
    "                        WordEmbeddingHelper(data=self.word_embeddings[\"wv_glove_wiki\"]),\n",
    "                        WordEmbeddingHelper(data=self.word_embeddings[\"wv_glove_twitter\"]),\n",
    "                        WordEmbeddingHelper(data=self.word_embeddings[\"wv_word2vec\"])\n",
    "                        ],\n",
    "                    'oversampler': ['passthrough', SMOTE(random_state=33)],\n",
    "                    'estimator__learning_rate': uniform(0.001, 0.5),\n",
    "                    'estimator__gamma': uniform(0, 0.5),\n",
    "                    'estimator__max_depth': randint(3, 10),\n",
    "                    'estimator__colsample_bytree': uniform(0.5, 0.5)\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Function to normalize a document\n",
    "    def __normalize_document(self, doc, tokenizer, stop_words):\n",
    "        doc = re.sub(r'@[\\w]+', '', doc)          # replace user mentions\n",
    "        doc = re.sub(r'http[\\S]+', 'URL', doc)    # replace URLs\n",
    "        doc = re.sub(r'[^\\w\\s]', '', doc)         # keep words and spaces\n",
    "        doc = doc.lower()\n",
    "        doc = doc.strip()\n",
    "        tokens = tokenizer.tokenize(doc)\n",
    "        filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "        doc = ' '.join(filtered_tokens)\n",
    "        return doc\n",
    "\n",
    "    # Function to normalize the provided data\n",
    "    def normalize(self):\n",
    "        stop_words = nltk.corpus.stopwords.words('english')\n",
    "        wpt = nltk.WordPunctTokenizer()\n",
    "\n",
    "        return self.data[self.input].apply(lambda x: self.__normalize_document(x, wpt, stop_words))\n",
    "\n",
    "    # Function to print performance scores, used for rule based sentiment\n",
    "    def print_performance_scores(self, true_values, prediction):\n",
    "        print(f\"Accuracy: {round(accuracy_score(true_values, prediction), 4)}\")\n",
    "        print(f\"Balanced Accuracy: {round(balanced_accuracy_score(true_values, prediction), 4)}\")\n",
    "        print(f\"F1 (macro average): {round(f1_score(true_values, prediction, average = 'macro'), 4)}\")\n",
    "        print(f\"F1 (weighted average): {round(f1_score(true_values, prediction, average = 'weighted'), 4)}\")\n",
    "        # Roc Auc value can only be computed from probabilities of shape (n_samples, n_classes) so we binarize the predicted values\n",
    "        lb = LabelBinarizer()\n",
    "        baseline_proba = lb.fit_transform(prediction)\n",
    "        print(f\"Roc Auc (macro average): {round(roc_auc_score(true_values, baseline_proba, average = 'macro', multi_class = 'ovo'), 4)}\")\n",
    "        print(f\"Roc Auc (weighted average): {round(roc_auc_score(true_values, baseline_proba, average = 'weighted', multi_class = 'ovo'), 4)}\")\n",
    "\n",
    "    # Function to show worst predictions, used for rule based sentiment\n",
    "    def show_worst_predictions(self, data, true_values, prediction, top_n_worst):\n",
    "        idx = (abs(prediction - true_values)).sort_values().index\n",
    "        worst = data.loc[idx, [self.input]].tail(top_n_worst)\n",
    "        \n",
    "        worst[\"original_text\"] = data.loc[idx, [\"text\"]].tail(top_n_worst)\n",
    "        worst[\"stars\"] = true_values[idx].tail(top_n_worst)\n",
    "        worst[\"prediction\"] = prediction[idx].tail(top_n_worst)\n",
    "        for row in worst.index:\n",
    "            print(\"Text input:\")\n",
    "            print(worst.loc[row][self.input])\n",
    "            print(\"Original text:\")\n",
    "            print(worst.loc[row][\"original_text\"])\n",
    "            print()\n",
    "            print(f\"Stars: {worst.loc[row]['stars']}\", end=\" | \"), print(f\"Prediction: {worst.loc[row]['prediction']}\")\n",
    "            print(\"--------------------------------------------------------------------------------------------------------------\")\n",
    "            print()\n",
    "\n",
    "    # Naive baseline performance - random guessing with probabilities equal to the class distribution of the target variable\n",
    "    def naive_baseline(self):\n",
    "        data = self.data\n",
    "        data[\"naive_baseline_prediction\"] = np.random.choice(self.target_distribution.index.to_numpy(), size = len(self.data), replace = True, p = self.target_distribution.values)\n",
    "        data[\"naive_baseline_prediction\"] = np.random.choice(self.target_distribution.index.to_numpy(), size = len(self.data), replace = True, p = self.target_distribution.values)\n",
    "\n",
    "        # Print performance metrics\n",
    "        print(\"Baseline Performance:\")\n",
    "        self.print_performance_scores(data[self.target], data.naive_baseline_prediction)\n",
    "\n",
    "    # Calculate rule based sentiments with Vader\n",
    "    def vader_sentiment(self, top_n_worst=5):\n",
    "        data = self.data\n",
    "        vader = SentimentIntensityAnalyzer()\n",
    "        data['vader_scores'] = data[self.input].apply(lambda x: vader.polarity_scores(x))\n",
    "        data['vader_compound'] = data.vader_scores.apply(lambda x: x['compound'])\n",
    "        data[\"vader_bins\"] = pd.cut(data.vader_compound, np.linspace(-1, 1, 6), include_lowest=True)\n",
    "        pred_table = pd.DataFrame({\"vader_bins\": data.vader_bins.sort_values().unique(), \"vader_prediction\": [0,1,2,3,4]})\n",
    "        data = data.merge(pred_table, how = \"left\", on = \"vader_bins\")\n",
    "\n",
    "        # Print performance metrics\n",
    "        print(\"Vader Performance:\")\n",
    "        self.print_performance_scores(data[self.target], data.vader_prediction)\n",
    "\n",
    "        # Show worst predictions\n",
    "        print()\n",
    "        self.show_worst_predictions(data, data[self.target], data.vader_prediction, top_n_worst)\n",
    "\n",
    "    # Calculate rule based sentiments with Textblob\n",
    "    def textblob_sentiment(self, top_n_worst=5):\n",
    "        data = self.data\n",
    "        data['textblob_score'] = data[self.input].apply(\n",
    "            lambda review: TextBlob(review).sentiment[0])\n",
    "        data[\"textblob_bins\"] = pd.cut(data.textblob_score, np.linspace(-1, 1, 6), include_lowest=True)\n",
    "        pred_table = pd.DataFrame({\"textblob_bins\": data.textblob_bins.sort_values().unique(), \"textblob_prediction\": [0,1,2,3,4]})\n",
    "        data = data.merge(pred_table, how = \"left\", on = \"textblob_bins\")\n",
    "\n",
    "        # Print performance metrics\n",
    "        print(\"Texblob Performance:\")\n",
    "        self.print_performance_scores(data[self.target], data.textblob_prediction)\n",
    "\n",
    "        # Show worst predictions\n",
    "        print()\n",
    "        self.show_worst_predictions(data, data[self.target], data.textblob_prediction, top_n_worst)\n",
    "\n",
    "    # Produce word embeddings for the whole dataset using the specified word vectors\n",
    "    def produce_embeddings(self, X, wv):\n",
    "        print(f\"Produce word embeddings for - {wv}\")\n",
    "\n",
    "        vocabulary = set(potential_wv[wv].index_to_key)\n",
    "        wpt = nltk.WordPunctTokenizer()\n",
    "\n",
    "        def avg_embeddings(document):\n",
    "            words = wpt.tokenize(document)\n",
    "            invocab = [word for word in words if word in vocabulary]\n",
    "            avg = np.mean(potential_wv[wv][invocab], axis=0) if len(invocab) >= 1 else []\n",
    "            return avg\n",
    "\n",
    "        doc_embeddings = np.array([avg_embeddings(doc) for doc in X.values])\n",
    "        return doc_embeddings\n",
    "\n",
    "\n",
    "    \n",
    "    # Evaluate a provided classifier through an outer cross validation together with hyperparameter optimization on an inner cross validation\n",
    "    def evaluate_classifier(self, name, model, params, iterations, cv_splits, cv_repeats):\n",
    "\n",
    "        # SVM in its default state does not produce the probabilities necessary for the roc_auc measures\n",
    "        if name == 'SVM':\n",
    "            scores = ['accuracy', 'balanced_accuracy', 'f1_macro', 'f1_weighted']\n",
    "        else:\n",
    "            scores = ['accuracy', 'balanced_accuracy', 'f1_macro', 'f1_weighted', 'roc_auc_ovo', 'roc_auc_ovo_weighted']\n",
    "\n",
    "\n",
    "        class Debug(BaseEstimator, TransformerMixin):\n",
    "            def transform(self, X):\n",
    "                # Comment in the following line to print data shape during Pipeline\n",
    "                #print(X.shape)\n",
    "                return X\n",
    "\n",
    "            def fit(self, X, y=None, **fit_params):\n",
    "                return self\n",
    "\n",
    "        # The preprocessor set here is just a default and gets overwritten by the possible preprocessors in the parameter space\n",
    "        pipeline = Pipeline([(\"debug1\", Debug()), ('preprocessor', CountVectorizer()), (\"debug2\", Debug()), ('oversampler', SMOTE()), ('estimator', model)])\n",
    "\n",
    "        # Setting up the Cross validation\n",
    "        inner_cv = RepeatedStratifiedKFold(n_splits = cv_splits, n_repeats = cv_repeats, random_state = 33)\n",
    "        outer_cv = RepeatedStratifiedKFold(n_splits = cv_splits, n_repeats = cv_repeats, random_state = 33)\n",
    "\n",
    "        # Doing hyperparameter optimization\n",
    "        optimization = RandomizedSearchCV(\n",
    "            estimator = pipeline,\n",
    "            param_distributions = params,\n",
    "            scoring = scores,\n",
    "            cv = inner_cv,\n",
    "            refit = 'balanced_accuracy',\n",
    "            n_iter = iterations,\n",
    "            n_jobs = os.cpu_count() -1,\n",
    "            verbose = 1,\n",
    "            random_state = 33)\n",
    "\n",
    "        # Here input data is just the index of the data\n",
    "        # The actual data gets forwarded in the preprocessing step of the pipeline\n",
    "        optimization.fit(np.array(range(len(self.data))), self.data[self.target])\n",
    "\n",
    "        # Evaluating the best model on the outer cross validation\n",
    "        performance_estimation = cross_validate(\n",
    "            estimator = optimization,\n",
    "            X = np.array(range(len(self.data))),\n",
    "            y = self.data[self.target],\n",
    "            scoring = scores,\n",
    "            cv = outer_cv,\n",
    "            n_jobs = os.cpu_count() -1)\n",
    "\n",
    "        return(\n",
    "        {\n",
    "            'name': name,\n",
    "            'optimization_cv_results': pd.DataFrame(optimization.cv_results_),\n",
    "            'best_params': optimization.best_params_,\n",
    "            'best_model': optimization.best_estimator_,\n",
    "            'acc': performance_estimation['test_accuracy'],\n",
    "            'balanced_acc': performance_estimation['test_balanced_accuracy'],\n",
    "            'f1_macro': performance_estimation['test_f1_macro'],\n",
    "            'f1_weighted': performance_estimation['test_f1_weighted'],\n",
    "            'roc_auc_macro': performance_estimation['test_roc_auc_ovo'] if 'test_roc_auc_ovo' in performance_estimation else np.nan,\n",
    "            'roc_auc_weighted': performance_estimation['test_roc_auc_ovo_weighted'] if 'test_roc_auc_ovo_weighted' in performance_estimation else np.nan\n",
    "        }\n",
    "        )\n",
    "\n",
    "\n",
    "    # Evaluate multiple classifiers after another\n",
    "    def run_experiments(self, iterations, cv_splits, cv_repeats, models = None):\n",
    "\n",
    "        self.experiment_results[f\"run_{self.__run}\"] = {}\n",
    "\n",
    "        # Determine which models to test\n",
    "        if models:\n",
    "            experiments = [experiment for experiment in self.experiments if experiment[\"name\"] in models]\n",
    "        else:\n",
    "            experiments = self.experiments\n",
    "\n",
    "        # Run experiment per model type\n",
    "        for experiment in experiments:\n",
    "\n",
    "            start_time= time.time()\n",
    "\n",
    "            print()\n",
    "            print()\n",
    "            print(experiment['name'])\n",
    "            print(\"-----------------\")\n",
    "\n",
    "            self.experiment_results[f\"run_{self.__run}\"][experiment['name']] = self.evaluate_classifier(\n",
    "                    name = experiment['name'],\n",
    "                    model = experiment['model'],\n",
    "                    params = experiment['params'],\n",
    "                    iterations = iterations,\n",
    "                    cv_splits = cv_splits,\n",
    "                    cv_repeats = cv_repeats\n",
    "                )\n",
    "\n",
    "            end_time = time.time() - start_time\n",
    "            print(f'Time: {int(round(end_time, 1))} seconds ({int(round(end_time/60, 1))} minutes)')\n",
    "\n",
    "        self.__run += 1\n",
    "\n",
    "\n",
    "    # Function to aggregate hyperparameter optimization trials\n",
    "    def aggregate_parameter_search(self):\n",
    "\n",
    "        param_df = pd.DataFrame()\n",
    "\n",
    "        # Loop over experiment results\n",
    "        for run in self.experiment_results.keys():\n",
    "            for model_name in self.experiment_results[run]:\n",
    "\n",
    "                corresponding_experiment = next((item for item in self.experiments if item[\"name\"] == model_name), None)\n",
    "\n",
    "                # Convert to string\n",
    "                intermediate_df = self.experiment_results[run][model_name]['optimization_cv_results']\n",
    "                intermediate_df[[\"param_\" + s for s in list(corresponding_experiment['params'].keys())]] = intermediate_df[\n",
    "                    [\"param_\" + s for s in list(corresponding_experiment['params'].keys())]].astype(str)\n",
    "\n",
    "                # Loop over tested parameters\n",
    "                for param in list(corresponding_experiment['params'].keys()):\n",
    "\n",
    "                    param_agg = intermediate_df.groupby(\"param_\" + param).agg(\n",
    "                        times_tested = (\"param_\" + param, \"count\"),\n",
    "                        mean_balanced_acc = (\"mean_test_balanced_accuracy\", np.mean)\n",
    "                    ).reset_index()\n",
    "                    param_agg.columns = [\"param_value\", \"times_tested\", \"mean_balanced_acc\"]\n",
    "                    param_agg[\"param\"] = param\n",
    "                    param_agg[\"model\"] = f'{run}-{model_name}'\n",
    "\n",
    "                    param_df = pd.concat([param_df, param_agg])\n",
    "\n",
    "        param_df[[\"model\", \"param\", \"param_value\", \"times_tested\", \"mean_balanced_acc\"]].sort_values(\n",
    "                [\"model\", \"param\", \"mean_balanced_acc\"], ascending = [True, True, False]).reset_index(drop = True)\n",
    "\n",
    "        return param_df\n",
    "\n",
    "\n",
    "    # Build a result table of the models tested so far and their performance metrics\n",
    "    def build_result_table(self):\n",
    "\n",
    "        # Looking at the results and building a result table\n",
    "        result_table = pd.DataFrame()\n",
    "        for run in self.experiment_results.keys():\n",
    "            for model_name in self.experiment_results[run]:\n",
    "                result = self.experiment_results[run][model_name]\n",
    "\n",
    "                result_table = pd.concat(\n",
    "                    [result_table, \n",
    "                    pd.DataFrame({\n",
    "                        'acc': np.mean(result['acc']),\n",
    "                        'balanced_acc': np.mean(result['balanced_acc']),\n",
    "                        'f1_macro': np.mean(result['f1_macro']),\n",
    "                        'f1_weighted': np.mean(result['f1_weighted']),\n",
    "                        'roc_auc_macro': np.nanmean(result['roc_auc_macro']),\n",
    "                        'roc_auc_weighted': np.nanmean(result['roc_auc_weighted'])\n",
    "                        },\n",
    "                        index = [model_name])\n",
    "                    ])\n",
    "\n",
    "        return result_table\n",
    "\n",
    "\n",
    "    # Funtion for producing a confusion matrix on single test split with each best model\n",
    "    def visualize_classifiers(self, test_share = 0.3):\n",
    "\n",
    "        # Refitting the best models on a single train test split to look at confusion matrix and ROC curve\n",
    "        X_train, X_test, y_train, y_test = train_test_split(np.array(range(len(self.data))), self.data[self.target], test_size=test_share, shuffle=True, random_state=33)\n",
    "        #X_train = pd.DataFrame(X_train, columns = features.columns)\n",
    "\n",
    "        # Subplots for Confusion Matrix\n",
    "        fig, axs = plt.subplots(3, 2, figsize=[15,7.5])\n",
    "        positions = [(0,0),(0,1),(1,0),(1,1),(2,0)]\n",
    "        position_index = 0\n",
    "\n",
    "        # Looping over experiment results\n",
    "        for run in self.experiment_results.keys():\n",
    "            for model_name in self.experiment_results[run]:\n",
    "                result = self.experiment_results[run][model_name]\n",
    "\n",
    "                # Refitting the model with the best parameter configuation (includes preprocessing and possible oversampling)\n",
    "                model = result['best_model']\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "                # Prediction on the splitted test data\n",
    "                y_pred = model.predict(X_test)\n",
    "\n",
    "                ConfusionMatrixDisplay.from_predictions(\n",
    "                    y_test, y_pred, \n",
    "                    labels = [0,1,2,3,4], \n",
    "                    ax = axs[positions[position_index]], colorbar = False, cmap = plt.cm.YlGnBu)\n",
    "                axs[positions[position_index]].set_title(result['name'])\n",
    "                position_index += 1\n",
    "\n",
    "        fig.tight_layout()\n",
    "        fig.suptitle(\"Confusion Matrices\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Produce word embeddings for - wv_glove_wiki\n",
      "Produce word embeddings for - wv_glove_twitter\n",
      "Produce word embeddings for - wv_word2vec\n"
     ]
    }
   ],
   "source": [
    "# Initialize the Sentiment Analyser\n",
    "sentiment = SentimentAnalyser(yelp.copy(deep=True), \"stars\", \"text\", normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Performance:\n",
      "Accuracy: 0.3309\n",
      "Balanced Accuracy: 0.212\n",
      "F1 (macro average): 0.2121\n",
      "F1 (weighted average): 0.3308\n",
      "Roc Auc (macro average): 0.5075\n",
      "Roc Auc (weighted average): 0.5051\n"
     ]
    }
   ],
   "source": [
    "sentiment.naive_baseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vader Performance:\n",
      "Accuracy: 0.4473\n",
      "Balanced Accuracy: 0.2696\n",
      "F1 (macro average): 0.2476\n",
      "F1 (weighted average): 0.3501\n",
      "Roc Auc (macro average): 0.5435\n",
      "Roc Auc (weighted average): 0.5523\n",
      "\n",
      "Text input:\n",
      "really good soup place typical phó small portion full artificial taste definitely recommend asiansvery noisy environment unfriendly service\n",
      "Original text:\n",
      "Not a really good soup place --- for typical Phó. Small portion, and full of artificial taste. ---- definitely not recommend for Asians.Very noisy environment and unfriendly  service.\n",
      "\n",
      "Stars: 0 | Prediction: 4\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Text input:\n",
      "confirmed yelp reservation accepted upon arrival staff tried call apparently absolving commitment argumentative rather apologetic okay\n",
      "Original text:\n",
      "Confirmed yelp reservation was not accepted upon arrival.  Staff \"tried to call\", apparently absolving them from their commitment.  Argumentative rather than apologetic.  Not okay.\n",
      "\n",
      "Stars: 0 | Prediction: 4\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Text input:\n",
      "made reservation 6 people 6pm arrived beginning female server major attitude everything said roll eyes us shrug shoulders fact one people party asked dark beer replied dont know colleague asked darkest heavier beer rolling eyes said pilsner know german beer light beer gets recommended total opposite lived germany letting slide kept attitude order food members party never asked questions menu kept acting waste time asked choice finally said ok rolling eyes one said ja finally said since walked restaurant sarcastic nasty attitude drop shock confronted walked away members party said thought noticed attitude want say anything deal server know members dinner party come hamburg every week sometimes 4 5 times month different colleagues work recommend places eat fact thought food quite mediocre ive much better germany food locations nearby definitely recommend restaurant colleagues longer personally never step one foot place better food restaurant franziskaner near lake center town\n",
      "Original text:\n",
      "We made a reservation for 6 people at 6pm. We arrived and from the beginning, the female server had a major attitude. Everything we said she had to roll her eyes at us and shrug her shoulders. In fact, one of the people in our party asked for a dark beer and she replied: \"I don't know.\" Again, my colleague asked for the darkest, heavier beer they had and rolling her eyes she said \"Pilsner\". If you know German beer, that is as light beer as it gets; so she recommended the total opposite. I lived in Germany and was letting it slide. She kept with the attitude while we order our food. Some of the members in my party have never been there and asked a few questions about the menu. She just kept acting as we were a waste of her time. When she asked me for my choice, I finally said to her: \"are you ok?\" And she, rolling her eyes one again, said: \"Ja, why?\" And I finally said, \"because since we walked in this restaurant you have had a very sarcastic and nasty attitude and you can now drop it.\" She was in shock that I confronted her. After she walked away, the members of my party said: \"I thought it was me who noticed and attitude but I did not want to say anything.\" But I did. And here is the deal: The server does not know that some if the members in my dinner party come to Hamburg every week, sometimes 4 to 5 times a month with different colleagues from work and we recommend to each other places to eat. In fact, I thought the food was quite mediocre and I've had much better Germany food in other locations nearby. I will definitely not recommend this restaurant to any of my colleagues any longer and personally, will never step one foot in this place again. (Better food: Restaurant Franziskaner near the lake in the center of town).\n",
      "\n",
      "Stars: 0 | Prediction: 4\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Text input:\n",
      "pros thoughtful attentive gracious service casual atmosphere solid wooden tables pizza great texture crispyness outside softened interior crust loaded gooey bubbly cheese couldnt decide two pizzas server offered make pizza half half really appreciated thatthe cons pizza salt bomb meats salty cheeses salty even crust excessively salty bite overwhelmingly dominated salt since got half salsiccia und peperoni half speck und geräuchertem scamorza thought half would character less salty uh uh really overwhelms contrasts flavor individual ingredients served bread dip start bread forgettable dip tastes like pure mayonnaise godawful many little peperoni tasted like brine packed jar anything fresh still stems either try surgically remove stems without destroying pizza underneath eat try spit stems salsiccia little round blackish balls rolled around slice tried eat little flavor since overcooked ordered kellerbier foul either turnover keg old havent cleaned lines long time bothwould eat ai volo even mood bubbly gooey beautifully textured za wouldnt eat salt bomb dont trust choice blending ingredients wouldnt drink beer either\n",
      "Original text:\n",
      "The pros:   * thoughtful, attentive , gracious service;         * casual atmosphere with solid wooden tables;         *  the pizza has great texture, with crispyness outside and a softened interior crust loaded with gooey, bubbly cheese;        * when I couldn't decide between two pizzas, the server offered to make the pizza half and half - - really appreciated that!The cons:    * the pizza was a salt bomb - - the meats are salty, the cheeses are salty - - but even the CRUST is excessively salty;  each bite was overwhelmingly dominated by salt - - since I got half  'Salsiccia und Peperoni'  and half  'Speck und geräuchertem Scamorza'  I thought the other half would have more character and be less salty - - but uh uh - - and this really overwhelms the contrasts of flavor of the individual ingredients;     * you are served bread and a dip to start - - the bread is forgettable, and the dip tastes like pure mayonnaise - - godawful;        * the many little peperoni tasted like the brine they are packed in from the can or jar - - they are anything but fresh - - and they still had the stems on, so you had to either try to surgically remove the stems without destroying the pizza underneath,  or eat them as is and try to spit out the stems;         *   the salsiccia were little round blackish balls that rolled around on the slice when you tried to eat them, and had little flavor since they were overcooked;         * I ordered a kellerbier,  and it was foul - - either they have no turnover so the keg is old,  or they haven't cleaned the lines in a long time - - or both.Would I eat at Ai Volo again?    Even when in the mood for a bubbly, gooey, beautifully textured 'za - - I wouldn't eat that salt bomb again,  and I now don't at all trust their choice and blending of ingredients.  I wouldn't drink a beer there again either.\n",
      "\n",
      "Stars: 0 | Prediction: 4\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Text input:\n",
      "dim sum ok main dish sweet sour terrible cheap low quality chicken sauce made love either 1190 real joke\n",
      "Original text:\n",
      "The Dim sum was ok but the main dish Sweet and Sour was terrible . Cheap low quality chicken. The sauce was not made with love either and for 11.90 a real joke\n",
      "\n",
      "Stars: 0 | Prediction: 4\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment.vader_sentiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texblob Performance:\n",
      "Accuracy: 0.319\n",
      "Balanced Accuracy: 0.2635\n",
      "F1 (macro average): 0.1955\n",
      "F1 (weighted average): 0.2534\n",
      "Roc Auc (macro average): 0.5397\n",
      "Roc Auc (weighted average): 0.5509\n",
      "\n",
      "Text input:\n",
      "fuck yeah finally pizza doesnt leave hating choices food delivery\n",
      "Original text:\n",
      "Fuck yeah, finally some pizza that doesn't leave me hating myself and my choices in food delivery.\n",
      "\n",
      "Stars: 4 | Prediction: 1\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Text input:\n",
      "location great beach stunning view harborfood service ok accept cash day age acceptable\n",
      "Original text:\n",
      "Location is great! On the beach with a stunning view of the harbor!Food and service is ok. But they only accept cash!!! In this day and age not acceptable.\n",
      "\n",
      "Stars: 0 | Prediction: 3\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Text input:\n",
      "want wine tapas well forget welcome wish owner surprisingly unfriendly rude wants real dinner guests tapas hole\n",
      "Original text:\n",
      "So you want wine and tapas? Well forget it - your not welcome there if you have that wish. The owner is surprisingly unfriendly and rude and only wants 'real' dinner guests in his tapas hole.\n",
      "\n",
      "Stars: 0 | Prediction: 3\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Text input:\n",
      "even possible provide slower service wouldt recommend stopping many places area coffee\n",
      "Original text:\n",
      "Is it even possible to provide slower service? I would't recommend stopping by there but there is not many places in the area for a coffee, so...\n",
      "\n",
      "Stars: 0 | Prediction: 3\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n",
      "Text input:\n",
      "good food waiter charged 3 euro glass water happy\n",
      "Original text:\n",
      "Good food but the waiter charged me 3 euro for a glass of water.... not happy at all.\n",
      "\n",
      "Stars: 0 | Prediction: 4\n",
      "--------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentiment.textblob_sentiment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Logistic Regression', 'KNeighbors', 'Random Forest', 'SVM', 'XGBoost']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print possible models\n",
    "[experiment[\"name\"] for experiment in sentiment.experiments]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "SVM\n",
      "-----------------\n",
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n",
      "[LibSVM]Time: 63 seconds (1 minutes)\n"
     ]
    }
   ],
   "source": [
    "sentiment.run_experiments(20, 5, 1, ['Logistic Regression', 'KNeighbors', 'Random Forest', 'SVM'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Logistic Regression\n",
      "-----------------\n",
      "Fitting 2 folds for each of 10 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ValentinStudium\\Envs\\sma\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 72 seconds (1 minutes)\n"
     ]
    }
   ],
   "source": [
    "sentiment.run_experiments(10, 5, 1, ['XGBoost'])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2822d77ee51dbb640527abe329425380abdeab763840ac762f0d0bc769537fcd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('sma')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
