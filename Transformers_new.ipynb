{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Portfolio-Exam Part I - Sentiment Analysis\n",
    "\n",
    "* Social Media Analytics - MADS-SMA\n",
    "* Valentin Werger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Addition to the first notebook: Evaluatin different transformer models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running in Google colab, uncomment the first two line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers &> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install datasets &> /dev/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import datasets\n",
    "from datasets import load_metric\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoConfig, get_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to compare the previous performances to those of finetuned transformer models on the same yelp reviews. In theory these models should be better at predicting the sentiment, since they have learned a contextualized understanding of the text and are able to apply that to the new task. In contrast the previous machine learning algorithms are only give static word counts or vectors, which fail to capture multiple meanings or meanings in a specific word order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "yelp = pd.read_csv(\"yelp_reviews_hamburg_en.csv\", parse_dates=[\"date\"], dtype={\"stars\":\"int64\"})\n",
    "# Subtract one from stars because some models (XGBoost, Transfomers) expects labels to be starting from 0\n",
    "yelp[\"stars\"] = yelp.stars - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For easier handling of the data they are transformed into a huggingface dataset. While doing that a test set of 20% size ist split off, so the performance of the finetuned model can be estimated later. This is less robust than the previous approach with cross validation, but will give an idea whether the transformers are actually stronger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['stars', 'text', 'review_length'],\n",
       "        num_rows: 2420\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['stars', 'text', 'review_length'],\n",
       "        num_rows: 605\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform data into huggingface\n",
    "yelp[\"review_length\"] = [len(review) for review in yelp[\"text\"]]\n",
    "yelp_huggingface = datasets.Dataset.from_pandas(yelp.drop(columns = [\"url\", \"date\"])).train_test_split(test_size=0.2, seed=10)\n",
    "yelp_huggingface = yelp_huggingface.sort(\"review_length\", reverse = True)\n",
    "\n",
    "yelp_huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56932c36e1f1490a89a289653c866a23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1105 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a97a1e9c418747b1a8d03f33018a4718",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False    2400\n",
      "True       20\n",
      "dtype: int64\n",
      "False    601\n",
      "True       4\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Test how often token amount would exceed the limit\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    result = tokenizer(\n",
    "        examples[\"text\"])\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "tokenized_datasets = yelp_huggingface.map(tokenize_function, batched=True)\n",
    "\n",
    "# How often would the amount of tokens exceed Berts maximum length of token inputs?\n",
    "print(pd.Series([len(x) > 512 for x in tokenized_datasets[\"train\"][\"input_ids\"]]).value_counts())\n",
    "print(pd.Series([len(x) > 512 for x in tokenized_datasets[\"test\"][\"input_ids\"]]).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only few instances where the tokenization produces an input longer than the maximum allowed input of 512 so it would not be a big problem to simply truncate those instances. But for the sake of experience I will accomodate for them in the following function. This is done by specifying return_overflowing_tokens=True which gives back the tokens longer than the max_length, which are simply appended so the model input becomes slightly longer than the original amount of reviews. A stride of 100 means that in these cases an overlap of 100 tokens is enforced to not loose import contexts by splitting the text. The sample map allows to map the orginial columns back at the correct positions. At last a pytorch DataLoader is created from the tokenized data.\n",
    "\n",
    "An alternative for very long documents, that is not explored, here would be to first summarize them before using them as input for the transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data, pretrained_model, batch_size = 8):\n",
    "\n",
    "  tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
    "\n",
    "  def tokenize_function(examples):\n",
    "    result = tokenizer(\n",
    "        examples[\"text\"], \n",
    "        max_length=512, \n",
    "        padding=\"max_length\", \n",
    "        truncation=True,\n",
    "        return_overflowing_tokens=True, \n",
    "        stride=100)\n",
    "    \n",
    "    sample_map = result.pop(\"overflow_to_sample_mapping\")\n",
    "    for key, values in examples.items():\n",
    "        result[key] = [values[i] for i in sample_map]\n",
    "    return result\n",
    "\n",
    "\n",
    "  tokenized_datasets = data.map(tokenize_function, batched=True)\n",
    "\n",
    "  tokenized_datasets = tokenized_datasets.remove_columns([\"text\", \"review_length\"])\n",
    "  tokenized_datasets = tokenized_datasets.rename_column(\"stars\", \"labels\")\n",
    "  tokenized_datasets.set_format(\"torch\")\n",
    "\n",
    "  train_dataloader = DataLoader(tokenized_datasets[\"train\"], shuffle=True, batch_size=batch_size)\n",
    "  eval_dataloader = DataLoader(tokenized_datasets[\"test\"], batch_size=batch_size)\n",
    "\n",
    "  return train_dataloader, eval_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The chosen model is trained with AdamW as optimizer and a fixed learning rate unless varied by the user. If a GPU device is available (for example in a colab Notebook marked as GPU runtime or on a local machine with cuda-enabled torch installation) the training loop is processed on that device increasing the speed by magnitudes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_dataloader, lr=5e-5, epochs=10):\n",
    "\n",
    "  optimizer = AdamW(model.parameters(), lr=lr)\n",
    "\n",
    "  num_training_steps = epochs * len(train_dataloader)\n",
    "  lr_scheduler = get_scheduler(\n",
    "      name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    "  )\n",
    "\n",
    "  device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "  print(device)\n",
    "  model.to(device)\n",
    "\n",
    "  progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "  model.train()\n",
    "  for epoch in range(epochs):\n",
    "      for batch in train_dataloader:\n",
    "          batch = {k: v.to(device) for k, v in batch.items()}\n",
    "          outputs = model(**batch)\n",
    "          loss = outputs.loss\n",
    "          loss.backward()\n",
    "\n",
    "          optimizer.step()\n",
    "          lr_scheduler.step()\n",
    "          optimizer.zero_grad()\n",
    "          progress_bar.update(1)\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The finetuned model can be evaluated with the eval_dataloader and a list of specified metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, eval_dataloader, metrics, print_metric_info=False):\n",
    "\n",
    "  device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "  metric_list = []\n",
    "  for metric_config in metrics:\n",
    "    metric_dict = {}\n",
    "    metric_dict[\"metric\"] = load_metric(metric_config[\"name\"])\n",
    "    metric_dict[\"additional_arguments\"] = metric_config[\"additional_arguments\"]\n",
    "    if print_metric_info:\n",
    "      print(metric_dict[\"metric\"].inputs_description)\n",
    "\n",
    "    metric_list.append(metric_dict)\n",
    "\n",
    "  model.eval()\n",
    "  for batch in eval_dataloader:\n",
    "      batch = {k: v.to(device) for k, v in batch.items()}\n",
    "      with torch.no_grad():\n",
    "          outputs = model(**batch)\n",
    "\n",
    "      logits = outputs.logits\n",
    "      predictions = torch.argmax(logits, dim=-1)\n",
    "\n",
    "      for metric in metric_list:\n",
    "        metric[\"metric\"].add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "  def merge_dicts(dict1, dict2):\n",
    "    return {**dict1, **dict2}\n",
    "  metrics_results = {}\n",
    "\n",
    "  for metric in metric_list:\n",
    "    metrics_results = merge_dicts(metrics_results, metric[\"metric\"].compute(**metric[\"additional_arguments\"]))\n",
    "\n",
    "  return metrics_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_gpu(msg):\n",
    "    \"\"\"\n",
    "    ref: https://discuss.pytorch.org/t/access-gpu-memory-usage-in-pytorch/3192/4\n",
    "    \"\"\"\n",
    "    def query(field):\n",
    "        return(subprocess.check_output(\n",
    "            ['nvidia-smi', f'--query-gpu={field}',\n",
    "                '--format=csv,nounits,noheader'], \n",
    "            encoding='utf-8'))\n",
    "    def to_int(result):\n",
    "        return int(result.strip().split('\\n')[0])\n",
    "    \n",
    "    used = to_int(query('memory.used'))\n",
    "    total = to_int(query('memory.total'))\n",
    "    pct = used/total\n",
    "    print('\\n' + msg, f'{100*pct:2.1f}% ({used} out of {total})') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three pretained transformer models from huggingface are tested in the following section:\n",
    "\n",
    "* bert-base-uncased\n",
    "  * pretrained on a large corpus of English data in a self-supervised fashion\n",
    "  * trained through Masked language modeling (MLM) and Next sentence prediction (NSP)\n",
    "* roberta-base\n",
    "  * trained on wikipedia corpus and a large corpus of englisch books\n",
    "  * self trained through Masked language modeling (MLM)\n",
    "* distilbert-base-uncased-finetunded-sst-2-english\n",
    "  * fine tuned version of distilbert on SST-2 film review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPU usage 32.0% (2625 out of 8192)\n"
     ]
    }
   ],
   "source": [
    "show_gpu(\"GPU usage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bert-base-uncased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a9ea1625f5e4334aa8784d861f7470a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b1d060205804c578efb725e9e0ad52b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataloader, eval_dataloader = prepare_data(yelp_huggingface, \"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58201ba0706e4392a85cbc9661536f7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/918 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=5)\n",
    "finetuned_model = train_model(model, train_dataloader, epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.6617405582922824,\n",
       " 'f1': 0.5855779921189521,\n",
       " 'precision': 0.6110926669001083,\n",
       " 'recall': 0.586010698537382}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(finetuned_model, \n",
    "               eval_dataloader, \n",
    "               [\n",
    "                {\"name\":\"accuracy\", \"additional_arguments\":{}}, \n",
    "                {\"name\":\"f1\", \"additional_arguments\": {\"average\":\"macro\"}},\n",
    "                {\"name\": \"precision\", \"additional_arguments\": {\"average\": \"macro\"}},\n",
    "                {\"name\": \"recall\", \"additional_arguments\": {\"average\": \"macro\"}}\n",
    "                ], \n",
    "               print_metric_info=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPU usage 22.8% (1864 out of 8192)\n"
     ]
    }
   ],
   "source": [
    "del finetuned_model\n",
    "torch.cuda.empty_cache()\n",
    "show_gpu(\"GPU usage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### roberta-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da2ef6bd19474a3991396040ffc704b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "891f98c54c6141849bbee4707452a235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataloader, eval_dataloader = prepare_data(yelp_huggingface, \"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28b423ac288e48759a31c975690e3b66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/915 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"roberta-base\", num_labels=5)\n",
    "finetuned_model = train_model(model, train_dataloader, epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.639344262295082,\n",
       " 'f1': 0.5084989366349728,\n",
       " 'precision': 0.6038327562186113,\n",
       " 'recall': 0.5351313328595577}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(finetuned_model, \n",
    "               eval_dataloader, \n",
    "               [\n",
    "                {\"name\":\"accuracy\", \"additional_arguments\":{}}, \n",
    "                {\"name\":\"f1\", \"additional_arguments\": {\"average\":\"macro\"}},\n",
    "                {\"name\": \"precision\", \"additional_arguments\": {\"average\": \"macro\"}},\n",
    "                {\"name\": \"recall\", \"additional_arguments\": {\"average\": \"macro\"}}\n",
    "                ], \n",
    "               print_metric_info=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPU usage 23.8% (1947 out of 8192)\n"
     ]
    }
   ],
   "source": [
    "del finetuned_model\n",
    "torch.cuda.empty_cache()\n",
    "show_gpu(\"GPU usage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### distilbert-base-uncased-finetunded-sst-2-english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e35df8129f014c57b52263c4292a956a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4b44e2556164f908f6cd02778d01346",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataloader, eval_dataloader = prepare_data(yelp_huggingface, \"distilbert-base-uncased-finetuned-sst-2-english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ab95f754a8846cea9ebfb8dc23474fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1530 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "config = AutoConfig.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "config.num_labels = 5\n",
    "model = AutoModelForSequenceClassification.from_config(config)\n",
    "\n",
    "finetuned_model = train_model(model, train_dataloader, epochs = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Valentins PC\\Envs\\sentiment\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.5467980295566502,\n",
       " 'f1': 0.35070834672395523,\n",
       " 'precision': 0.36899411449973246,\n",
       " 'recall': 0.35379784305685036}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_model(finetuned_model, \n",
    "               eval_dataloader, \n",
    "               [\n",
    "                {\"name\":\"accuracy\", \"additional_arguments\":{}}, \n",
    "                {\"name\":\"f1\", \"additional_arguments\": {\"average\":\"macro\"}},\n",
    "                {\"name\": \"precision\", \"additional_arguments\": {\"average\": \"macro\"}},\n",
    "                {\"name\": \"recall\", \"additional_arguments\": {\"average\": \"macro\"}}\n",
    "                ], \n",
    "               print_metric_info=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GPU usage 27.4% (2244 out of 8192)\n"
     ]
    }
   ],
   "source": [
    "del finetuned_model\n",
    "torch.cuda.empty_cache()\n",
    "show_gpu(\"GPU usage\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison\n",
    "\n",
    "bert-base-uncased achieves the best accuracy with about 0.66 in contrast to roberta-base with only about 0.64. But the more relevant metric is the F1 Score. It is the harmonic mean between precision and recall, so the model gets punished for being especially bad in one of them, and thus represents the tradeoff between precision and recall a model faces. In the multiclass case like here, there are several ways to average the F1 score that is being generated for every class. Here we use macro-averaging, where a simple mean is taken that weights every class the same. This gives a higher importance to the F1 score for minority classes like 1 or 2 stars compared to their share of the data. The F1 scores are a little bit lower but not that much more than the simple accuracy, indicating that the finetuned models do not just classify better reviews and indeed are able to also classify bad reviews to a reasonable degree. In this metric roberta-base is quite a bit lower than bert with 0.5.\n",
    "\n",
    "The last model represents an already finetuned model based on distilbert, that was trained to predict binary sentiments from english movie reviews. We replaced the classification head to output 5 labels and further finetuned this model on our yelp review data. The premise is that a model that is not just trained on general language but already specialised on reviews might perform better. In practice however the scores are well below those of the original transformers. Especially the F1 score is a lot lower than the accuracy and the F1 scores of the first two models, which can be due to a worse general performance, bigger gap between precision and recall and unequal performance on the different classes, favoring the majority classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "In general as expected the finetuned base transformer models are significantly better than the previously tried machine learning approaches with word counts or embeddings as features. They reach f1_macro scores of almost 0.6 against the 0.42 of Logistic Regression which was the best model in the previous experiment.\n",
    "\n",
    "However increasing the amount of epochs did not seem to significantly improve test performance after 3 epochs for the two base transformers so one would need to find other ways to increase the performance on the yelp reviews even further.\n",
    "\n",
    "As a last point one has to note, that the transformer models were only tested on a single test split. The choice of that split has a strong impact on evaluation of the model so the achieved numbers in this notebook are a lot less robust than those obtained via cross validation for the machine learning models."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e894770f61f995391357fd814fa5a3f428c83e087f85246b41c1f2369ee52d7d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.13 ('sentiment')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
